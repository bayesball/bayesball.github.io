<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.309">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian Modeling - 2&nbsp; Modeling Measurement and Count Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<link href="./mcmc.html" rel="next">
<link href="./proportion.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Modeling Measurement and Count Data</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Modeling</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proportion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Learning About a Binomial Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mean.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Modeling Measurement and Count Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Simulation by Markov Chain Monte Carlo</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Bayesian Hierarchical Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"> <span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#modeling-measurements" id="toc-modeling-measurements" class="nav-link" data-scroll-target="#modeling-measurements"> <span class="header-section-number">2.2</span> Modeling Measurements</a>
  <ul class="collapse">
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"> <span class="header-section-number">2.2.1</span> Examples</a></li>
  <li><a href="#the-general-approach" id="toc-the-general-approach" class="nav-link" data-scroll-target="#the-general-approach"> <span class="header-section-number">2.2.2</span> The general approach</a></li>
  <li><a href="#outline-of-chapter" id="toc-outline-of-chapter" class="nav-link" data-scroll-target="#outline-of-chapter"> <span class="header-section-number">2.2.3</span> Outline of chapter</a></li>
  </ul></li>
  <li><a href="#Normal:Discrete" id="toc-Normal:Discrete" class="nav-link" data-scroll-target="#Normal\:Discrete"> <span class="header-section-number">2.3</span> Bayesian Inference with Discrete Priors</a>
  <ul class="collapse">
  <li><a href="#Normal:Discrete:Roger" id="toc-Normal:Discrete:Roger" class="nav-link" data-scroll-target="#Normal\:Discrete\:Roger"> <span class="header-section-number">2.3.1</span> Example: Roger Federer’s time-to-serve</a></li>
  <li><a href="#Normal:SamplingModel:derivation" id="toc-Normal:SamplingModel:derivation" class="nav-link" data-scroll-target="#Normal\:SamplingModel\:derivation"> <span class="header-section-number">2.3.2</span> Simplification of the likelihood</a></li>
  <li><a href="#Normal:SamplingModel:inference" id="toc-Normal:SamplingModel:inference" class="nav-link" data-scroll-target="#Normal\:SamplingModel\:inference"> <span class="header-section-number">2.3.3</span> Inference: Federer’s time-to-serve</a></li>
  </ul></li>
  <li><a href="#Normal:Continuous" id="toc-Normal:Continuous" class="nav-link" data-scroll-target="#Normal\:Continuous"> <span class="header-section-number">2.4</span> Continuous Priors</a>
  <ul class="collapse">
  <li><a href="#Normal:Continuous:prior" id="toc-Normal:Continuous:prior" class="nav-link" data-scroll-target="#Normal\:Continuous\:prior"> <span class="header-section-number">2.4.1</span> The Normal prior for mean <span class="math inline">\(\mu\)</span></a></li>
  <li><a href="#Normal:Continuous:choosing" id="toc-Normal:Continuous:choosing" class="nav-link" data-scroll-target="#Normal\:Continuous\:choosing"> <span class="header-section-number">2.4.2</span> Choosing a Normal prior</a></li>
  </ul></li>
  <li><a href="#Normal:ContinuousUpdate" id="toc-Normal:ContinuousUpdate" class="nav-link" data-scroll-target="#Normal\:ContinuousUpdate"> <span class="header-section-number">2.5</span> Updating the Normal Prior</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"> <span class="header-section-number">2.5.1</span> Introduction</a></li>
  <li><a href="#Normal:ContinuousUpdate:Overview" id="toc-Normal:ContinuousUpdate:Overview" class="nav-link" data-scroll-target="#Normal\:ContinuousUpdate\:Overview"> <span class="header-section-number">2.5.2</span> A quick peak at the update procedure</a></li>
  <li><a href="#Normal:ContinuousUpdate:BayesRule" id="toc-Normal:ContinuousUpdate:BayesRule" class="nav-link" data-scroll-target="#Normal\:ContinuousUpdate\:BayesRule"> <span class="header-section-number">2.5.3</span> Bayes’ rule calculation</a></li>
  <li><a href="#Normal:ContinuousUpdate:Conjugate" id="toc-Normal:ContinuousUpdate:Conjugate" class="nav-link" data-scroll-target="#Normal\:ContinuousUpdate\:Conjugate"> <span class="header-section-number">2.5.4</span> Conjugate Normal prior</a></li>
  </ul></li>
  <li><a href="#Normal:ContinuousInference" id="toc-Normal:ContinuousInference" class="nav-link" data-scroll-target="#Normal\:ContinuousInference"> <span class="header-section-number">2.6</span> Bayesian Inferences for Continuous Normal Mean</a>
  <ul class="collapse">
  <li><a href="#Normal:ContinuousInference:HTandCI" id="toc-Normal:ContinuousInference:HTandCI" class="nav-link" data-scroll-target="#Normal\:ContinuousInference\:HTandCI"> <span class="header-section-number">2.6.1</span> Bayesian hypothesis testing and credible interval</a></li>
  <li><a href="#Normal:ContinuousInference:Prediction" id="toc-Normal:ContinuousInference:Prediction" class="nav-link" data-scroll-target="#Normal\:ContinuousInference\:Prediction"> <span class="header-section-number">2.6.2</span> Bayesian prediction</a></li>
  </ul></li>
  <li><a href="#Normal:PPC" id="toc-Normal:PPC" class="nav-link" data-scroll-target="#Normal\:PPC"> <span class="header-section-number">2.7</span> Posterior Predictive Checking</a></li>
  <li><a href="#modeling-count-data" id="toc-modeling-count-data" class="nav-link" data-scroll-target="#modeling-count-data"> <span class="header-section-number">2.8</span> Modeling Count Data</a>
  <ul class="collapse">
  <li><a href="#examples-1" id="toc-examples-1" class="nav-link" data-scroll-target="#examples-1"> <span class="header-section-number">2.8.1</span> Examples</a></li>
  <li><a href="#the-poisson-distribution" id="toc-the-poisson-distribution" class="nav-link" data-scroll-target="#the-poisson-distribution"> <span class="header-section-number">2.8.2</span> The Poisson distribution</a></li>
  <li><a href="#bayesian-inferences" id="toc-bayesian-inferences" class="nav-link" data-scroll-target="#bayesian-inferences"> <span class="header-section-number">2.8.3</span> Bayesian inferences</a></li>
  <li><a href="#case-study-learning-about-website-counts" id="toc-case-study-learning-about-website-counts" class="nav-link" data-scroll-target="#case-study-learning-about-website-counts"> <span class="header-section-number">2.8.4</span> Case study: Learning about website counts</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Modeling Measurement and Count Data</span></h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell">

</div>
<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>We first consider the general situation where there is a hypothetical population of individuals of interest and there is a continuous-valued measurement <span class="math inline">\(Y\)</span> associated with each individual. One represents the collection of measurements from all individuals by means of a continuous probability density <span class="math inline">\(f(y)\)</span>. As discussed in Chapter 5, one summarizes this probability density with the mean <span class="math inline">\(\mu\)</span>: <span class="math display">\[\begin{equation}
\mu = \int y f(y) dy.
\end{equation}\]</span> The value <span class="math inline">\(\mu\)</span> gives us a sense of the location of a typical value of the continuous measurement <span class="math inline">\(Y\)</span>.</p>
<p>To learn about the population of measurements, a random sample of individuals <span class="math inline">\(Y_1, ..., Y_n\)</span> will be taken. The general inferential problem is to use these measurements together with any prior beliefs to learn about the population mean <span class="math inline">\(\mu\)</span>. In other words, the goal is to use the collected measurements to learn about a typical value of the population of measurements.</p>
</section>
<section id="modeling-measurements" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="modeling-measurements"><span class="header-section-number">2.2</span> Modeling Measurements</h2>
<section id="examples" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="examples"><span class="header-section-number">2.2.1</span> Examples</h3>
<section id="college-applications" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="college-applications">College applications</h4>
<p>How many college applications does a high school senior in the United States complete? Here one imagines a population of all American high school seniors and the measurement is the number of completed college applications. The unknown quantity of interest is the mean number of applications <span class="math inline">\(\mu\)</span> completed by these high school seniors. The inferential question may be stated by asking, on average, how many college applications does an American high school senior complete. The answer to this question gives one a sense of the number of completed applications for a typical high school senior. To learn about the average <span class="math inline">\(\mu\)</span>, it would be infeasible to collect this measurement from every high school senior in the U.S. Instead, a survey is typically conducted to a sample of high school seniors (ideally a sample representative of all American high school seniors) and based on the measurements from this sample, some inference is performed about the mean number of college applications.</p>
</section>
<section id="household-spending" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="household-spending">Household spending</h4>
<p>How much does a household in San Francisco spend on housing every month? One visualizes the population of households in San Francisco and the continuous measurement is the amount of money spent on housing (either rent for renters and mortgage for homeowners) for a resident. One can ask “on average, how much does a household spend on housing every month in San Francisco?”, and the answer to this question gives one a sense of the housing costs for a typical household in San Francisco. To learn about the mean value of housing <span class="math inline">\(\mu\)</span> of all San Francisco residents, a sample survey is conducted. The mean value of the housing costs <span class="math inline">\(\bar y\)</span> from this sample of surveyed households is informative about the mean housing cost <span class="math inline">\(\mu\)</span> for all residents.</p>
</section>
<section id="weights-of-cats" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="weights-of-cats">Weights of cats</h4>
<p>Suppose you have a domestic shorthair cat weighing 14 pounds and you want to find out if she is overweight. One imagines a population of all domestic shorthair cats and the continuous measurement is the weight in pounds. Suppose you were able to compute the mean weight <span class="math inline">\(\mu\)</span> of all shorthair cats. Then by comparing 14 pounds (the weight of our cat) to this mean, you would know whether your cat is overweight, or underweight, or close to the mean. If we were able to find the distribution of the weights of all domestic shorthair cats, then one observes the proportion of weights smaller than 14 pounds in the distribution and learns if the cat is severely overweight. To learn if our cat is overweight, you can ask the vet. How does the vet know? Extensive research has been conducted periodically to record weights of a large sample of domestic shorthair cats, and by using these sample of weights, the vet performs an inference about the mean <span class="math inline">\(\mu\)</span> of the weights of all domestic shorthair cats.</p>
</section>
<section id="comment-elements-of-an-inference-problem" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="comment-elements-of-an-inference-problem">Comment elements of an inference problem</h4>
<p>All three examples have common elements:</p>
<ul>
<li><p>One has an underlying population of measurements, where the measurement is an integer, such as the number of college applications, or continuous, such as a housing cost or a cat weight.</p></li>
<li><p>One is interested in learning about the value of the mean <span class="math inline">\(\mu\)</span> of the population of measurements.</p></li>
<li><p>It is impossible or impractical to collect all measurements from the population, so one will collect a sample of measurements <span class="math inline">\(Y_1, ..., Y_n\)</span> and use the observed measurements to learn about the unknown population mean <span class="math inline">\(\mu\)</span>.</p></li>
</ul>
</section>
</section>
<section id="the-general-approach" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="the-general-approach"><span class="header-section-number">2.2.2</span> The general approach</h3>
<p>Recall the three general steps of Bayesian inference discussed in Chapter 7 in the context of an unknown proportion <span class="math inline">\(p\)</span>.</p>
<ul>
<li><p>Step 1: <strong>Prior</strong> We express an opinion about the location of the proportion <span class="math inline">\(p\)</span> before sampling.</p></li>
<li><p>Step 2: <strong>Data/Likelihood</strong> We take the sample and record the observed proportion.</p></li>
<li><p>Step 3: <strong>Posterior</strong> We use Bayes’ rule to sharpen and update the previous opinion about <span class="math inline">\(p\)</span> given the information from the sample.</p></li>
</ul>
<p>In this setting, we have a continuous population of measurements that we represent by the random variable <span class="math inline">\(Y\)</span> with density function <span class="math inline">\(f(y)\)</span>. It is convenient to assume that this population has a Normal shape with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. That is, a single measurement <span class="math inline">\(Y\)</span> is assume to come from the density function <span class="math display">\[\begin{equation}
f(y) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{(y - \mu)^2}{2 \sigma^2}\right\}, -\infty &lt; y&lt; \infty.
\end{equation}\]</span> displayed in Figure 8.1. To simplify the discussion, it is convenient to assume that the standard deviation <span class="math inline">\(\sigma\)</span> of the measurement distribution is known. Then the objective is to learn about the single mean measurement <span class="math inline">\(\mu\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter8/normaldensity.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Normal sampling density with mean <span class="math inline">\(\mu\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Step 1 in Bayesian inference is to express an opinion about the parameter. In this continuous measurement setting, one constructs a prior for the mean parameter <span class="math inline">\(\mu\)</span> that expresses one’s opinion about the location of this mean. In this chapter, we discuss different ways to specify a prior distribution for <span class="math inline">\(\mu\)</span>. One attractive discrete approach for expressing this prior opinion, similar to the approach in Chapter 7 for a proportion <span class="math inline">\(p\)</span>, has two steps. First one constructs a list of possible values of <span class="math inline">\(\mu\)</span>, and then one assigns probabilities to the possible values to reflect one’s belief. Alternatively, we will describe the use of a continuous prior to represent one’s belief for <span class="math inline">\(\mu\)</span>. This is a more realistic approach for constructing a prior since one typically views the mean as a real-valued parameter.</p>
<p>Step 2 of our process is to collect measurements from a random sample to gain more information about the parameter <span class="math inline">\(\mu\)</span>. In our first situation, one collects the number of applications from a sample of 100 high school seniors. In the second example, one collects a sample of 2000 housing costs, each from a sampled San Francisco household. The third example collects a sample of 200 different weights of domestic shorthair cats, each from a sampled cat. If these measurements are viewed as independent observations from a Normal sampling density with mean <span class="math inline">\(\mu\)</span>, then one constructs a likelihood function which is the joint density of the sampled measurements viewed as a function of the unknown parameter.</p>
<p>Once the prior is specified and measurements have been collected, one proceeds to Step 3 to use Bayes’ rule to update one’s prior opinion to obtain a posterior distribution for the mean <span class="math inline">\(\mu\)</span>. The algebraic implementation of Bayes’ rule is a bit more tedious when dealing with continuous data with a Normal sampling density. But we will see there is a simple procedure for computing the posterior mean and standard deviation.</p>
</section>
<section id="outline-of-chapter" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="outline-of-chapter"><span class="header-section-number">2.2.3</span> Outline of chapter</h3>
<p>Throughout this chapter, the entire inferential process is described for learning about a mean <span class="math inline">\(\mu\)</span> assuming a Normal sampling density for the measurements. This chapter discusses how to construct a prior distribution that matches one’s prior belief, how to extract information from the data by the likelihood function, and how to update one’s opinion in the posterior, combining the prior and data information in a natural way.</p>
<p>Section 8.3 introduces inference with a discrete prior distribution for the mean <span class="math inline">\(\mu\)</span> and Section 8.4 introduces the continuous family of Normal prior distributions for the mean. The inferential process with a Normal prior distribution is described in detail in Section 8.5. Section 8.6 describes some general Bayesian inference methods in this Normal data/Normal prior setting, such as Bayesian hypothesis testing, Bayesian credible intervals and Bayesian prediction. These sections describe the use of both exact analytical solutions and approximation simulation-based calculations. Section 8.7 introduces the use of the posterior predictive distribution as a general tool for checking if the observed data is consistent with predictions from the Bayesian model.</p>
<p>The chapter concludes in Section 8.8 by introducing a popular one-parameter model for counts, the Poisson distribution, and its conjugate Gamma distribution for representing prior opinion. Although this section does not deal with the Normal mean situation, the exposure to the important Gamma-Poisson conjugacy will enhance our understanding and knowledge of the analytical process of combining the prior and likelihood to obtain the posterior distribution.</p>
</section>
</section>
<section id="Normal:Discrete" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="Normal:Discrete"><span class="header-section-number">2.3</span> Bayesian Inference with Discrete Priors</h2>
<section id="Normal:Discrete:Roger" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="Normal:Discrete:Roger"><span class="header-section-number">2.3.1</span> Example: Roger Federer’s time-to-serve</h3>
<p>Roger Federer is recognized as one of the greatest players in tennis history. One aspect of his play that people enjoy is his businesslike way of serving to start a point in tennis. Federer appears to be efficient in his preparation to serve and some of his service games are completed very quickly. One measures one’s service efficiency by the time-to-serve which is the measured time in seconds between the end of the previous point and the beginning of the current point.</p>
<p>Since Federer is viewed as an efficient server, this raises the question: how long, on average, is Federer’s time-to-serve? We know two things about his time-to-serve measurements. First, since they are time measurements, they are continuous variables. Second, due to a number of other variables, the measurements will vary from serve to serve. Suppose one collects a single time-to-serve measurement in seconds. denoted as <span class="math inline">\(Y\)</span>. It seems reasonable to assume <span class="math inline">\(Y\)</span> is Normally distributed with unknown mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. From previous data, we assume that the standard deviation is known and given by <span class="math inline">\(\sigma = 4\)</span> seconds.</p>
<p>Recall the Normal probability curve has the general form</p>
<p><span class="math display">\[\begin{equation}
f(y) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{(y - \mu)^2}{2 \sigma^2}\right\}, -\infty &lt; y&lt; \infty.
\end{equation}\]</span> Since <span class="math inline">\(\sigma = 4\)</span> is known, the only parameter in Equation (8.3) is <span class="math inline">\(\mu\)</span>. We are interested in learning about the mean time-to-serve <span class="math inline">\(\mu\)</span>.</p>
<p>A convenient first method of implementing Bayesian inference is by the use of a discrete prior. One specifies a subjective discrete prior for Federer’s mean time-to-serve by specifying a list of plausible values for <span class="math inline">\(\mu\)</span> and assigning a probability to each of these values.</p>
<p>In particular suppose one thinks that values of the equally spaced values <span class="math inline">\(\mu\)</span> = 15, 16, <span class="math inline">\(\cdots\)</span>, 22 are plausible. In addition, one does not have any good reason to think that any of these values for the mean are more or less likely, so a Uniform prior will be assigned where each value of <span class="math inline">\(\mu\)</span> is assigned the same probability <span class="math inline">\(\frac{1}{8}\)</span>. <span class="math display">\[\begin{equation}
\pi(\mu) = \frac{1}{8}, \, \, \, \, \mu = 15, 16, ..., 22.
\end{equation}\]</span> Each value of <span class="math inline">\(\mu\)</span> corresponds to a particular Normal sampling curve for the time-to-serve measurement. Figure 8.2 displays the eight possible Normal sampling curves. Our prior says that each of these eight sampling curves has the same prior probability.</p>
<div class="cell">

</div>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Eight possible Normal sampling curves corresponding to a discrete Uniform prior on <span class="math inline">\(\mu\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To learn more about the mean <span class="math inline">\(\mu\)</span>, one collects a single time-to-serve measurement for Federer, and suppose it is 15.1 seconds, that is, one observes <span class="math inline">\(Y = 15.1\)</span>. The likelihood function is the Normal density of the actual observation <span class="math inline">\(y\)</span> viewed as a function of the mean <span class="math inline">\(\mu\)</span> (remember that it was assumed that <span class="math inline">\(\sigma = 4\)</span> was given). By substituting in the observation <span class="math inline">\(y = 15.1\)</span> and the known value of <span class="math inline">\(\sigma = 4\)</span>, one writes the likelihood function as</p>
<p><span class="math display">\[\begin{eqnarray*}
L(\mu) = \frac{1}{\sqrt{2 \pi} 4} \exp\left\{- \frac{1}{2 (4)^2}(15.1 - \mu)^2\right\}.
\end{eqnarray*}\]</span></p>
<p>For each possible value of <span class="math inline">\(\mu\)</span>, we substitute the value into the likelihood expression. For example, the likelihood of <span class="math inline">\(\mu = 15\)</span> is equal to <span class="math display">\[\begin{eqnarray*}
L(15) &amp;=&amp; \frac{1}{\sqrt{2 \pi} (4)} \exp\left(- \frac{1}{2 (4)^2}(15.1 - 15)^2\right) \nonumber \\
&amp;\approx &amp; 0.0997.
\end{eqnarray*}\]</span> This calculation is repeated for each of the eight values <span class="math inline">\(\mu = 15, 16, \cdots, 22\)</span>, obtaining eight likelihood values.</p>
<p>A discrete prior has been assigned to the list of possible values of <span class="math inline">\(\mu\)</span> and one is now able to apply Bayes’ rule to obtain the posterior distribution for <span class="math inline">\(\mu\)</span>. The posterior probability of the value <span class="math inline">\(\mu = \mu_i\)</span> given the data <span class="math inline">\(y\)</span> for a discrete prior has the form <span class="math display">\[\begin{equation}
\pi(\mu_i \mid y) = \frac{\pi(\mu_i) \times L(\mu_i)}{\sum_j \pi(\mu_j) \times L(\mu_j)},
\end{equation}\]</span> where <span class="math inline">\(\pi(\mu_i)\)</span> is the prior probability of <span class="math inline">\(\mu = \mu_i\)</span> and <span class="math inline">\(L(\mu_i)\)</span> is the likelihood function evaluated at <span class="math inline">\(\mu = \mu_i\)</span>.</p>
<p>If a discrete Uniform prior distribution for <span class="math inline">\(\mu\)</span> is assigned, one has <span class="math inline">\(\pi(\mu_i) = \frac{1}{8}\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span>, and <span class="math inline">\(\pi(\mu_i)\)</span> is canceled out from the numerator and denominator in Equation (8.5). In this case one calculates the likelihood values <span class="math inline">\(L(\mu_i)\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span> and normalizes these values to obtain the posterior probabilities <span class="math inline">\(\pi(\mu_i \mid y)\)</span>. Table 8.1 displays the values of <span class="math inline">\(\mu\)</span> and the corresponding values of Prior, Data/Likelihood, and Posterior. Readers are encouraged to verify the results shown in the table.</p>
<p>Table 8.1. Value, prior, data/likelihood and posterior for () with a single observation.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">()</th>
<th style="text-align: center;">Prior</th>
<th style="text-align: center;">Data/Likelihood</th>
<th style="text-align: center;">Posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0997</td>
<td style="text-align: center;">0.1888</td>
</tr>
<tr class="even">
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0972</td>
<td style="text-align: center;">0.1842</td>
</tr>
<tr class="odd">
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0891</td>
<td style="text-align: center;">0.1688</td>
</tr>
<tr class="even">
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0767</td>
<td style="text-align: center;">0.1452</td>
</tr>
<tr class="odd">
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0620</td>
<td style="text-align: center;">0.1174</td>
</tr>
<tr class="even">
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0471</td>
<td style="text-align: center;">0.0892</td>
</tr>
<tr class="odd">
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0336</td>
<td style="text-align: center;">0.0637</td>
</tr>
<tr class="even">
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0225</td>
<td style="text-align: center;">0.0427</td>
</tr>
</tbody>
</table>
<p>With the single measurement of time-to-serve of <span class="math inline">\(y = 15.1\)</span>, one sees from Table 8.1 that the posterior distribution for <span class="math inline">\(\mu\)</span> favors values <span class="math inline">\(\mu\)</span> = 15, and 16. In fact, the posterior probabilities decrease as a function of <span class="math inline">\(\mu\)</span>. The Prior column reminds us that the prior distribution is Uniform. Bayesian inference uses the collected data to sharpen one’s belief about the unknown parameter from the prior distribution to the posterior distribution. For this single observation, the sample mean is <span class="math inline">\(y = 15.1\)</span> and the <span class="math inline">\(\mu\)</span> value closest to the sample mean (<span class="math inline">\(\mu = 15\)</span>) is assigned the highest posterior probability.</p>
<p>Typically one collects multiple time-to-serve measurements. Suppose one collects <span class="math inline">\(n\)</span> time-to-serve measurements, denoted as <span class="math inline">\(Y_1, ..., Y_n\)</span>, that are Normally distributed with mean <span class="math inline">\(\mu\)</span> and fixed standard deviation <span class="math inline">\(\sigma = 4\)</span>. Each observation follows the same Normal density <span class="math display">\[\begin{equation}
f(y_i) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{\frac{-(y_i - \mu)^2}{2 \sigma^2}\right\}, -\infty &lt; y_i &lt; \infty.
\end{equation}\]</span> Again since <span class="math inline">\(\sigma = 4\)</span> is known, the only parameter in Equation (8.6) is <span class="math inline">\(\mu\)</span> and we are interested in learning about this mean parameter <span class="math inline">\(\mu\)</span>. Suppose the same discrete Uniform prior is used as in Equation (8.4) and graphed in Figure 8.2. The mean <span class="math inline">\(\mu\)</span> takes on the values <span class="math inline">\(\{15, 16, \cdots, 22\}\)</span> with each value assigned the same probability of <span class="math inline">\(\frac{1}{8}\)</span>.</p>
<p>Suppose one collects a sample of 20 times-to-serve for Federer:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fl">15.1</span> <span class="fl">11.8</span> <span class="fl">21.0</span> <span class="fl">22.7</span> <span class="fl">18.6</span> <span class="fl">16.2</span> <span class="fl">11.1</span> <span class="fl">13.2</span> <span class="fl">20.4</span> <span class="fl">19.2</span> </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fl">21.2</span> <span class="fl">14.3</span> <span class="fl">18.6</span> <span class="fl">16.8</span> <span class="fl">20.3</span> <span class="fl">19.9</span> <span class="fl">15.0</span> <span class="fl">13.4</span> <span class="fl">19.9</span> <span class="fl">15.3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When multiple time-to-serve measurements are taken, the likelihood function is the joint density of the actual observed values <span class="math inline">\(y_1, ..., y_n\)</span> viewed as a function of the mean <span class="math inline">\(\mu\)</span>. After some algebra (detailed derivation in Section 8.3.2), one writes the likelihood function as <span class="math display">\[\begin{eqnarray}
L(\mu) &amp; = &amp;\prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{1}{2 \sigma^2}(y_i - \mu)^2\right\} \nonumber \\
&amp; \propto &amp;\exp\left\{-\frac{n}{2 \sigma^2}(\bar y - \mu)^2\right\} \nonumber \\
&amp; = &amp; \exp\left\{-\frac{20}{2 (4)^2}(\bar y - \mu)^2\right\} ,
\end{eqnarray}\]</span> where we have substituted the known values <span class="math inline">\(n = 20\)</span> and the standard deviation <span class="math inline">\(\sigma = 4\)</span>. From our sample, we compute the sample mean <span class="math inline">\(\bar y = (15.1 + 11.8 + ... + 15.3) / 20 = 17.2\)</span>. The value of <span class="math inline">\(\bar y\)</span> is substituted into Equation (8.7), and for each possible value of <span class="math inline">\(\mu\)</span>, we substitute the value to find the corresponding likelihood. For example, the likelihood of <span class="math inline">\(\mu = 15\)</span> is equal to <span class="math display">\[\begin{align*}
L(15) &amp; = \exp\left\{-\frac{20}{2 (4)^2}(17.2 - 15)^2\right\} \nonumber \\
&amp; \approx 0.022.
\end{align*}\]</span> This calculation is repeated for each of the eight values <span class="math inline">\(\mu = 15, 16, ..., 22\)</span>, obtaining eight likelihood values.</p>
<p>One now applies Bayes’ rule to obtain the posterior distribution for <span class="math inline">\(\mu\)</span>. The posterior probability of <span class="math inline">\(\mu = \mu_i\)</span> given the sequence of recorded times-to-serve <span class="math inline">\(y_1, \cdots, y_n\)</span> has the form <span class="math display">\[\begin{equation}
\pi(\mu_i \mid y_1, \cdots, y_n) = \frac{\pi(\mu_i) \times L(\mu_i)}{\sum_j \pi(\mu_j) \times L(\mu_j)},
\end{equation}\]</span> where <span class="math inline">\(\pi(\mu_i)\)</span> is the prior probability of <span class="math inline">\(\mu = \mu_i\)</span> and <span class="math inline">\(L(\mu_i)\)</span> is the likelihood function evaluated at <span class="math inline">\(\mu = \mu_i\)</span>. We saw in equation @ref(eq:normaldiscretejointlikelihood) that only the sample mean, <span class="math inline">\(\bar{y}\)</span>, is needed in the calculation of the likelihood, so <span class="math inline">\(\bar{y}\)</span> is used in place of <span class="math inline">\(y_1, \cdots, y_n\)</span> in the formula.</p>
<p>With a discrete Uniform prior distribution for <span class="math inline">\(\mu\)</span>, again one has <span class="math inline">\(\pi(\mu_i) = \frac{1}{8}\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span> and <span class="math inline">\(\pi(\mu_i)\)</span> is canceled out from the numerator and denominator in Equation (8.8). One calculates the posterior probabilities by computing <span class="math inline">\(L(\mu_i)\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span> and normalizing these values. Table 8.2 displays the values of <span class="math inline">\(\mu\)</span> and the corresponding values of Prior, Data/Likelihood, and Posterior. Readers are encouraged to verify the results shown in the table.</p>
<p>Table 8.2. Value, prior, data/likelihood, and posterior for () with (n) observations.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">()</th>
<th style="text-align: center;">Prior</th>
<th style="text-align: center;">Data/Likelihood</th>
<th style="text-align: center;">Posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">15</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0217</td>
<td style="text-align: center;">0.0217</td>
</tr>
<tr class="even">
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.1813</td>
<td style="text-align: center;">0.1815</td>
</tr>
<tr class="odd">
<td style="text-align: center;">17</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.4350</td>
<td style="text-align: center;">0.4353</td>
</tr>
<tr class="even">
<td style="text-align: center;">18</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.2990</td>
<td style="text-align: center;">0.2992</td>
</tr>
<tr class="odd">
<td style="text-align: center;">19</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0589</td>
<td style="text-align: center;">0.0589</td>
</tr>
<tr class="even">
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0033</td>
<td style="text-align: center;">0.0033</td>
</tr>
<tr class="odd">
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0001</td>
</tr>
<tr class="even">
<td style="text-align: center;">22</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.0000</td>
<td style="text-align: center;">0.0000</td>
</tr>
</tbody>
</table>
<p>It is helpful to construct a graph (see Figure 8.3) where one contrasts the prior and probability probabilities for the mean time-to-serve <span class="math inline">\(\mu\)</span>. While the prior distribution is flat, the posterior distribution for <span class="math inline">\(\mu\)</span> favors the values <span class="math inline">\(\mu\)</span> = 16, 17, and 18 seconds. Bayesian inference uses the observed data to revise one’s belief about the unknown parameter from the prior distribution to the posterior distribution. Recall that the sample mean <span class="math inline">\(\bar{y}\)</span> = 17.2 seconds. From Table 8.2 and Figure 8.3 one sees the clear effect of the observed sample mean – <span class="math inline">\(\mu\)</span> is likely to be close to the value 17.2.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Prior and posterior probabilities of the Normal mean <span class="math inline">\(\mu\)</span> with a sample of observations.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="Normal:SamplingModel:derivation" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="Normal:SamplingModel:derivation"><span class="header-section-number">2.3.2</span> Simplification of the likelihood</h3>
<p>The likelihood function is the joint density of the observations <span class="math inline">\(y_1, ..., y_n\)</span>, viewed as a function of the mean <span class="math inline">\(\mu\)</span> (since <span class="math inline">\(\sigma=4\)</span> is given). With <span class="math inline">\(n\)</span> observations being <em>identically and independently distributed (i.i.d.)</em> as <span class="math inline">\({\rm{Normal}}({\mu, 4})\)</span>, the likelihood function is the product of Normal density terms. In the algebra work that will be done shortly, the likelihood, as a function of <span class="math inline">\(\mu\)</span>, is found to be Normal with mean <span class="math inline">\(\bar y\)</span> and standard deviation <span class="math inline">\(\sigma / \sqrt{n}\)</span>.</p>
<p>The calculation of the posterior probabilities is an application of Bayes’ rule illustrated in earlier chapters. One creates a data frame of values <code>mu</code> and corresponding probabilities <code>Prior</code>. One computes the likelihood values in the variable <code>Likelihood</code> and the posterior probabilities are found using the <code>bayesian_crank()</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">mu =</span> <span class="fu">seq</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="dv">1</span>),</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">Prior =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">8</span>, <span class="dv">8</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Likelihood =</span> <span class="fu">dnorm</span>(mu, <span class="fl">17.2</span>, <span class="dv">4</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">20</span>))) </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">bayesian_crank</span>(df) </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(df, <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  mu Prior Likelihood Product Posterior
1 15 0.125     0.0217  0.0027    0.0217
2 16 0.125     0.1813  0.0227    0.1815
3 17 0.125     0.4350  0.0544    0.4353
4 18 0.125     0.2990  0.0374    0.2992
5 19 0.125     0.0589  0.0074    0.0589
6 20 0.125     0.0033  0.0004    0.0033
7 21 0.125     0.0001  0.0000    0.0001
8 22 0.125     0.0000  0.0000    0.0000</code></pre>
</div>
</div>
<section id="derivation-of-lmu-propto-exp-left-fracn2-sigma2bary---mu2right" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="derivation-of-lmu-propto-exp-left-fracn2-sigma2bary---mu2right">Derivation of <span class="math inline">\(L(\mu) \propto \exp \left(-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right)\)</span></h4>
<p>In the following, we combine the terms in the exponent, expand all of the summation terms, and complete the square to get the result.</p>
<p><span class="math display">\[\begin{eqnarray}
L(\mu) &amp;=&amp;  \prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{1}{2 \sigma^2}(y_i - \mu)^2\right\}  \nonumber \\
       &amp;=&amp; \left(\frac{1}{\sqrt{2 \pi}\sigma}\right)^n \exp\left\{-\frac{1}{2 \sigma^2} \sum_{i=1}^{n} (y_i - \mu)^2\right\}\nonumber \\
&amp;\propto&amp; \exp \left\{ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} (y_i^2 - 2\mu y_i + \mu^2)\right\} \nonumber \\
\texttt{[expand the $\sum$ terms]} &amp;=&amp; \exp \left\{ -\frac{1}{2 \sigma^2} \left( \sum_{i=1}^{n} y_i^2 - 2\mu \sum_{i=1}^{n} y_i + n\mu^2 \right) \right\} \nonumber \\
&amp;\propto&amp; \exp \left\{- \frac{1}{2 \sigma^2} \left(-2 \mu \sum_{i=1}^{n} y_i + n \mu^2 \right) \right\}\nonumber \\
\texttt{[replace $\sum$ with $n\bar{y}$]} &amp;=&amp; \exp \left\{ - \frac{1}{2 \sigma^2} \left(-2 n \mu \bar{y} + n \mu^2 \right) \right\}\nonumber \\
\texttt{[complete the square]} &amp;=&amp; \exp \left\{ -\frac{n}{2 \sigma^2} (\mu^2 - 2\mu \bar{y} + \bar{y}^2) + \frac{n}{2 \sigma^2} \bar{y}^2\right\} \nonumber \\
&amp;\propto&amp; \exp \left\{-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right\}
\end{eqnarray}\]</span></p>
</section>
<section id="sufficient-statistic" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="sufficient-statistic">Sufficient statistic</h4>
<p>There are different ways of writing and simplifying the likelihood function. One can choose to keep the product sign and each <span class="math inline">\(y_i\)</span> term, and leave the likelihood function as <span class="math display">\[\begin{equation}
L(\mu) =  \prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{1}{2 \sigma^2}(y_i - \mu)^2\right\}.
\end{equation}\]</span> Doing so requires one to calculate the individual likelihood from each time-to-serve measurement <span class="math inline">\(y_i\)</span> and multiply these values to obtain the function <span class="math inline">\(L(\mu)\)</span> used to obtain the posterior probability.</p>
<p>If one instead simplifies the likelihood to be <span class="math display">\[\begin{equation}
L(\mu) \propto \exp \left\{-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right\},
\end{equation}\]</span> all the proportionality constants drop out in the calculation of the posterior probabilities for different values of <span class="math inline">\(\mu\)</span>. In the application of Bayes’ rule, one only needs to know the number of observations <span class="math inline">\(n\)</span> and the mean time to serve <span class="math inline">\(\bar{y}\)</span> to calculate the posterior. Since the likelihood function depends on the data only through the value <span class="math inline">\(\bar{y}\)</span>, the statistic <span class="math inline">\(\bar{y}\)</span> is called a <strong>sufficient statistic</strong> for the mean <span class="math inline">\(\mu\)</span>.</p>
</section>
</section>
<section id="Normal:SamplingModel:inference" class="level3" data-number="2.3.3">
<h3 data-number="2.3.3" class="anchored" data-anchor-id="Normal:SamplingModel:inference"><span class="header-section-number">2.3.3</span> Inference: Federer’s time-to-serve</h3>
<p>What has one learned about Federer’s mean time-to-serve from this Bayesian analysis? Our prior said that any of the eight possible values of <span class="math inline">\(\mu\)</span> were equally likely with probability <span class="math inline">\(0.125\)</span>. After observing the sample of 20 measurements, one believes <span class="math inline">\(\mu\)</span> is most likely <span class="math inline">\(16\)</span>, 17, and <span class="math inline">\(18\)</span> seconds, with respective probabilities <span class="math inline">\(0.181, 0.425\)</span>, and <span class="math inline">\(0.299\)</span>. In fact, if one adds up the posterior probabilities, one says that <span class="math inline">\(\mu\)</span> is in the set {16, 17, 18} seconds with probability <span class="math inline">\(0.915\)</span>. <span class="math display">\[\begin{eqnarray*}
Prob(16 \leq \mu \leq 18) = 0.181 + 0.435 + 0.299 = 0.915
\end{eqnarray*}\]</span> This region of values of <span class="math inline">\(\mu\)</span> is called a <span class="math inline">\(91.5\%\)</span> posterior probability region for the mean time-to-serve <span class="math inline">\(\mu\)</span>.</p>
</section>
</section>
<section id="Normal:Continuous" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="Normal:Continuous"><span class="header-section-number">2.4</span> Continuous Priors</h2>
<section id="Normal:Continuous:prior" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="Normal:Continuous:prior"><span class="header-section-number">2.4.1</span> The Normal prior for mean <span class="math inline">\(\mu\)</span></h3>
<p>Returning to our example, one is interested in learning about the time-to-serve for the tennis player Roger Federer. His serving times are believed to be Normally distributed with unknown mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma = 4\)</span>. The focus is on learning about the mean value <span class="math inline">\(\mu\)</span>.</p>
<p>In the prior construction in Section 8.3, we assumed <span class="math inline">\(\mu\)</span> was discrete, taking only integer values from <span class="math inline">\(15\)</span> to <span class="math inline">\(22\)</span>. However, the mean time-to-serve <span class="math inline">\(\mu\)</span> does not have to be an integer. In fact, it is more realistic to assume <span class="math inline">\(\mu\)</span> is continuous-valued. One widely-used approach for representing one’s belief about a Normal mean is based on a Normal prior density with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\sigma_0\)</span>, that is <span class="math display">\[\begin{eqnarray*}
\mu \sim {\rm{Normal}}(\mu_0, \sigma_0).
\end{eqnarray*}\]</span></p>
<p>There are two parameters for this Normal prior: the value <span class="math inline">\(\mu_0\)</span> represents one’s “best guess” at the mean time-to-serve <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma_0\)</span> indicates how sure one thinks about the guess.</p>
<p>To illustrate the use of different priors for <span class="math inline">\(\mu\)</span>, let’s consider the opinion of one tennis fan Joe who has strong prior information about the mean. His best guess at Federer’s mean time-to-serve is 18 seconds so he lets <span class="math inline">\(\mu_0 = 18\)</span>. He is very sure of this guess and so he chooses <span class="math inline">\(\sigma_0\)</span> to be the relatively small value of <span class="math inline">\(0.4\)</span>. In contrast, a second tennis fan Kate also thinks that Federer’s mean time-to-serve is 18 seconds, but does not have a strong belief in this guess and chooses the large value <span class="math inline">\(2\)</span> of the standard deviation <span class="math inline">\(\sigma_0\)</span>. Figure 8.4 shows these two Normal priors for the mean time-to-serve <span class="math inline">\(\mu\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Two priors for the Normal mean <span class="math inline">\(\mu\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Both curves are symmetric and bell-shaped, centered at <span class="math inline">\(\mu_0\)</span> = 18. The main difference is the spread of the two curves: a Normal(8, 0.4) curve is much more concentrated around the mean <span class="math inline">\(\mu_0\)</span> = 18 compared to the Normal(8, 2) curve. Since the value of the probability density function at a point reflects the probability at that value, the Normal(8, 0.4) prior reflects the belief that the mean time to serve will most likely be around <span class="math inline">\(\mu_0\)</span> = 18 seconds, whereas the Normal(8, 2) prior indicates that the mean <span class="math inline">\(\mu\)</span> could be as small as 15 seconds and as large as 20 seconds.</p>
</section>
<section id="Normal:Continuous:choosing" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="Normal:Continuous:choosing"><span class="header-section-number">2.4.2</span> Choosing a Normal prior</h3>
<section id="informative-prior" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="informative-prior">Informative prior</h4>
<p>How does one in practice choose a Normal prior for <span class="math inline">\(\mu\)</span> that reflects prior beliefs about the location of this parameter? One indirect strategy for choosing for selecting values of the prior parameters <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\sigma_0\)</span> is based on the specification of quantiles. On the basis of one’s prior beliefs, one specifies two quantiles of the Normal density. Then the Normal parameters are found by matching these two quantiles to a particular Normal curve.</p>
<p>Recall the definition of a quantile — in this setting it is a value of the mean <span class="math inline">\(\mu\)</span> such that the probability of being smaller than that value is a given probability. To construct one’s prior for Federer’s mean time-to-serve, one thinks first about two quantiles. Suppose one specifies the 0.5 quantile to be 18 seconds — this means that <span class="math inline">\(\mu\)</span> is equally likely to be smaller or larger than 18 seconds. Next, one decides that the 0.9 quantile is 20 seconds. This means that one’s probability that <span class="math inline">\(\mu\)</span> is smaller than 20 seconds is 90%. Given values of these two quantiles, the unique Normal curve is found that matches this information.</p>
<p>The matching is performed by the R function <code>normal.select()</code>. One inputs two quantiles by ```list} statements, and the output is the mean and standard deviation of the Normal prior.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">normal.select</span>(<span class="fu">list</span>(<span class="at">p =</span> <span class="fl">0.5</span>, <span class="at">x =</span> <span class="dv">18</span>), <span class="fu">list</span>(<span class="at">p =</span> <span class="fl">0.9</span>, <span class="at">x =</span> <span class="dv">20</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$mu
[1] 18

$sigma
[1] 1.560608</code></pre>
</div>
</div>
<p>The Normal curve with mean <span class="math inline">\(\mu_0 = 18\)</span> and <span class="math inline">\(\sigma_0 = 1.56\)</span>, displayed in Figure 8.5, matches the prior information stated by the two quantiles.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">A person’s Normal prior for Federer’s mean time-to-serve <span class="math inline">\(\mu\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Since our measurement skills are limited, this prior is just an approximation to one’s beliefs about <span class="math inline">\(\mu\)</span>. We recommend in practice that one perform several checks to see if this Normal prior makes sense. Several functions are available to help in this prior checking.</p>
<p>For example, one finds the 0.25 quantile of our prior using the <code>qnorm()</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.25</span>, <span class="dv">18</span>, <span class="fl">1.56</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 16.9478</code></pre>
</div>
</div>
<p>This prior says that the prior probability that <span class="math inline">\(\mu\)</span> is smaller than 16.95 is 25%. If this does not seem reasonable, one would make adjustments in the values of the Normal mean and standard deviation until a reasonable Normal prior is found.</p>
</section>
<section id="weekly-informative-prior" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="weekly-informative-prior">Weekly informative prior</h4>
<p>We have been assuming that one has some information about the mean parameter <span class="math inline">\(\mu\)</span> that is represented by a Normal prior. What would a user do in the situation where little is known about the location on <span class="math inline">\(\mu\)</span>? For a Normal prior, the standard deviation <span class="math inline">\(\sigma_0\)</span> represents the sureness of one’s belief in one’s guess <span class="math inline">\(\mu_0\)</span> at the value of the mean. If one is really unsure about any guess at <span class="math inline">\(\mu\)</span>, then one assigns the standard deviation <span class="math inline">\(\sigma_0\)</span> a large value. Then the choice of the prior mean will not matter, so we suggest using a Normal(0, <span class="math inline">\(\sigma_0\)</span>) with a large value for <span class="math inline">\(\sigma_0\)</span>. This prior indicates that <span class="math inline">\(\mu\)</span> may plausibly range over a large interval and represents weakly informative prior belief about the parameter.</p>
<p>As will be seen later in this chapter, when a vague prior is chosen, the posterior inference for <span class="math inline">\(\mu\)</span> will largely be driven by the data. This behavior is desirable since this person knows little about the location of <span class="math inline">\(\mu\)</span> <em>a priori</em> in this situation and wants the data to inform about the location of <span class="math inline">\(\mu\)</span> with little influence by the prior.</p>
</section>
</section>
</section>
<section id="Normal:ContinuousUpdate" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="Normal:ContinuousUpdate"><span class="header-section-number">2.5</span> Updating the Normal Prior</h2>
<section id="introduction-1" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">2.5.1</span> Introduction</h3>
<p>Continuing our discussion on learning about the mean time-to-serve for Roger Federer, the current prior beliefs about Federer’s mean time-to-serve <span class="math inline">\(\mu\)</span> are represented by a Normal curve with mean <span class="math inline">\(18\)</span> seconds and standard deviation 1.56 seconds.</p>
<p>Next some data is collected — Federer’s time-to-serves are recorded for 20 serves and the sample mean is <span class="math inline">\(17.2\)</span> seconds. Recall that we are assuming the population standard deviation <span class="math inline">\(\sigma = 4\)</span> seconds. The likelihood is given by <span class="math display">\[\begin{equation}
L(\mu) \propto \exp \left\{-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right\},
\end{equation}\]</span> and with substitution of the values <span class="math inline">\(\bar y = 17.2\)</span>, <span class="math inline">\(n = 20\)</span>, and <span class="math inline">\(\sigma = 4\)</span>, we obtain <span class="math display">\[\begin{eqnarray}
L(\mu) &amp;\propto&amp; \exp \left\{-\frac{20}{2 (4)^2}(17.2 - \mu)^2\right\} \nonumber \\
&amp;=&amp; \exp \left\{-\frac{1}{2(4/\sqrt{20})^2}(\mu - 17.2)^2\right\}.
\end{eqnarray}\]</span> Viewing the likelihood as a function of the parameter <span class="math inline">\(\mu\)</span> as in Equation (8.13), the likelihood is recognized as a Normal density with mean <span class="math inline">\(\bar y = 17.2\)</span> and standard deviation <span class="math inline">\(\sigma / \sqrt{n} = 4 / \sqrt{20} = 0.89\)</span>.</p>
<p>The Bayes’ rule calculation is very familiar to the reader — one obtains the posterior density curve by multiplying the Normal prior by the likelihood. If one writes down the product of the Normal likelihood and the Normal prior density and works through some messy algebra, one will discover that the posterior density also has the Normal density form.</p>
<p>The Normal prior is said to be <em>conjugate</em> since the prior and posterior densities come from the same distribution family: Normal. To be more specific, suppose the observation has a Normal sampling density with unknown mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma\)</span>. If one specifies a Normal prior for the unknown mean <span class="math inline">\(\mu\)</span> with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\sigma_0\)</span>, one obtains a Normal posterior for <span class="math inline">\(\mu\)</span> with updated parameters <span class="math inline">\(\mu_n\)</span> and <span class="math inline">\(\sigma_n\)</span>.</p>
<p>In Section 8.5.2, we provide a quick peak at this posterior updating without worrying about the mathematical derivation and Section 8.5.3 describes the details of the Bayes’ rule calculation. Section 8.5.4 looks at the conjugacy more closely and provides some insight on the effects of prior and likelihood on the posterior distribution.</p>
</section>
<section id="Normal:ContinuousUpdate:Overview" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="Normal:ContinuousUpdate:Overview"><span class="header-section-number">2.5.2</span> A quick peak at the update procedure</h3>
<p>It is convenient to describe the updating procedure by use of a table. In Table 8.3, there are rows corresponding to Prior, Data/Likelihood, and Posterior and columns corresponding to Mean, Precision, and Standard Deviation. The mean and standard deviation of the Normal prior are placed in the “Prior” row, and the sample mean and standard error are placed in the “Data/Likelihood” row.</p>
<p>Table 8.3. Updating the Normal prior: step 1.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Precision</th>
<th style="text-align: right;">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Prior</td>
<td style="text-align: right;">18.00</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1.56</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data/Likelihood</td>
<td style="text-align: right;">17.20</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">0.89</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Posterior</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>We define the <em>precision</em>, <span class="math inline">\(\phi\)</span>, to be the reciprocal of the square of the standard deviation. We compute the precisions of the prior and data from the given standard deviations: <span class="math display">\[\begin{equation*}
\phi_{prior} = \frac{1}{\sigma_0^2} = \frac{1}{1.56^2} = 0.41, \, \, \,
\phi_{data} = \frac{1}{\sigma^2 / n} = \frac{1}{0.89^2} = 1.26.
\end{equation*}\]</span> We enter the precisions in the corresponding rows of Table 8.4 <span class="math inline">\(\ref{table:normalupdate2}\)</span>.</p>
<p>Table 8.4. Updating the Normal prior: step 2.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Precision</th>
<th style="text-align: right;">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Prior</td>
<td style="text-align: right;">18.00</td>
<td style="text-align: right;">0.41</td>
<td style="text-align: right;">1.56</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data/Likelihood</td>
<td style="text-align: right;">17.20</td>
<td style="text-align: right;">1.26</td>
<td style="text-align: right;">0.89</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Posterior</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>We will shortly see that the Posterior precision is the sum of the Prior precision and the Data/Likelihood precisions:<br>
<span class="math display">\[\begin{equation*}
\phi_{post} = \phi_{prior} + \phi_{data} = 0.41 + 1.26 = 1.67.
\end{equation*}\]</span> Once the posterior precision is computed, the posterior standard deviation is computed as the reciprocal of the square root of the precision. <span class="math display">\[\begin{equation*}
\sigma_n = \frac{1}{\sqrt{\phi_{post}}} = \frac{1}{\sqrt{1.67}} = 0.77.
\end{equation*}\]</span> These precisions and standard deviations are entered into Table 8.5.</p>
<p>Table 8.5. Updating the Normal prior: step 3.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Precision</th>
<th style="text-align: right;">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Prior</td>
<td style="text-align: right;">18.00</td>
<td style="text-align: right;">0.41</td>
<td style="text-align: right;">1.56</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data/Likelihood</td>
<td style="text-align: right;">17.20</td>
<td style="text-align: right;">1.26</td>
<td style="text-align: right;">0.89</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Posterior</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1.67</td>
<td style="text-align: right;">0.77</td>
</tr>
</tbody>
</table>
<p>The posterior mean is a weighted average of the Prior and Data/Likelihood means where the weights are given by the corresponding precisions. That is, the formula is given by <span class="math display">\[\begin{eqnarray}
\mu_n = \frac{\phi_{prior} \times \mu_0 + \phi_{data} \times \bar y}{\phi_{prior} + \phi_{data}}.
\end{eqnarray}\]</span> By making appropriate substitutions, we obtain the posterior mean: <span class="math display">\[\begin{eqnarray*}
\mu_n = \frac{0.41 \times 18.00 + 1.26 \times 17.20}{0.41 + 1.26} = 17.40.
\end{eqnarray*}\]</span> The posterior density is Normal with mean <span class="math inline">\(17.40\)</span> seconds and standard deviation <span class="math inline">\(0.77\)</span> seconds. See Table 8.6 for the final update step.</p>
<p>Table 8.6. Updating the Normal prior: step 4.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Precision</th>
<th style="text-align: right;">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Prior</td>
<td style="text-align: right;">18.00</td>
<td style="text-align: right;">0.41</td>
<td style="text-align: right;">1.56</td>
</tr>
<tr class="even">
<td style="text-align: left;">Data/Likelihood</td>
<td style="text-align: right;">17.20</td>
<td style="text-align: right;">1.26</td>
<td style="text-align: right;">0.89</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Posterior</td>
<td style="text-align: right;">17.40</td>
<td style="text-align: right;">1.67</td>
<td style="text-align: right;">0.77</td>
</tr>
</tbody>
</table>
<p>The Normal updating is performed by the R function <code>normal_update()</code>. One inputs two vectors – <code>prior</code> is a vector of the prior mean and standard deviation and <code>data</code> is a vector of the sample mean and standard error. The output is a vector of the posterior mean and posterior standard deviation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>prior <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">18</span>, <span class="fl">1.56</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">17.20</span>, <span class="fl">0.89</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">normal_update</span>(prior, data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 17.3964473  0.7730412</code></pre>
</div>
</div>
<p>The prior and posterior densities are displayed in Figure 8.6. As usually the case, the posterior density has a smaller spread since the posterior has more information than the prior about Federer’s mean time-to-serve. More information about a parameter indicates less uncertainty and a smaller spread of the posterior density. In the process from prior to posterior, one sees how the data modifies one’s initial belief about the parameter <span class="math inline">\(\mu\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter8/normalpriorpost.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Prior and posterior curves for Federer’s mean time-to-serve <span class="math inline">\(\mu\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="Normal:ContinuousUpdate:BayesRule" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="Normal:ContinuousUpdate:BayesRule"><span class="header-section-number">2.5.3</span> Bayes’ rule calculation</h3>
<p>Section 8.5.2 gave an overview of the updating procedure for a Normal prior and Normal sampling. In this section we explain (1) why it is preferable to work with the precisions instead of the standard deviations; (2) why the precisions act as the weights in the calculation of the posterior mean and (3) why the posterior is a Normal distribution.</p>
<p>Recall a precision is the reciprocal of the square of the standard deviation. We use <span class="math inline">\(\phi = \frac{1}{\sigma^2}\)</span> to represent the precision of a single observation in the Normal data/likelihood, and <span class="math inline">\(\phi_0 = \frac{1}{\sigma_0^2}\)</span> to represent the precision in the Normal prior.</p>
<ul>
<li>We write down the likelihood of <span class="math inline">\(\mu\)</span>, combining terms, and writing the expression in terms of the precision <span class="math inline">\(\phi\)</span>. <span class="math display">\[\begin{eqnarray}
y_1, \cdots, y_n \mid \mu, \sigma &amp;\overset{i.i.d.}{\sim}&amp; {\rm{Normal}}(\mu, \sigma)\\% \equiv {\rm{Normal}}(\mu, \frac{1}{\phi}) \\
\end{eqnarray}\]</span> <span class="math display">\[\begin{eqnarray}
L(\mu) = f(y_1, \cdots, y_n \mid \mu, \sigma) &amp;=&amp; \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(y_i-\mu)^2\right\} \nonumber \\
&amp;=&amp; \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}}\phi^{\frac{1}{2}} \exp\left\{-\frac{\phi}{2}(y_i - \mu)^2)\right\}\nonumber \\
&amp;=&amp; \left(\frac{1}{\sqrt{2\pi}}\right)^n \phi^{\frac{n}{2}} \exp\left\{-\frac{\phi}{2}\sum_{i=1}^{n}(y_i - \mu)^2)\right\}\nonumber \\
\end{eqnarray}\]</span></li>
</ul>
<p>Note that <span class="math inline">\(\sigma\)</span> is assumed known, therefore the likelihood function is only in terms of <span class="math inline">\(\mu\)</span>, i.e.&nbsp;<span class="math inline">\(L(\mu)\)</span>.</p>
<ul>
<li><p>In similar fashion, we write down the prior density for <span class="math inline">\(\mu\)</span> including the prior precision <span class="math inline">\(\phi_0\)</span>. <span class="math display">\[\begin{eqnarray}
\mu  &amp;\sim&amp; {\rm{Normal}}(\mu_0, \sigma_0) \\%\equiv {\rm{Normal}}(\mu_0, \frac{1}{\phi_0}) \\
\end{eqnarray}\]</span> <span class="math display">\[\begin{eqnarray}
\pi(\mu) &amp;=&amp; \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left\{-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2)\right\}\nonumber \\
&amp;=&amp; \frac{1}{\sqrt{2\pi}}\phi_0^{\frac{1}{2}} \exp\left\{-\frac{\phi_0}{2}(\mu - \mu_0)^2\right\}
\end{eqnarray}\]</span></p></li>
<li><p>Bayes’ rule is applied by multiplying the prior by the likelihood to obtain the posterior. In deriving the posterior of <span class="math inline">\(\mu\)</span>, the manipulations require careful consideration regarding what is known. The only unknown variable is <span class="math inline">\(\mu\)</span>, so any “constants” or known quantities not depending on <span class="math inline">\(\mu\)</span> can be dropped/added with the proportionality sign “<span class="math inline">\(\propto\)</span>”.</p></li>
</ul>
<p><span class="math display">\[\begin{eqnarray}
\pi(\mu \mid y_1, \cdots, y_n, \sigma) &amp;\propto&amp;  \pi(\mu) L(\mu) \nonumber \\
&amp;\propto&amp; \exp\left\{-\frac{\phi_0}{2}(\mu - \mu_0)^2\right\} \times \exp\left\{-\frac{n\phi}{2}(\mu - \bar{y})^2\right\} \nonumber \\
&amp;\propto&amp; \exp\left\{-\frac{1}{2}(\phi_0 +n\phi)\mu^2 + \frac{1}{2}(2\mu_0\phi_0 + 2n\phi\bar{y})\mu\right\} \nonumber \\
\texttt{[complete the square]} &amp;\propto&amp; \exp\left\{-\frac{1}{2}(\phi_0 + n\phi)(\mu - \frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi})^2\right\} \\
\end{eqnarray}\]</span></p>
<p>Looking closely at the final expression, one recognizes that the posterior for <span class="math inline">\(\mu\)</span> is a Normal density with mean and precision parameters. Specifically we recognize <span class="math inline">\((\phi_0 + n \phi)\)</span> as the posterior precision and <span class="math inline">\((\frac{\phi_0 \mu_0 + n \phi\bar{y}}{\phi_0 + n \phi})\)</span> as the posterior mean. Summarizing, we have derived the following posterior distribution of <span class="math inline">\(\mu\)</span>,</p>
<p><span class="math display">\[\begin{eqnarray}
\mu \mid y_1, \cdots, y_n, \sigma \sim {\rm{Normal}}\left(\frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi}, \sqrt{\frac{1}{\phi_0 + n \phi}}\right).
\end{eqnarray}\]</span></p>
<p>In passing, it should be noted that the same result would be attained using the standard deviations, <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\sigma_0\)</span>, instead of the precisions, <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\phi_0\)</span>. It is preferable to work with the precisions due to the relative simplicity of the notation. In particular, one sees in Table Table 8.5 that the posterior precision is the sum of the prior and data/likelihood precisions, that is, the posterior precision <span class="math inline">\(\phi_n = \phi_0 + n \phi\)</span>.</p>
</section>
<section id="Normal:ContinuousUpdate:Conjugate" class="level3" data-number="2.5.4">
<h3 data-number="2.5.4" class="anchored" data-anchor-id="Normal:ContinuousUpdate:Conjugate"><span class="header-section-number">2.5.4</span> Conjugate Normal prior</h3>
<p>Let’s summarize our calculations in Section 8.5.3. We collect a sequence of continuous observations that are assumed identically and independently distributed as <span class="math inline">\(\textrm{Normal}(\mu, \sigma)\)</span>, and a Normal prior is assigned to the mean parameter <span class="math inline">\(\mu\)</span>.</p>
<ul>
<li><p>The sampling model: <span class="math display">\[\begin{eqnarray}
Y_1, \cdots, Y_n \mid \mu, \sigma &amp;\overset{i.i.d.}{\sim}&amp; {\rm{Normal}}(\mu, \sigma)
\end{eqnarray}\]</span> When <span class="math inline">\(\sigma\)</span> (or <span class="math inline">\(\phi\)</span>) is known, and mean <span class="math inline">\(\mu\)</span> is the only parameter in the likelihood.</p></li>
<li><p>The prior distribution: <span class="math display">\[\begin{eqnarray}
\mu  &amp;\sim&amp; {\rm{Normal}}(\mu_0, \sigma_0)
\end{eqnarray}\]</span></p></li>
<li><p>After <span class="math inline">\(Y_1 = y_1, ..., Y_n = y_n\)</span> are observed, the posterior distribution for the mean <span class="math inline">\(\mu\)</span> is another Normal distribution with mean <span class="math inline">\(\frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi}\)</span> and precision <span class="math inline">\(\phi_0 + n \phi\)</span> (thus standard deviation <span class="math inline">\(\sqrt{\frac{1}{\phi_0 + n \phi}}\)</span>):</p></li>
</ul>
<p><span class="math display">\[\begin{eqnarray}
\mu \mid y_1, \cdots, y_n, \sigma \sim {\rm{Normal}}\left(\frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi}, \sqrt{\frac{1}{\phi_0 + n \phi}}\right).
\end{eqnarray}\]</span></p>
<p>In this situation where the sampling standard deviation <span class="math inline">\(\sigma\)</span> is known, the Normal density is a conjugate prior for the mean of a Normal distribution, as the posterior distribution for <span class="math inline">\(\mu\)</span> is another Normal density with updated parameters. Conjugacy is a convenient property as the posterior distribution for <span class="math inline">\(\mu\)</span> has a convenient functional form. Conjugacy allows one to conduct Bayesian inference through exact analytical solutions and simulation. Also conjugacy provides insight on how the data and prior are combined in the posterior distribution.</p>
<section id="the-posterior-compromises-between-the-prior-and-the-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-posterior-compromises-between-the-prior-and-the-sample">The posterior compromises between the prior and the sample</h4>
<p>Recall that Bayesian inference is a general approach where one initializes a prior belief for an unknown quantity, collects data expressed through a likelihood function, and combines prior and likelihood to give an updated belief for the unknown quantity. In Chapter 7, we have seen how the posterior mean of a proportion is a compromise between the prior mean and sample proportion (refer to Section 7.4.2 as needed). In the current Normal mean case, the posterior mean is similarly viewed as an estimate that compromises between the prior mean and sample mean. One rewrites the posterior mean in Equation (8.23) as follows: <span class="math display">\[\begin{eqnarray}
\mu_n = \frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi} &amp;=&amp; \frac{\phi_0}{\phi_0 + n\phi} \mu_0 +  
\frac{n\phi}{\phi_0 + n\phi}  \bar{y}.
\end{eqnarray}\]</span> The prior precision is equal to <span class="math inline">\(\phi_0\)</span> and the precision in the likelihood for any <span class="math inline">\(y_i\)</span> is <span class="math inline">\(\phi\)</span>. Since there are <span class="math inline">\(n\)</span> observations, the precision in the joint likelihood is <span class="math inline">\(n\phi\)</span>. The posterior mean is a weighted average of the prior mean <span class="math inline">\(\mu_0\)</span> and sample mean <span class="math inline">\(\bar y\)</span> where the weights are proportional to the associated precisions.</p>
</section>
<section id="the-posterior-accumulates-information-in-the-prior-and-the-sample" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="the-posterior-accumulates-information-in-the-prior-and-the-sample">The posterior accumulates information in the prior and the sample</h4>
<p>In addition, the precision of the posterior Normal mean is the sum of the precisions of the prior and likelihood. That is, <span class="math display">\[\begin{equation}
\phi_n = \phi_0 + n \phi.
\end{equation}\]</span> The implication is that the posterior standard deviation will always be smaller than either the prior standard deviation or the sampling standard error: <span class="math display">\[\begin{equation*}
\sigma_n &lt; \sigma_0, \, \, \, \sigma_n &lt; \frac{\sigma}{\sqrt{n}}.
\end{equation*}\]</span></p>
</section>
</section>
</section>
<section id="Normal:ContinuousInference" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="Normal:ContinuousInference"><span class="header-section-number">2.6</span> Bayesian Inferences for Continuous Normal Mean</h2>
<p>Continuing with the example about Federer’s time-to-serve, our Normal prior had mean 18 seconds and standard deviation 1.56 seconds. After collecting 20 time-to-serve measurements with a sample mean of 17.2, the posterior distribution <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span> reflects our opinion about the mean time-to-serve.</p>
<p>Bayesian inferences about the mean <span class="math inline">\(\mu\)</span> are based on various summaries of this posterior Normal distribution. Because the exact posterior distribution of mean <span class="math inline">\(\mu\)</span> is Normal, it is convenient to use R functions such as <code>pnorm()</code> and <code>qnorm()</code> to conduct Bayesian hypothesis testing and construct Bayesian credible intervals. Simulation-based methods utilizing functions such as <code>rnorm()</code> are also useful to provide approximations to those inferences. A sequence of examples are given in Section 8.6.1.</p>
<p>Predictions of future data are also of interest. For example, one might want to predict the next time-to-serve measurement based on the posterior distribution of <span class="math inline">\(\mu\)</span> being <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span>. In Section 8.6.2, details of the prediction procedure and examples are provided.</p>
<section id="Normal:ContinuousInference:HTandCI" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="Normal:ContinuousInference:HTandCI"><span class="header-section-number">2.6.1</span> Bayesian hypothesis testing and credible interval</h3>
<section id="a-testing-problem" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a-testing-problem">A testing problem</h4>
<p>In a <em>testing</em> problem, one is interested in checking the validity of a statement about a population quantity. In our tennis example, suppose someone says that Federer takes on average at least 19 seconds to serve. Is this a reasonable statement?</p>
<p>The current beliefs about Federer’s mean time-to-serve are summarized by a Normal distribution with mean 17.4 seconds and standard deviation 0.77 seconds. To assess if the statement ``<span class="math inline">\(\mu\)</span> is 19 seconds or more” is reasonable, one simply computes its posterior probability, <span class="math inline">\(Prob(\mu \geq 19 \mid \mu_n = 17.4, \sigma_n = 0.77)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pnorm</span>(<span class="dv">19</span>, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.01885827</code></pre>
</div>
</div>
<p>This probability is about 0.019, a small value, so one would conclude that this person’s statement is unlikely to be true.</p>
<p>This is the exact solution using the <code>pnorm()</code> function with mean 17.4 and standard deviation 0.77. As seen in Chapter 7, simulation provides an alternative approach to obtaining the probability <span class="math inline">\(Prob(\mu \geq 19 \mid \mu_n = 17.4, \sigma_n = 0.77)\)</span>. To implement the simulation approach, recall that one generates a large number of values from the posterior distribution and summarizes this simulated sample. In particular, using the following R script, one generates 1000 values from the <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span> distribution and approximates the probability of “<span class="math inline">\(\mu\)</span> is 19 seconds or more” by computing the percentage of values that falls above 19.</p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>NormalSamples <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(NormalSamples <span class="sc">&gt;=</span> <span class="dv">19</span>) <span class="sc">/</span> S</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.024</code></pre>
</div>
</div>
<p>The reader might notice that the approximated value of 0.024 differs from the exact answer of 0.019 using the <code>pnorm()</code> function. One way to improve the accuracy of the approximation is by increasing the number of simulated values. For example, increasing S from 1000 to 10,000 provides a better approximation to the exact probability 0.019.</p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>NormalSamples <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(NormalSamples <span class="sc">&gt;=</span> <span class="dv">19</span>) <span class="sc">/</span> S</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0175</code></pre>
</div>
</div>
</section>
<section id="a-bayesian-interval-estimate" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="a-bayesian-interval-estimate">A Bayesian interval estimate</h4>
<p>Bayesian credible intervals for the mean parameter <span class="math inline">\(\mu\)</span> can be achieved both by exact calculation and simulation. Recall that a Bayesian credible interval is an interval that contains the unknown parameter with a certain probability content. For example, a 90% Bayesian credible interval for the parameter <span class="math inline">\(\mu\)</span> is an interval containing <span class="math inline">\(\mu\)</span> with a probability of 0.90.</p>
<p>The exact interval is obtained by using the R function <code>qnorm()</code>. For example, with the posterior distribution for <span class="math inline">\(\mu\)</span> being <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span>, the following R script shows that a 90% central Bayesian credible interval is (16.133, 18.667). That is, the posterior probability of <span class="math inline">\(\mu\)</span> falls between 16.133 and 18.667 is exactly 90%.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>), <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 16.13346 18.66654</code></pre>
</div>
</div>
<p>For simulation-based inference, one generates a large number of values from its posterior distribution, then finds the 5th and 95th sample quantiles to obtain the middle 90% of the generated values. Below one sees that a 90% credible interval for posterior of <span class="math inline">\(\mu\)</span> is approximately (16.151, 18.691).</p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>NormalSamples <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(NormalSamples, <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      5%      95% 
16.15061 18.69062 </code></pre>
</div>
</div>
<p>The Bayesian credible intervals can also be used for testing hypothesis. Suppose one again wants to evaluate the statement “Federer takes on average at least 19 seconds to serve.” One answers this question by computing the 90% credible interval. One notes that the values of <span class="math inline">\(\mu\)</span> ``at least 19” are not included in the exact 90% credible interval (16.15, 18.69). The interpretation is that the probability is at least 0.90 that Federer’s average time-to-service is smaller than 19 seconds. One could obtain a wider credible interval, say by computing a central 95% credible interval (see the R output below), and observe that 19 is out of the interval. This indicates we are 95% confident that 19 seconds is not the value of Federer’s average time-to-serve.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 15.89083 18.90917</code></pre>
</div>
</div>
<p>On the basis of this credible interval calculation, one concludes that the statement about Federer’s time-to-serve is unlikely to be true. This conclusion is consistent with the typical Bayesian hypothesis testing procedure given at the beginning of this section.</p>
</section>
</section>
<section id="Normal:ContinuousInference:Prediction" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="Normal:ContinuousInference:Prediction"><span class="header-section-number">2.6.2</span> Bayesian prediction</h3>
<p>Suppose one is interested in predicting Federer’s future time-to-serve. Since one has already updated the belief about the parameter, the mean <span class="math inline">\(\mu\)</span>, the prediction is made based on its posterior predictive distribution.</p>
<p>How to make one future prediction of Federer’s time-to-serve? In Chapter 7, we have seen two different approaches for predicting of a new survey outcome of students’ dining preferences. One approach in Chapter 7 is based on the derivation of the exact posterior predictive distribution <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid Y = y)\)</span> which was shown to be a Beta-Binomial distribution. The second approach is a simulation-based approach, which involves two steps: first, sample a value of the parameter from its posterior distribution (a Beta distribution), and second, sample a prediction from the data model based on the sampled parameter draw (a Binomial distribution). When the sample size in the simulation-based approach is sufficiently large, a prediction interval from the simulation-based approach is an accurate approximation to the exact prediction interval.</p>
<section id="exact-predictive-distribution" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="exact-predictive-distribution">Exact predictive distribution</h4>
<p>We first describe the exact posterior predictive distribution. Consider making a prediction of a single Federer’s time-to-serve <span class="math inline">\(\tilde{Y}\)</span>. In general, suppose the sampling density of <span class="math inline">\(\tilde{Y}\)</span> given <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> is <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid \mu)\)</span> and suppose the current beliefs about <span class="math inline">\(\mu\)</span> are represented by the density <span class="math inline">\(\pi(\mu)\)</span>. The joint density of <span class="math inline">\((\tilde{y}, \mu)\)</span> is given by the product <span class="math display">\[\begin{equation}
f(\tilde{Y} = \tilde{y}, \mu) = f(\tilde{Y} = \tilde{y} \mid \mu) \pi(\mu),
\end{equation}\]</span> and by integrating out <span class="math inline">\(\mu\)</span>, the predictive density of <span class="math inline">\(\tilde{Y}\)</span> is given by <span class="math display">\[\begin{equation}
f(\tilde{Y} = \tilde{y}) = \int f(\tilde{Y} = \tilde{y} \mid \mu) \pi(\mu) d\mu.
\end{equation}\]</span></p>
<p>The computation of the predictive density is possible for this Normal sampling model with a Normal prior. It is assumed that <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid \mu)\)</span> is Normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> and that the current beliefs about <span class="math inline">\(\mu\)</span> are described by a Normal density with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\sigma_0\)</span>. Then it is possible to integrate out <span class="math inline">\(\mu\)</span> from the joint density of <span class="math inline">\((\tilde{y}, \mu)\)</span> and one finds that the predictive density for <span class="math inline">\(\tilde{Y}\)</span> is Normal with mean and standard deviation given by <span class="math display">\[\begin{equation}
E(\tilde{Y}) = \mu_0, \, \, SD(\tilde{Y}) = \sqrt{\sigma^2 + \sigma_0^2}.
\end{equation}\]</span></p>
<p>This result can be used to derive the posterior predictive distribution of <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid Y_1, \cdots, Y_n)\)</span>, where <span class="math inline">\(\tilde{Y}\)</span> is a future observation and <span class="math inline">\(Y_1, \cdots, Y_n\)</span> are <span class="math inline">\(n\)</span> <span class="math inline">\(i.i.d.\)</span> observations from a Normal sampling density with unknown mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma\)</span>. After observing the sample values <span class="math inline">\(y_1, \cdots, y_n\)</span>, the current beliefs about the mean <span class="math inline">\(\mu\)</span> are represented by a Normal<span class="math inline">\((\mu_n, \sigma_n)\)</span> density, where the mean and standard deviation are given by <span class="math display">\[\begin{equation}
\mu_n = \frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi},  \sigma_n = \sqrt{\frac{1}{\phi_0 + n \phi}}.
\end{equation}\]</span> Then by applying our general result in Equation (8.28), the posterior predictive density of the single future observation <span class="math inline">\(\tilde{Y}\)</span> is Normal with mean <span class="math inline">\(\mu_n\)</span> and standard deviation <span class="math inline">\(\sqrt{\sigma^2 + \sigma_n^2}.\)</span> That is, <span class="math display">\[\begin{eqnarray}
\tilde{Y} = \tilde{y} \mid y_1, \cdots, y_n, \sigma \sim \textrm{Normal}(\mu_n, \sqrt{\sigma^2 + \sigma_n^2}).
\end{eqnarray}\]</span></p>
<p>An important aspect of the predictive distribution for <span class="math inline">\(\tilde{Y}\)</span> is on the variance term <span class="math inline">\(\sigma^2 + \sigma_n^2\)</span>. The variability of a future prediction comes from two sources: (1) the data model variance <span class="math inline">\(\sigma^2\)</span>, and (2) the posterior variance <span class="math inline">\(\sigma_n^2\)</span>. Recall that the posterior variance <span class="math inline">\(\sigma_n^2 = \frac{1}{\phi_0 + n\phi}\)</span>. If one fixes values of <span class="math inline">\(\phi_0\)</span> and <span class="math inline">\(\phi\)</span> and allow the sample size <span class="math inline">\(n\)</span> to grow, the posterior variance will approach zero. In this ``large <span class="math inline">\(n\)</span>” case, the uncertainty in inference about the population mean <span class="math inline">\(\mu\)</span> will decrease – essentially we are certain about the location of <span class="math inline">\(\mu\)</span>. However the uncertainty in prediction will not decrease towards zero. In contrast, in this large sample case, the variance of <span class="math inline">\(\tilde{Y}\)</span> will decrease and approach the sampling variance <span class="math inline">\(\sigma^2\)</span>.</p>
</section>
<section id="predictions-by-simulation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="predictions-by-simulation">Predictions by simulation</h4>
<p>The alternative method of computing the predictive distribution is by simulation. In this setting, there are two unknowns – the mean parameter <span class="math inline">\(\mu\)</span> and the future observation <span class="math inline">\(\tilde Y\)</span>. One simulates a value from the predictive distribution in two steps: first, one simulates a value of the parameter <span class="math inline">\(\mu\)</span> from its posterior distribution; second, use this simulated parameter draw to simulate a future observation <span class="math inline">\(\tilde Y\)</span> from the data model. In particular, the following algorithm is used to simulate a single value from the posterior predictive distribution.</p>
<ol type="1">
<li><p>Sample a value of <span class="math inline">\(\mu\)</span> from its posterior distribution <span class="math display">\[\begin{eqnarray}
\mu \sim \textrm{Normal}\left(\frac{\phi_0\mu_0 + n\phi\bar{y}}{\phi_0 + n\phi}, \sqrt{\frac{1}{\phi_0 + n\phi}}\right),
\end{eqnarray}\]</span></p></li>
<li><p>Sample a new observation <span class="math inline">\(\tilde{Y}\)</span> from the data model (i.e.&nbsp;a prediction) <span class="math display">\[\begin{eqnarray}
\tilde{Y} \sim \textrm{Normal}(\mu, \sigma).
\end{eqnarray}\]</span></p></li>
</ol>
<p>This two-step procedure is implemented for our time-to-serve example using the following R script.</p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>mu_n <span class="ot">&lt;-</span> <span class="fl">17.4</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>sigma_n <span class="ot">&lt;-</span> <span class="fl">0.77</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>pred_mu_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, mu_n, sigma_n)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>(pred_y_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, pred_mu_sim, sigma))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 16.04772</code></pre>
</div>
</div>
<p>The script can easily be updated to create <span class="math inline">\(S\)</span> = 1000 predictions, which is helpful to make summary about predictions.</p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>pred_mu_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, mu_n, sigma_n)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>pred_y_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, pred_mu_sim, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The vector <code>pred_y_sim</code> contains 1000 predictions of Federer’s time-to-serve.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Display of the exact and simulated time-to-serve for Federer’s example.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To evaluate the accuracy of the simulation-based predictions, Figure 8.7 displays the exact and a density estimate of the simulation-based predictive densities for a single time-to-serve measurement. One observes pretty good agreement using these two computation methods in this example.</p>
</section>
</section>
</section>
<section id="Normal:PPC" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="Normal:PPC"><span class="header-section-number">2.7</span> Posterior Predictive Checking</h2>
<p>In Section 8.6, the use of the posterior predictive distribution for predicting a future time-to-serve measurement was described. As discussed in Chapter 7, this distribution is also helpful for assessing the suitability of the Bayesian model.</p>
<p>In our example, we observed 20 times-to-serve for Federer. The question is whether these observed times are consistent with replicated data from the posterior predictive distribution. In this setting, replicated refers to the same sample size as our original sample. In other words, if one takes samples of 20 from the posterior predictive distribution, do these replicated datasets resemble the observed sample?</p>
<p>Since the population standard deviation is known as <span class="math inline">\(\sigma = 4\)</span> seconds, the sampling distribution of <span class="math inline">\(Y\)</span> is Normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. One simulates replicated data <span class="math inline">\(\tilde Y_1, ..., \tilde Y_{20}\)</span> from the posterior predictive distribution in two steps:</p>
<ol type="1">
<li><p>Sample a value of <span class="math inline">\(\mu\)</span> from its posterior distribution <span class="math display">\[\begin{eqnarray}
\mu \sim \textrm{Normal}\left(\frac{\phi_0\mu_0 + n\phi\bar{y}}{\phi_0 + n\phi}, \sqrt{\frac{1}{\phi_0 + n\phi}}\right).
\end{eqnarray}\]</span></p></li>
<li><p>Sample <span class="math inline">\(\tilde Y_1, ..., \tilde Y_{20}\)</span> from the data model <span class="math display">\[\begin{eqnarray}
\tilde{Y} \sim \textrm{Normal}(\mu, \sigma).
\end{eqnarray}\]</span></p></li>
</ol>
<p>This method is implemented in the following R script to simulate 1000 replicated samples from the posterior predictive distribution. The vector <code>pred_mu_sim</code> contains draws from the posterior distribution and the matrix <code>ytilde</code> contains the simulated predictions where each row of the matrix is a simulated sample of 20 future times.</p>
<div class="cell">

</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>mu_n <span class="ot">&lt;-</span> <span class="fl">17.4</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>sigma_n <span class="ot">&lt;-</span> <span class="fl">0.77</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>pred_mu_sim <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(S, mu_n, sigma_n)</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>sim_ytilde <span class="ot">&lt;-</span> <span class="cf">function</span>(j){</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rnorm</span>(<span class="dv">20</span>, pred_mu_sim[j], sigma)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>ytilde <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>S, sim_ytilde))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To judge goodness of fit, we wish to compare these simulated replicated datasets from the posterior predictive distribution with the observed data. One convenient way to implement this comparison is to compute some “testing function”, <span class="math inline">\(T(\tilde y)\)</span>, on each replicated dataset. If we have 1000 replicated datasets, one has 1000 values of the testing function. One constructs a graph of these values and overlays the value of the testing function on the observed data <span class="math inline">\(T(y)\)</span>. If the observed value is in the tail of the posterior predictive distribution of <span class="math inline">\(T(\tilde y)\)</span>, this indicates some misfit of the observed data with the Bayesian model.</p>
<p>To implement this procedure, one needs to choose a testing function <span class="math inline">\(T(\tilde y)\)</span>. Suppose, for example, one decides to use the sample mean <span class="math inline">\(T(\tilde y) = \sum \tilde y_j / 20\)</span>. In the R script, we compute the sample mean on each row of the simulated prediction matrix.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>pred_ybar_sim <span class="ot">&lt;-</span> <span class="fu">apply</span>(ytilde, <span class="dv">1</span>, mean)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Figure 8.8 displays a density estimate of the simulated values from the posterior predictive distribution of <span class="math inline">\(\bar Y\)</span> and the observed value of the sample mean <span class="math inline">\(\bar Y = 17.20\)</span> is displayed as a vertical line. Since this observed mean is in the middle of this distribution, one concludes that this observation is consistent with samples predicted from the Bayesian model. It should be noted that this conclusion about model fit is sensitive to the choice of checking function <span class="math inline">\(T()\)</span>. In the end-of-chapter exercises, the reader will explore the suitability of this model using alternative choices for the checking function.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Display of the posterior predictive mean time-to-serve for twenty observations. The observed mean time-to-serve value is displayed by a vertical line.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="modeling-count-data" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="modeling-count-data"><span class="header-section-number">2.8</span> Modeling Count Data</h2>
<p>To further illustrate the Bayesian approach to inference for measurements, consider Poisson sampling, a popular model for count data. One assumes that one observes a random sample from a Poisson distribution with an unknown rate parameter <span class="math inline">\(\lambda\)</span>. The conjugate prior for the Poisson mean is the Gamma distribution. This scenario provides further practice in various Bayesian computations, such as computing the likelihood function and posterior distribution, and obtaining the predictive distribution to learn about future data. In this section, we focus on the main results and the detailed derivations are left as end-of-chapter exercises.</p>
<section id="examples-1" class="level3" data-number="2.8.1">
<h3 data-number="2.8.1" class="anchored" data-anchor-id="examples-1"><span class="header-section-number">2.8.1</span> Examples</h3>
<p><strong>Counts of patients in an emergency room</strong></p>
<p>A hospital wants to determine how many doctors and nurses to assign on their emergency room (ER) team between 10pm and 11pm during the week. An important piece of information is the count of patients arriving in the ER in this one-hour period.</p>
<p>For a count measurement variable such as the count of patients, a popular sampling model is the Poisson distribution. This distribution is used to model the number of times an event occurs in an interval of time or space. In the current example, the event is a patient’s arrival to the ER, and the time interval is the period between 10pm and 11pm. The hospital wishes to learn about the average count of patients arriving to the ER each hour. Perhaps more importantly, the hospital wants to predict the patient count since that will directly address the scheduling of doctors and nurses question.</p>
<p><strong>Counts of visitors to a website</strong></p>
<p>As a second example, suppose one is interested in monitoring the popularity of a particular blog focusing on baseball analytics. Table 8.7 displays the number of visitors viewing this blog for 28 days during June of 2019. In this setting, the event of interest is a visit to the blog website and the time interval is a single day. The blog author is particularly interested in learning about the average number of visitors during the days Monday through Friday and predicting the number of visits for a future day in the summer of 2019.</p>
<p>Table 8.7. Number of visitors to a baseball blog site during different days during June, 2019.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th style="text-align: right;">Fri</th>
<th style="text-align: right;">Sat</th>
<th style="text-align: right;">Sun</th>
<th style="text-align: right;">Mon</th>
<th style="text-align: right;">Tue</th>
<th style="text-align: right;">Wed</th>
<th style="text-align: right;">Thu</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">Week 1</td>
<td style="text-align: right;">95</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">85</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">111</td>
<td style="text-align: right;">130</td>
<td style="text-align: right;">113</td>
</tr>
<tr class="even">
<td style="text-align: right;">Week 2</td>
<td style="text-align: right;">92</td>
<td style="text-align: right;">65</td>
<td style="text-align: right;">78</td>
<td style="text-align: right;">96</td>
<td style="text-align: right;">118</td>
<td style="text-align: right;">120</td>
<td style="text-align: right;">104</td>
</tr>
<tr class="odd">
<td style="text-align: right;">Week 3</td>
<td style="text-align: right;">91</td>
<td style="text-align: right;">91</td>
<td style="text-align: right;">79</td>
<td style="text-align: right;">106</td>
<td style="text-align: right;">91</td>
<td style="text-align: right;">114</td>
<td style="text-align: right;">110</td>
</tr>
<tr class="even">
<td style="text-align: right;">Week 4</td>
<td style="text-align: right;">98</td>
<td style="text-align: right;">61</td>
<td style="text-align: right;">84</td>
<td style="text-align: right;">96</td>
<td style="text-align: right;">126</td>
<td style="text-align: right;">119</td>
<td style="text-align: right;">90</td>
</tr>
</tbody>
</table>
<p>Count of visitors to blog during 28 days during June 2019.</p>
</section>
<section id="the-poisson-distribution" class="level3" data-number="2.8.2">
<h3 data-number="2.8.2" class="anchored" data-anchor-id="the-poisson-distribution"><span class="header-section-number">2.8.2</span> The Poisson distribution</h3>
<p> Let the random variable <span class="math inline">\(Y\)</span> denote the number of occurrences of an event in an interval with sample space <span class="math inline">\(\{0, 1, 2, \cdots \}\)</span>. In contrast to the Normally distributed continuous measurement, note that <span class="math inline">\(Y\)</span> only takes integer values from 0 to infinity. The variable <span class="math inline">\(Y\)</span> follows a Poisson distribution with rate parameter <span class="math inline">\(\lambda\)</span> when the probability mass function (pmf) of observing <span class="math inline">\(y\)</span> events in an interval is given by</p>
<p><span class="math display">\[\begin{eqnarray}
f(Y = y \mid \lambda) = e^{-\lambda}\frac{\lambda^y}{y!}, \, \, y = 0, 1, 2, ...
\end{eqnarray}\]</span> where <span class="math inline">\(\lambda\)</span> is the average number of events per interval, <span class="math inline">\(e = 2.71828...\)</span> is Euler’s number, and <span class="math inline">\(y!\)</span> is the factorial of <span class="math inline">\(y\)</span>.</p>
<p>The Poisson sampling model is based on several assumptions about the sampling process. One assumes that the time interval is fixed, counts of arrivals occurring during different time intervals are independent, and the rate <span class="math inline">\(\lambda\)</span> at which the arrivals occur is constant over time. To check the suitability of the Poisson distribution for the examples, one needs to check the conditions one by one.</p>
<ol type="1">
<li>The time interval is fixed in the ER example as we observe patient arrivals during a one hour period between 10pm and 11pm. For the blog visits example, the fixed time period is one day.</li>
<li>In both examples, one assumes that events occur independently during different time intervals. In the ER example it is reasonable to assume that the time of one patient’s arrival does not influence the time of another patient’s arrival. For the website visits example, if different people are visiting the website on different days, then one could assume the number of visits on one day would be independent of the number of visits on another day.</li>
<li>Is it reasonable to assume the rate <span class="math inline">\(\lambda\)</span> at which events occur is constant through the time interval? In the ER example, one might not think that the rate of patient arrivals would change much through one hour during the evening, so it seems reasonable to assume that the average number of events is constant in the fixed interval. Similarly, if one focuses on weekdays, then for the website visits example, it is reasonable to assume that the average number of visits remains constant across days.</li>
</ol>
<p>In some situations, the second and third conditions will be violated. In our ER example, the occurrence of serious accidents may bring multiple groups of patients to the ER at certain time intervals. In this case, arrival times of patients may not be independent and the arrival rate <span class="math inline">\(\lambda\)</span> in one subinterval will be higher than the arrival rate of another subinterval. When such situations occur, one needs to decide about the severity of the violation of the conditions and possibly use an alternative sampling model instead of the Poisson.</p>
<p>As evident in Equation (8.35), the Poisson distribution has only one parameter, the rate parameter <span class="math inline">\(\lambda\)</span>, so the Poisson sampling model belongs to the family of one-parameter sampling models. The Binomial data model with success probability <span class="math inline">\(p\)</span> and the Normal data model with mean parameter <span class="math inline">\(\mu\)</span> (with known standard deviation) are two other examples of one-parameter models. One distinguishes these models by the type of possible sample values, discrete or continuous. The Binomial random variable is the number of successes and the Poisson random variable is a count of arrivals, so they both are discrete one-parameter models. In contrast, the Normal sampling data model is a continuous one-parameter model.</p>
</section>
<section id="bayesian-inferences" class="level3" data-number="2.8.3">
<h3 data-number="2.8.3" class="anchored" data-anchor-id="bayesian-inferences"><span class="header-section-number">2.8.3</span> Bayesian inferences</h3>
<p> The reader should be familiar with the typical procedure of Bayesian inference and prediction for one-parameter models. We rewrite this procedure in the context of the Poisson sampling model.</p>
<ol type="1">
<li><strong>[Step 1]</strong> One constructs a prior expressing an opinion about the location of the rate <span class="math inline">\(\lambda\)</span> before any data is collected.</li>
<li><strong>[Step 2]</strong> One takes the sample of intervals and records the number of arrivals in each interval. From this data, one forms the likelihood, the probability of these observations expressed as a function of <span class="math inline">\(\lambda\)</span>.</li>
<li><strong>[Step 3]</strong> One uses Bayes’ rule to compute the posterior – this distribution updates the prior opinion about <span class="math inline">\(\lambda\)</span> given the information from the data.</li>
<li>In addition, one computes the predictive distribution to learn about the number of arrivals in future intervals. The posterior predictive distribution is also useful in checking the appropriateness of our model.</li>
</ol>
<p><strong>Gamma prior distribution</strong></p>
<p>One begins by constructing a prior density to express one’s opinion about the rate parameter <span class="math inline">\(\lambda\)</span>. Since the rate is a positive continuous parameter, one needs to construct a prior density that places its support only on positive values. The convenient choice of prior distributions for Poisson sampling is the Gamma distribution which has a density function given by <span class="math display">\[\begin{eqnarray}
\pi(\lambda \mid \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma{(\alpha)}} \lambda^{\alpha-1}e^{-\beta \lambda}, \,\,\, \text{for}\,\, \lambda &gt; 0, \,\, \text{and}\,\,  \alpha, \beta &gt; 0,
\end{eqnarray}\]</span> where <span class="math inline">\(\Gamma(\alpha)\)</span> is the Gamma function evaluated at <span class="math inline">\(\alpha\)</span>. The Gamma density is a continuous density where the support is on positive values. It depends on two parameters, a positive shape parameter <span class="math inline">\(\alpha\)</span> and a positive rate parameter <span class="math inline">\(\beta\)</span>.</p>
<p>The Gamma density is a flexible family of distributions that can reflect many different types of prior beliefs about the location of the parameter <span class="math inline">\(\lambda\)</span>. One chooses values of the shape <span class="math inline">\(\alpha\)</span> and the rate <span class="math inline">\(\beta\)</span> so that the Gamma density matches one’s prior information about the location of <span class="math inline">\(\lambda\)</span>. In R, the function <code>dgamma()</code> gives the density, <code>pgamma()</code> gives the distribution function and <code>qgamma()</code> gives the quantile function for the Gamma distribution. These functions are helpful in graphing the prior and choosing values of the shape and rate parameters that match prior statements about Gamma percentiles and probabilities. We provide an illustration of choosing a subjective Gamma prior in the example.</p>
<p><strong>Sampling and the likelihood</strong></p>
<p>Suppose that <span class="math inline">\(Y_1, ..., Y_n\)</span> represent the observed counts in <span class="math inline">\(n\)</span> time intervals where the counts are independent and each <span class="math inline">\(Y_i\)</span> follows a Poisson distribution with rate <span class="math inline">\(\lambda\)</span>. The joint mass function of <span class="math inline">\(Y_1, ..., Y_n\)</span> is obtained by multiplying the Poisson densities. <span class="math display">\[\begin{eqnarray}
f(Y_1 = y_1, ... ,  Y_n = y_n \mid \lambda ) &amp;=&amp; \prod_{i=1}^{n}f(y_i \mid \lambda) \nonumber \\
                                       &amp;\propto&amp; \lambda^{\sum_{i=1}^{n}y_i} e^{-n\lambda}.   
\end{eqnarray}\]</span> Once the counts <span class="math inline">\(y_1, ..., y_n\)</span> are observed, the likelihood of <span class="math inline">\(\lambda\)</span> is the joint probability of observing this data, viewed as a function of the rate parameter <span class="math inline">\(\lambda\)</span>. <span class="math display">\[\begin{equation}
L(\lambda) = \lambda^{\sum_{i=1}^{n}y_i} e^{-n\lambda}.
\end{equation}\]</span></p>
<p>If the rate parameter <span class="math inline">\(\lambda\)</span> in the Poisson sampling model follows a Gamma prior distribution, then it turns out that the posterior distribution for <span class="math inline">\(\lambda\)</span> will also have a Gamma density with updated parameters. This demonstrates that the Gamma density is the conjugate distribution for Poisson sampling as the prior and posterior densities both come from the same family of distribution: Gamma.</p>
<p>We begin by assuming that the Poisson parameter <span class="math inline">\(\lambda\)</span> has a Gamma distribution with shape and rate parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, that is, <span class="math inline">\(\lambda \sim\)</span> Gamma<span class="math inline">\((\alpha, \beta)\)</span>. If one multiplies the Gamma prior by the likelihood function <span class="math inline">\(L(\lambda)\)</span>, then in an end-of-chapter exercise you will show that the posterior density of <span class="math inline">\(\lambda\)</span> is Gamma<span class="math inline">\((\alpha_n, \beta_n)\)</span>, where the updated parameters <span class="math inline">\(\alpha_n\)</span> and <span class="math inline">\(\beta_n\)</span> are given by <span class="math display">\[\begin{equation}
\alpha_n = \alpha + \sum_{i=1}^n y_i, \, \, \, \beta_n = \beta + n.
\end{equation}\]</span></p>
<p><strong>Inference about <span class="math inline">\(\lambda\)</span></strong></p>
<p>Once the posterior distribution has been derived, then all inferences about the Poisson parameter <span class="math inline">\(\lambda\)</span> are performed by computing particular summaries of the Gamma posterior distribution. In particular, one may be interested in testing if <span class="math inline">\(\lambda\)</span> falls in a particular region by computing a posterior probability. All of these computations are facilitated using the <code>pgamma()</code>, <code>qgamma()</code>, and <code>rgamma()</code> functions. Or one may be interested in constructing an interval estimate for <span class="math inline">\(\lambda\)</span>. In the end-of-chapter exercises, there are opportunities to perform these inferences using a dataset containing a sample of ER arrival counts.</p>
<p><strong>Prediction of future data</strong></p>
<p>One advantage of using a conjugate prior is that the predictive density for a future observation <span class="math inline">\(\tilde Y\)</span> is available in closed form. Suppose <span class="math inline">\(\lambda\)</span> is assigned a <span class="math inline">\(\textrm{Gamma}(\alpha, \beta\)</span>) prior. Then the prior predictive density of <span class="math inline">\(\tilde Y\)</span> is given by <span class="math display">\[\begin{eqnarray}
f(\tilde{Y} = \tilde y)  &amp;=&amp; \int f(\tilde{Y} = \tilde{y} \mid \lambda) \pi(\lambda) \lambda  \nonumber \\
&amp; =&amp; \int \frac{e^{-\lambda} \lambda^{\tilde y}} {\tilde y!}  
\frac{\beta^{\alpha}}{\Gamma{(\alpha)}} \lambda^{\alpha-1}e^{-\beta \lambda} d \lambda  \nonumber \\
&amp;=&amp; \frac{\Gamma(\alpha + \tilde y)}{\Gamma(\alpha)}
\frac{\beta^\alpha}{(\beta + 1)^{\tilde y + \alpha}}.
\end{eqnarray}\]</span></p>
<p>In addition, the posterior distribution of <span class="math inline">\(\lambda\)</span> also has the Gamma form with updated parameters <span class="math inline">\(\alpha_n\)</span> and <span class="math inline">\(\beta_n\)</span>. So Equation (8.40) also provides the posterior predictive distribution for a future count <span class="math inline">\(\tilde Y\)</span> using the updated parameter values.</p>
<p>For prediction purposes, there are several ways of summarizing the predictive distribution. One can use the formula in Equation (8.40) to directly compute <span class="math inline">\(f(\tilde Y)\)</span> for a list of values of <span class="math inline">\(\tilde Y\)</span> and then one uses the computed probabilities to form a prediction interval for <span class="math inline">\(\tilde Y\)</span>. Alternately, one simulates values of <span class="math inline">\(\tilde Y\)</span> in a two-step process. For example, if one wants to simulate a draw from the posterior predictive distribution, one would first simulate a value <span class="math inline">\(\lambda\)</span> from its posterior distribution, and given that simulated draw <span class="math inline">\(\lambda^*\)</span>, simulate <span class="math inline">\(\tilde Y\)</span> from a Poisson distribution with mean <span class="math inline">\(\lambda^*\)</span>. Repeating this process for a large number of iterations provides a sample from the posterior prediction distribution that one uses to construct a prediction interval.</p>
</section>
<section id="case-study-learning-about-website-counts" class="level3" data-number="2.8.4">
<h3 data-number="2.8.4" class="anchored" data-anchor-id="case-study-learning-about-website-counts"><span class="header-section-number">2.8.4</span> Case study: Learning about website counts</h3>
<p>Let’s return to the website example where one is interested in learning about the average weekday visits to a baseball analytics blog site. One observes the counts <span class="math inline">\(y_1, ..., y_{20}\)</span> displayed in the “Mon”, “Tue”, “Wed”, “Thu”, “Fri” columns of Table 8.7. We assume the {<span class="math inline">\(y_i\)</span>} represent a random sample from a Poisson distribution with mean parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>Suppose one’s prior guess at the value of <span class="math inline">\(\lambda\)</span> is 80 and one wishes to match this information with a Gamma(<span class="math inline">\(\alpha, \beta\)</span>) prior. Two helpful facts about the Gamma distribution are that the mean and variance are equal to <span class="math inline">\(\mu = \alpha / \beta\)</span> and <span class="math inline">\(\sigma^2 =\alpha / \beta^2 = \mu / \beta,\)</span> respectively. Figure 8.9 displays three Gamma curves for values <span class="math inline">\((\alpha, \beta\)</span>) = (80, 1), (40, 0.5), and (20, 0.25). Each of these Gamma curves have a mean of 80 and the curves become more diffuse as the parameter <span class="math inline">\(\beta\)</span> moves from 1 to 0.25. After some thought, the user believes that the Gamma(80, 1) matches her prior beliefs. To check, she computes a prior probability interval. Using the <code>qgamma()</code> function, she finds that her 90% prior probability interval is <span class="math inline">\(Prob(65.9 &lt; \lambda &lt; 95.3) = 0.90\)</span> and this appears to be a reasonable approximation to her prior beliefs.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-32-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Three Gamma(<span class="math inline">\(\alpha, \beta\)</span>) plausible prior distributions for the average number of weekday visits to the website.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From the data, we compute <span class="math inline">\(\sum_{i=1}^{20} y_i = 2120\)</span> and the sample size is <span class="math inline">\(n = 20\)</span>. The posterior distribution is Gamma(<span class="math inline">\(\alpha_n, \beta_n)\)</span> where the updated parameters are <span class="math display">\[\begin{equation*}
\alpha_n = 80 + 2120 = 2200, \, \, \beta_n = 1 + 20 = 21.
\end{equation*}\]</span> Figure 8.10 displays the Gamma posterior curve for <span class="math inline">\(\lambda\)</span>. This figure displays a 90% probability interval which is found using the <code>qgamma()</code> function to be (101.1, 108.5). The interpretation is that the average number of visits lies between 101.1 and 108.5 with probability 0.90.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Posterior curve for the mean number of visits <span class="math inline">\(\lambda\)</span> to the website. The shaded region shows the limits of a 90% interval estimate.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Suppose the user is interested in predicting the number of blog visits <span class="math inline">\(\tilde Y\)</span> at a future summer weekday. One simulates the posterior predictive distribution by first simulating 1000 values from the Gamma posterior, and then simulating values of <span class="math inline">\(\tilde Y\)</span> from Poisson distributions where the Poisson means come from the posterior. Figure 8.11 displays a histogram of the simulated values from the predictive distribution. The 5th and 95th quantiles of this distribution are computed to be 88 and 123 – there is a 90% probability that that the number of visitors in a future weekday will fall in the interval (88, 123).</p>
<div class="cell" data-layout-align="center">
<div class="cell-output cell-output-stdout">
<pre><code>    5%    95% 
 88.95 123.00 </code></pre>
</div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mean_files/figure-html/unnamed-chunk-34-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Histogram of a simulated sample from the posterior predictive distribution of the number of visitors to the website on a future day.</figcaption><p></p>
</figure>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./proportion.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Learning About a Binomial Probability</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mcmc.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Simulation by Markov Chain Monte Carlo</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>