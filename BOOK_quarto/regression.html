<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.309">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian Modeling - 6&nbsp; Simple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<link href="./multipleregression.html" rel="next">
<link href="./hierarchical.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Modeling</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proportion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Learning About a Binomial Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Modeling Measurement and Count Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Simulation by Markov Chain Monte Carlo</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hierarchical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Hierarchical Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multipleregression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Bayesian Multiple Regression and Logistic Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./casestudies.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Case Studies</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"> <span class="header-section-number">6.1</span> Introduction</a></li>
  <li><a href="#example-prices-and-areas-of-house-sales" id="toc-example-prices-and-areas-of-house-sales" class="nav-link" data-scroll-target="#example-prices-and-areas-of-house-sales"> <span class="header-section-number">6.2</span> Example: Prices and Areas of House Sales</a></li>
  <li><a href="#a-simple-linear-regression-model" id="toc-a-simple-linear-regression-model" class="nav-link" data-scroll-target="#a-simple-linear-regression-model"> <span class="header-section-number">6.3</span> A Simple Linear Regression Model</a></li>
  <li><a href="#a-weakly-informative-prior" id="toc-a-weakly-informative-prior" class="nav-link" data-scroll-target="#a-weakly-informative-prior"> <span class="header-section-number">6.4</span> A Weakly Informative Prior</a></li>
  <li><a href="#posterior-analysis" id="toc-posterior-analysis" class="nav-link" data-scroll-target="#posterior-analysis"> <span class="header-section-number">6.5</span> Posterior Analysis</a></li>
  <li><a href="#inference-through-mcmc" id="toc-inference-through-mcmc" class="nav-link" data-scroll-target="#inference-through-mcmc"> <span class="header-section-number">6.6</span> Inference through MCMC</a></li>
  <li><a href="#bayesian-inferences-with-simple-linear-regression" id="toc-bayesian-inferences-with-simple-linear-regression" class="nav-link" data-scroll-target="#bayesian-inferences-with-simple-linear-regression"> <span class="header-section-number">6.7</span> Bayesian Inferences with Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#simulate-fits-from-the-regression-model" id="toc-simulate-fits-from-the-regression-model" class="nav-link" data-scroll-target="#simulate-fits-from-the-regression-model"> <span class="header-section-number">6.7.1</span> Simulate fits from the regression model</a></li>
  <li><a href="#learning-about-the-expected-response" id="toc-learning-about-the-expected-response" class="nav-link" data-scroll-target="#learning-about-the-expected-response"> <span class="header-section-number">6.7.2</span> Learning about the expected response</a></li>
  <li><a href="#prediction-of-future-response" id="toc-prediction-of-future-response" class="nav-link" data-scroll-target="#prediction-of-future-response"> <span class="header-section-number">6.7.3</span> Prediction of future response</a></li>
  <li><a href="#posterior-predictive-model-checking" id="toc-posterior-predictive-model-checking" class="nav-link" data-scroll-target="#posterior-predictive-model-checking"> <span class="header-section-number">6.7.4</span> Posterior predictive model checking</a></li>
  </ul></li>
  <li><a href="#informative-prior" id="toc-informative-prior" class="nav-link" data-scroll-target="#informative-prior"> <span class="header-section-number">6.8</span> Informative Prior</a>
  <ul class="collapse">
  <li><a href="#standardization" id="toc-standardization" class="nav-link" data-scroll-target="#standardization"> <span class="header-section-number">6.8.1</span> Standardization</a></li>
  <li><a href="#prior-distributions" id="toc-prior-distributions" class="nav-link" data-scroll-target="#prior-distributions"> <span class="header-section-number">6.8.2</span> Prior distributions</a></li>
  <li><a href="#posterior-analysis-1" id="toc-posterior-analysis-1" class="nav-link" data-scroll-target="#posterior-analysis-1"> <span class="header-section-number">6.8.3</span> Posterior Analysis</a></li>
  </ul></li>
  <li><a href="#a-conditional-means-prior" id="toc-a-conditional-means-prior" class="nav-link" data-scroll-target="#a-conditional-means-prior"> <span class="header-section-number">6.9</span> A Conditional Means Prior</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">6.1</span> Introduction</h2>
<p>For continuous response variables such as Roger Federer’s time-to-serve data in Chapter 8 and snowfall amounts in Buffalo, New York in Chapter 9, Normal sampling models have been applied. The basic underlying assumption in a Normal sampling model is that observations are identically and independently distributed (i.i.d.) according to a Normal density, as in <span class="math inline">\(Y_i \overset{i.i.d.}{\sim}\textrm{Normal}(\mu, \sigma)\)</span>.</p>
<p><strong>Adding a predictor variable</strong></p>
<p>When continuous responses are observed, it is common that other variables are recorded that may be associated with the primary response measure. In the Buffalo snowfall example, one may also observe the average temperature in winter season and one believes that the average season temperature is associated with the corresponding amount of snowfall. For the tennis example, one may believe that the time-to-serve measurement is related to the rally length of the previous point. Specifically, a long rally in the previous point may be associated with a long time-to-serve in the current point.</p>
<p>In Chapter 9, a Normal curve was used to model the snowfalls <span class="math inline">\(Y_1, ..., Y_n\)</span> for <span class="math inline">\(n\)</span> winters, <span class="math display">\[\begin{equation}
Y_i \mid \mu, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma), \, \, i = 1, \cdots, n.
\label{eq:introLik1}
\end{equation}\]</span> The model in Equation (11.1) assumes that each winter snowfall follows the same Normal density with mean <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. From a Bayesian viewpoint, one assigns prior distributions for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and bases inferences about these parameters from the posterior distribution.</p>
<p>However when the average temperature in winter <span class="math inline">\(i\)</span>, <span class="math inline">\(x_i\)</span>, is also available, one might wonder if the snowfall amount <span class="math inline">\(Y_i\)</span> can be explained by the average temperature <span class="math inline">\(x_i\)</span> in the same winter. One typically calls <span class="math inline">\(x_i\)</span> a predictor variable as one is interested in predicting the snowfall amount <span class="math inline">\(Y_i\)</span> from the value of <span class="math inline">\(x_i\)</span>. How does one extend the basic Normal sampling model in Equation (11.1) to study the possible relationship between the average temperature and the snowfall amount?</p>
<p><strong>An observation-specific mean</strong></p>
<p>The model in Equation (11.1) assumes a common mean <span class="math inline">\(\mu\)</span> for each <span class="math inline">\(Y_i\)</span>. Since one wishes to introduce a new variable <span class="math inline">\(x_i\)</span> specific to winter <span class="math inline">\(i\)</span>, the model in Equation (11.1) is adjusted to Equation (11.2) where the common mean <span class="math inline">\(\mu\)</span> is replaced by a winter specific mean <span class="math inline">\(\mu_i\)</span> . <span class="math display">\[\begin{equation}
Y_i \mid \mu_i, \sigma \overset{ind}{\sim} \textrm{Normal}(\mu_i, \sigma), \, \, i = 1, \cdots, n.
\label{eq:introLik2}
\end{equation}\]</span> Note that the observations <span class="math inline">\(Y_1, ..., Y_n\)</span> are no longer identically distributed since they have different means, but the observations are still independent which is indicated by <span class="math inline">\(ind\)</span> written over the distributed <span class="math inline">\(\sim\)</span> symbol in the formula.</p>
<p><strong>Linear relationship between the mean and the predictor</strong></p>
<p>One basic approach for relating a predictor <span class="math inline">\(x_i\)</span> and the response <span class="math inline">\(Y_i\)</span> is to assume that the mean of <span class="math inline">\(Y_i\)</span>, <span class="math inline">\(\mu_i\)</span>, is a linear function of <span class="math inline">\(x_i\)</span>. This linear relationship is written as <span class="math display">\[\begin{equation}
\mu_i = \beta_0 + \beta_1 x_i,
\label{eq:introLink}
\end{equation}\]</span> for <span class="math inline">\(i = 1, \dots, n\)</span>. In Equation (11.3), each <span class="math inline">\(x_i\)</span> is a known constant (that is why a small letter is used for <span class="math inline">\(x\)</span>) and <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown parameters. As one might guess, these intercept and slope parameters are random. One assigns a prior distribution to <span class="math inline">\((\beta_0, \beta_1)\)</span> and perform inference by summarizing the posterior distribution of these parameters.</p>
<p>In this model, the linear function <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> is interpreted as the <strong>expected</strong> snowfall amount when the average temperature is equal to <span class="math inline">\(x_i\)</span>. The intercept <span class="math inline">\(\beta_0\)</span> represents the expected snowfall when the winter temperature is <span class="math inline">\(x_i = 0\)</span>. The slope parameter <span class="math inline">\(\beta_1\)</span> gives the increase in the expected snowfall when the temperature <span class="math inline">\(x_i\)</span> increases by one degree. It is important to note that the linear relationship in Equation (11.3) with parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> describes the association between the mean <span class="math inline">\(\mu_i\)</span> and the predictor <span class="math inline">\(x_i\)</span>. This linear relationship is a statement about the expected or average snowfall amount <span class="math inline">\(\mu_i\)</span>, not the <strong>actual</strong> snowfall amount <span class="math inline">\(Y_i\)</span>.</p>
<p><strong>Linear regression model</strong></p>
<p>Substituting Equation (11.3) into the model in Equation (11.2), one obtains the linear regression model. <span class="math display">\[\begin{equation}
Y_i \mid \beta_0, \beta_1, \sigma \overset{ind}{\sim} \textrm{Normal}(\beta_0 + \beta_1 x_i, \sigma), \, \, i = 1, \cdots, n.
\label{eq:introLik3}
\end{equation}\]</span> This is a special case of a Normal sampling model, where the <span class="math inline">\(Y_i\)</span> independently follow a Normal density with observation specific mean <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> and common standard deviation <span class="math inline">\(\sigma\)</span>. Since there is only a single predictor <span class="math inline">\(x_i\)</span>, this model is commonly called the simple linear regression model.</p>
<p>One restates this regression model as <span class="math display">\[\begin{equation}
Y_i  = \mu_i + \epsilon_i, i = 1, \cdots, n,
\label{eq:introLik3a}
\end{equation}\]</span> where the mean response <span class="math inline">\(\mu_i = \beta_0 + \beta_1 x_i\)</span> and the residuals <span class="math inline">\(\epsilon_1, ..., \epsilon_n\)</span> are <span class="math inline">\(i.i.d.\)</span> from a Normal distribution with mean 0 and standard deviation <span class="math inline">\(\sigma\)</span>. In the context of our example, this model says that the snowfall for a particular season <span class="math inline">\(Y_i\)</span> is a linear function of the average season temperature <span class="math inline">\(x_i\)</span> plus a random error <span class="math inline">\(\epsilon_i\)</span> that is Normal with mean 0 and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>The simple linear regression model is displayed in Figure 11.1. The line in the graph represents the equation <span class="math inline">\(\beta_0 + \beta_1 x\)</span> for the mean response <span class="math inline">\(\mu = E(Y)\)</span>. The actual response <span class="math inline">\(Y\)</span> is equal to <span class="math inline">\(\beta_0 + \beta_1 x + \epsilon\)</span> where the random variable <span class="math inline">\(\epsilon\)</span> is distributed Normal with mean 0 and standard deviation <span class="math inline">\(\sigma\)</span>. The Normal curves (drawn sideways) represent the locations of the response <span class="math inline">\(Y\)</span> for three distinct values of the predictor <span class="math inline">\(x\)</span>. The parameter <span class="math inline">\(\sigma\)</span> represents the deviation of the response <span class="math inline">\(Y\)</span> about the mean value <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. One is interested in learning about the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that describe the line and the standard deviation <span class="math inline">\(\sigma\)</span> which describes the deviations of the random response about the line.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/Regression_View.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Display of linear regression model. The line represents the unknown regression line <span class="math inline">\(b_0 + b_1 x\)</span> and the Normal curves (drawn sideways) represent the distribution of the response <span class="math inline">\(Y\)</span> about the line.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In the linear regression model, the observation <span class="math inline">\(Y_i\)</span> is random, the predictor <span class="math inline">\(x_i\)</span> is a fixed constant and the unknown parameters are <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma\)</span>. Using the Bayesian paradigm, a joint prior distribution is assigned to <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span>. After the response values <span class="math inline">\(Y_i = y_i, i = 1, ..., n\)</span> are observed, one learns about the parameters through the posterior distribution. An MCMC algorithm will be used to simulate a posterior sample, and using the simulation sample, one makes inferences about the expected response <span class="math inline">\(\beta_0 + \beta_1 x\)</span> for a specific value of the predictor <span class="math inline">\(x\)</span>. Also, one will be able to assess the sizes of the errors by summarizing the posterior density of the standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>In our snowfall example, one is interested in learning about the relationship between the average temperature and the mean snowfall that is described by the linear model <span class="math inline">\(\mu = \beta_0 + \beta_1 x\)</span>. If the posterior probability that <span class="math inline">\(\beta_1 &lt; 0\)</span> is large, that indicates that lower average temperatures will likely result in larger mean snowfall. Also one is interested in using this model for prediction. If given the average winter temperature in the following season, can one predict the Buffalo snowfall? This question is addressed by use of the posterior predictive distribution of a future snowfall <span class="math inline">\(\tilde Y\)</span>. Using the usual computing strategy, one simulates a large sample of values from the posterior predictive distribution and finds an interval that contains <span class="math inline">\(\tilde Y\)</span> with a prescribed probability.</p>
<p>In this chapter, regression is introduced in Section 11.2 by a dataset containing several characteristics of 24 house sales in an area in Ohio. In this example, one is interested in predicting the price of a house given the house size and Section 11.3 presents a simple linear regression model to explain this relationship. The practice of standardizing variables will be introduced which is helpful in the process of assigning an informative prior on the regression parameters. Inference through MCMC is presented in Section 11.6 and methods for performing Bayesian inferences with simple linear regression are illustrated in Section 11.7.</p>
</section>
<section id="example-prices-and-areas-of-house-sales" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="example-prices-and-areas-of-house-sales"><span class="header-section-number">6.2</span> Example: Prices and Areas of House Sales</h2>
<p>Zillow is an online real estate database company that collects information on 110 million homes across the United States. Data is collected from a random sample of 24 houses for sale in the Findlay, Ohio area during October 2018. For each house, the dataset contains the selling price (in $1000) and size (in 1000 square feet). Table 11.1 displays the first five observations of the dataset.</p>
<p>Table 11.1. The house index, price (in $1000), and size (in 1000 sq feet) of 5 house sales in Findlay, Ohio area during October 2018. The random sample contains 24 house sales.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Index</th>
<th style="text-align: center;">Price ($1000)</th>
<th style="text-align: center;">Size (1000 sq feet)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">167</td>
<td style="text-align: center;">1.625</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">236</td>
<td style="text-align: center;">1.980</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">355</td>
<td style="text-align: center;">2.758</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">148</td>
<td style="text-align: center;">1.341</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">1.465</td>
</tr>
</tbody>
</table>
<p>Suppose one is interested in predicting a house’s selling price from its house size. In this example, one is treating price as the response variable and size as the single predictor. Figure 11.2 constructs a scatterplot of price (y-axis) against the size (x-axis) for the houses in the sample. This figure shows a positive relationship between the size and the price of a house sale, suggesting that the house sale price increases as the house size increases. Can one quantify this relationship through a Bayesian linear regression model? In particular, is there sufficient evidence that there is a positive association among the population of all homes? Can one predict the sale price of a home given its size?</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/PriceAreaScatterplot.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Scatterplot of price against size of house sales.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-simple-linear-regression-model" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="a-simple-linear-regression-model"><span class="header-section-number">6.3</span> A Simple Linear Regression Model</h2>
<p>The house sale example can be fit into the linear regression model framework. It is assumed the response variable, the price of a house sale, is a continuous variable is distributed as a Normal random variable. Specifically, the price <span class="math inline">\(Y_i\)</span> for house <span class="math inline">\(i\)</span>, is Normally distributed with mean <span class="math inline">\(\mu_i\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. <span class="math display">\[\begin{equation}
Y_i \mid \mu_i, \sigma \overset{ind}{\sim} \textrm{Normal}(\mu_i, \sigma),
\label{eq:modelLik1}
\end{equation}\]</span> where <span class="math inline">\(i = 1, \cdots, n\)</span>, where <span class="math inline">\(n = 24\)</span> is the number of homes in the dataset. The <span class="math inline">\(ind\)</span> over <span class="math inline">\(\sim\)</span> in Equation (11.6) indicates that each response <span class="math inline">\(Y_i\)</span> independently follows its own Normal density. Moreover, unlike the house-specific mean <span class="math inline">\(\mu_i\)</span>, a common standard deviation <span class="math inline">\(\sigma\)</span> is shared among all responses <span class="math inline">\(Y_i\)</span>’s.</p>
<p>Since one believes the size of the house is helpful in understanding a house’s price,<br>
one represents the mean price <span class="math inline">\(\mu_i\)</span> as a linear function of the house size <span class="math inline">\(x_i\)</span> depending on two parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\mu_i = \beta_0 + \beta_1 x_i
\label{eq:modelLink}
\end{equation}\]</span></p>
<p>How does one interpret the intercept and slope parameters? The intercept <span class="math inline">\(\beta_0\)</span> gives the expected price <span class="math inline">\(\mu_i\)</span> for a house <span class="math inline">\(i\)</span> that has zero square feet (<span class="math inline">\(x_i = 0\)</span>). This is not a meaningful parameter since no house (not even a tiny house) has zero square feet. The slope parameter <span class="math inline">\(\beta_1\)</span> gives the change in the expected price <span class="math inline">\(\mu_i\)</span>, when the size <span class="math inline">\(x_i\)</span> of house <span class="math inline">\(i\)</span> increases by 1 unit, i.e., increases by 1000 square feet.</p>
</section>
<section id="a-weakly-informative-prior" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="a-weakly-informative-prior"><span class="header-section-number">6.4</span> A Weakly Informative Prior</h2>
<p>In some situations, the user has limited prior information about the location of the regression parameters or the standard deviation. To implement the Bayesian approach, one has to assign a prior distribution, but it is desirable in this situation to assign a prior that has little impact on the posterior distribution.</p>
<p>Suppose that one’s beliefs about the regression coefficients <span class="math inline">\((\beta_0, \beta_1)\)</span> are independent from one’s opinion about the standard deviation <span class="math inline">\(\sigma\)</span>. Then the joint prior density for the parameters <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span> is written as <span class="math display">\[\begin{equation*}
\pi(\beta_0, \beta_1, \sigma) = \pi(\beta_0, \beta_1) \pi(\sigma).
\end{equation*}\]</span> The choice of weakly informative priors on <span class="math inline">\((\beta_0, \beta_1)\)</span> and <span class="math inline">\(\sigma\)</span> are described in separate sections.</p>
<p><strong>Prior on the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span></strong></p>
<p>If one assumes independence of one’s opinion about the intercept and the slope, one represents the joint prior <span class="math inline">\(\pi(\beta_0, \beta_1)\)</span> as the product of priors <span class="math inline">\(\pi(\beta_0) \pi(\beta_1)\)</span>, and it is convenient to use Normal priors. So it is assumed <span class="math inline">\(\beta_0 \sim \textrm{Normal}(\mu_0, s_0)\)</span> and <span class="math inline">\(\beta_1 \sim \textrm{Normal}(\mu_1, s_1)\)</span>.</p>
<p>The choice of the standard deviation <span class="math inline">\(s_j\)</span> in the Normal prior reflects how confident the person believes in a prior guess of <span class="math inline">\(\beta_j\)</span>. If one has little information about the location of a regression parameter, then the choice of the prior guess <span class="math inline">\(\mu_j\)</span> is not that important and one chooses a large value for the prior standard deviation <span class="math inline">\(s_j\)</span>. So the regression intercept and slope are each assigned a Normal prior with a mean of 0 and standard deviation equal to the large value of 100.</p>
<p><strong>Prior on sampling standard deviation <span class="math inline">\(\sigma\)</span></strong></p>
<p>In the current regression model, one assumes that <span class="math inline">\(Y_i \sim \textrm{Normal}(\beta_0 + \beta_1 x_i, \sigma)\)</span> and <span class="math inline">\(\sigma\)</span> represents the variability of the house price about the regression line. It is typically hard to specify informative beliefs about a standard deviation than a mean parameter such as <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. So following the suggestions from Chapter 9 and Chapter 10, one assigns a weakly informative prior for the standard deviation <span class="math inline">\(\sigma\)</span>. A Gamma prior for the precision parameter <span class="math inline">\(\phi = 1/\sigma^2\)</span> with small values of the shape and rate parameters, say <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 1\)</span>, was seen in those chapters to represent weak prior information, and a similar prior is assigned in this regression setting. <span class="math display">\[\begin{equation*}
\phi = 1/\sigma^2 \sim \textrm{Gamma}(1, 1).
\end{equation*}\]</span></p>
</section>
<section id="posterior-analysis" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="posterior-analysis"><span class="header-section-number">6.5</span> Posterior Analysis</h2>
<p>In the sampling model one has that <span class="math inline">\(Y_1, ..., Y_n\)</span> are independent with <span class="math inline">\(Y_i \sim \textrm{Normal}(\beta_0 + \beta_1 x_i, \sigma)\)</span>. Suppose the pairs <span class="math inline">\((x_1, y_1), ..., (x_n, y_n)\)</span> are observed. The likelihood is the joint density of these observations viewed as a function of <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span>. For convenience, the standard deviation <span class="math inline">\(\sigma\)</span> is reexpressed as the precision <span class="math inline">\(\phi = 1 / \sigma^2\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray}
L(\beta_0, \beta_1, \phi) &amp;= &amp; \prod_{i=1}^n \left[\frac{\sqrt{\phi}}{\sqrt{2 \pi}}
\exp\left\{-\frac{\phi}{2}(y_i - \beta_0 - \beta_1 x_i)^2\right\}\right]
\nonumber \\
&amp; \propto &amp; \phi^{\frac{n}{2}} \exp\left\{-\frac{\phi}{2}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2\right\}
\end{eqnarray}\]</span></p>
<p>By multiplying the likelihood by the prior for <span class="math inline">\((\beta_0, \beta_1, \phi)\)</span>, one obtains an expression for the posterior density. <span class="math display">\[\begin{eqnarray}
\pi(\beta_0, \beta_1, \phi \mid y_1, \cdots, y_n) &amp;\propto &amp; \phi^{\frac{n}{2}} \exp\left\{-\frac{\phi}{2}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2\right\} \nonumber \\
&amp; \times &amp; \exp\left\{-\frac{1}{2 s_0^2}(\beta_0 - \mu_0)^2\right\}
\exp\left\{-\frac{1}{2 s_1^2}(\beta_1 - \mu_1)^2\right\} \nonumber \\
&amp; \times &amp; \phi^{a-1} \exp(-b \phi)
\end{eqnarray}\]</span> Since this is not a familiar probability distribution, one needs to use an MCMC algorithm to obtain simulated draws from the posterior.</p>
</section>
<section id="inference-through-mcmc" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="inference-through-mcmc"><span class="header-section-number">6.6</span> Inference through MCMC</h2>
<p>It is convenient to draw an MCMC sample from a regression model using the JAGS software. One attractive feature of JAGS is that it is straightforward to transpose the statement of the Bayesian model (sampling density and prior) directly to the JAGS model script.</p>
<p><strong>Describe the model by a script</strong></p>
<p>The first step in using JAGS is writing the following script defining the linear regression model, saving the script in the character string <code>modelString</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>modelString <span class="ot">&lt;-</span><span class="st">"</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="st">## sampling</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="st">for (i in 1:N){</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">   y[i] ~ dnorm(beta0 + beta1*x[i], invsigma2)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="st">## priors</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">beta0 ~ dnorm(mu0, g0)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">beta1 ~ dnorm(mu1, g1)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">invsigma2 ~ dgamma(a, b)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="st">sigma &lt;- sqrt(pow(invsigma2, -1))</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="st">}"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the sampling section of the script, the loop goes from <code>1</code> to <code>N</code>, where <code>N</code> is the number of observations with index <code>i</code>. Recall that the Normal distribution <code>dnorm</code> in JAGS is stated in terms of the mean and precision, and so the variable <code>invsigma2</code> corresponds to the Normal sampling precision. The variable <code>sigma</code> is defined in the prior section of the script so one can track the simulated values of the standard deviation <span class="math inline">\(\sigma\)</span>. Also the variables <code>g0</code> and <code>g1</code> correspond to the precisions of the Normal prior densities for <code>beta0</code> and <code>beta1</code>.</p>
<p><strong>Define the data and prior parameters</strong></p>
<p>The next step is to provide the observed data and the values for the prior parameters. In the R script below, a list <code>the_data</code> contains the vector of sale prices, the vector of house sizes, and the number of observations. This list also contains the means and precisions of the Normal priors for <code>beta0</code> and <code>beta1</code>, and the values of the two parameters <code>a</code> and <code>b</code> of the Gamma prior for <code>invsigma2</code>. The prior standard deviations of the Normal priors on <code>beta0</code> and <code>beta1</code> are both 100, and so the corresponding precision values of <code>g0</code> and <code>g1</code> are both <span class="math inline">\(1/100^2 = 0.0001\)</span>.</p>
<pre><code>y &lt;- PriceAreaData$price  
x &lt;- PriceAreaData$newsize   
N &lt;- length(y)  
the_data &lt;- list("y" = y, "x" = x, "N" = N,
                 "mu0" = 0, "g0" = 0.0001,
                 "mu1" = 0, "g1" = 0.0001,
                 "a" = 1, "b" = 1)</code></pre>
<p><strong>Generate samples from the posterior distribution</strong></p>
<p>The <code>run.jags()</code> function in the <code>runjags</code> package generates posterior samples by the MCMC algorithm using the JAGS software. The script below runs one MCMC chain with an adaption period of 1000 iterations, a burn-in period of 5000 iterations, and an additional set of 5000 iterations to be run and collected for inference. By using the argument <code>monitor = c("beta0", "beta1", "sigma")</code>, one keeps tracks of all three model parameters. The output variable <code>posterior</code> contains a matrix of simulated draws.</p>
<pre><code>posterior &lt;- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("beta0", "beta1", "sigma"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000)</code></pre>
<p><strong>MCMC diagnostics and summarization</strong></p>
<p>Using JAGS one obtains 5000 posterior samples for the vector of parameters. Below the first 10 posterior samples are displayed for the triplet <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span>. Note that the index starts from 6001 since 6000 samples were already generated in the adaption and burn-in periods.</p>
<pre><code>     beta0 beta1 sigma
6001 -17.62 103.3 40.68
6002 -21.35 107.3 44.92
6003 -34.34 114.0 37.11
6004 -42.06 110.5 51.84
6005 -47.71 111.4 62.63
6006 -47.49 113.9 53.80
6007 -18.85 106.0 50.92
6008 -28.50 114.8 42.71
6009 -32.10 105.1 47.41
6010 -37.41 119.3 45.88</code></pre>
<p>To obtain valid inferences from the posterior draws from the MCMC simulation, convergence of the MCMC chain is necessary. The <code>plot()</code> function with the argument input <code>vars</code> returns four diagnostic plots (trace plot, empirical CDF, histogram and autocorrelation plot) for the specified parameter. For example, Figure 11.3 shows the diagnostic plots for the intercept parameter <span class="math inline">\(\beta_0\)</span> by the following command.</p>
<pre><code>plot(posterior, vars = "beta0")</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/JA_LR_beta0.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">MCMC diagnostics plots for the regression intercept parameter.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The upper left trace plot shows good MCMC mixing for the 5000 simulated draws of <span class="math inline">\(\beta_0\)</span>. The lower right autocorrelation plot indicates close to zero correlation between adjacent posterior draws of <span class="math inline">\(\beta_0\)</span>. Overall these indicate convergence of the MCMC chain for <span class="math inline">\(\beta_0\)</span>. In usual practice, one should perform these diagnostics for all three parameters in the model.</p>
<p>Figure 11.4 displays a scatterplot of the simulated draws of the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. It is interesting to note the strong negative correlation in these parameters. If one assigned informative independent priors on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, these prior beliefs would be counter to the correlation between the two parameters observed in the data.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/Post_beta0_beta1.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Scatterplot of posterior draws of the intercept and slope parameters.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Posterior summaries of the parameters are obtained by use of the <code>print(posterior, digits = 3)</code> command. Note that these summaries are based on the 5000 iterations from the sampling period excluding the samples from the adaption and burn-in periods.</p>
<pre><code>print(posterior, digits = 3)
      Lower95 Median Upper95  Mean   SD Mode MCerr 
beta0    -122  -46.2    31.4 -45.7 37.6   --  2.98     
beta1    78.7    117     159   117   20   --  1.65     
sigma    33.2     45    59.3  45.7 6.93   -- 0.157  
</code></pre>
<p>Then intercept parameter <span class="math inline">\(\beta_0\)</span> does not have a useful interpretation, so values of these particular posterior summaries will not be interpreted. The summaries of the slope <span class="math inline">\(\beta_1\)</span> indicate a positive slope with a posterior median of 117 and a 90% credible interval (78.7, 159). That is, with every 1000 square feet increase of the house size, the house price increases by $117,000. In addition, this increase in the house price falls in the interval ($78,700, $159,000) with 90% posterior probability.<br>
The posterior median of the standard deviation <span class="math inline">\(\sigma\)</span> is the large value 45 or $45,000 which indicates that there are likely additional variables than house size that determine the price.</p>
</section>
<section id="bayesian-inferences-with-simple-linear-regression" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="bayesian-inferences-with-simple-linear-regression"><span class="header-section-number">6.7</span> Bayesian Inferences with Simple Linear Regression</h2>
<section id="simulate-fits-from-the-regression-model" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="simulate-fits-from-the-regression-model"><span class="header-section-number">6.7.1</span> Simulate fits from the regression model</h3>
<p>The intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> determine the linear relationship between the mean of the response <span class="math inline">\(Y\)</span> and the predictor <span class="math inline">\(x\)</span>. <span class="math display">\[\begin{equation}
E(Y) = \beta_0 + \beta_1 x.
\label{eq:ExpLink}
\end{equation}\]</span> Each pair of values (<span class="math inline">\(\beta_0, \beta_1\)</span>) corresponds to a line <span class="math inline">\(\beta_0 + \beta_1 x\)</span> in the space of values of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. If one finds the posterior mean of these coefficients, say <span class="math inline">\(\tilde {\beta_0}\)</span> and <span class="math inline">\(\tilde {\beta_1}\)</span>, then the line <span class="math display">\[\begin{equation*}
y = \tilde{\beta_0} + \tilde{\beta_1} x
\end{equation*}\]</span> corresponds to a “best” line of fit through the data.</p>
<p>This best line represents a most likely value of the line <span class="math inline">\(\beta_0 + \beta_1 x\)</span> from the posterior distribution. One learns about the uncertainty of this line estimate by drawing a sample of <span class="math inline">\(J\)</span> rows from the matrix of posterior draws of <span class="math inline">\((\beta_0, \beta_1)\)</span> and collecting the line estimates <span class="math display">\[\begin{equation*}
\tilde{\beta_0}^{(j)} + \tilde{\beta_1}^{(j)} x, j = 1, ..., J.
\end{equation*}\]</span></p>
<p>Using the R script below, one produces a graph showing the best line of fit (solid line) and ten simulated fits from the posterior as in Figure 11.5.</p>
<pre><code>post &lt;- as.mcmc(posterior)
post_means &lt;- apply(post, 2, mean)
post &lt;- as.data.frame(post)
ggplot(PriceAreaData, aes(newsize, price)) +
  geom_point(size=3) +
  geom_abline(data=post[1:10, ],
              aes(intercept=beta0, slope=beta1),
              alpha = 0.5) +
  geom_abline(intercept = post_means[1],
              slope = post_means[2],
              size = 2) +
  ylab("Price") + xlab("Size") +
  theme_grey(base_size = 18, base_family = "")</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/Simulated10fits.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Scatterplot of the (size, price) data with the best line of fit (solid line) and ten simulated fits <span class="math inline">\(b_0 + _1 x\)</span> from the posterior distribution.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From Figure 11.5, since there is inferential uncertainty about the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span>, one sees variation among the ten fits from the posterior of the linear regression line <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. This variation about the best-fitting line is understandable since the size of our sample of data is the relatively small value of 24. A larger sample size would help to reduce the posterior variation for the intercept and slope parameters and result in posterior samples of fits that are more tightly clustered about the best fitting line in Figure 11.5.</p>
</section>
<section id="learning-about-the-expected-response" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="learning-about-the-expected-response"><span class="header-section-number">6.7.2</span> Learning about the expected response</h3>
<p>In regression modeling, one may be interested in learning about the expected response <span class="math inline">\(E(Y)\)</span> for a specific value of the predictor <span class="math inline">\(x\)</span>. In the house sale example, one may wish to learn about the expected house price for a specific value of the house size. Since the expected response <span class="math inline">\(E(Y) = \beta_0 + \beta_1 x\)</span> is a linear function of the intercept and slope parameters, one obtains a simulated sample from the posterior of <span class="math inline">\(\beta_0 + \beta_1 x\)</span> by computing this function on each of the simulated pairs from the posterior of <span class="math inline">\((\beta_0, \beta_1)\)</span>.</p>
<p>For example, suppose one is interested in the expected price <span class="math inline">\(E(Y)\)</span> for a house with a size of 1, i.e.&nbsp;<span class="math inline">\(x = 1\)</span> (1000 sq feet). In the R script below, one simulates 5000 draws from the posterior of the expected house prices, <span class="math inline">\(E[Y]\)</span> from the 5000 posterior samples of the pair <span class="math inline">\((\beta_0, \beta_1)\)</span>.</p>
<pre><code>size &lt;- 1
mean_response &lt;- post[, "beta0"] + size * post[, "beta1"]</code></pre>
<p>This process is repeated for the four sizes <span class="math inline">\(x = 1.2, 1.6, 2.0, 2.4\)</span> (1200 sq feet, 1600 sq feet, 2000 sq feet, and 2400 sq feet). Let <span class="math inline">\(E(Y \mid x)\)</span> denotes the expected price for a house with size <span class="math inline">\(x\)</span>. Figure 11.6 displays density plots of the simulated posterior samples for the expected prices <span class="math inline">\(E(Y \mid 1.2)\)</span>, <span class="math inline">\(E(Y \mid 1.6)\)</span>, <span class="math inline">\(E(Y \mid 2.0)\)</span>, <span class="math inline">\(E(Y \mid 2.4)\)</span> for these four house sizes.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/ExpResponse2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Density plots of the simulated draws of the posterior expected house price for four different values of the house size.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The R output below provides summaries of the posterior of the expected price for each of the four values of the house size. From this output, one sees that, for a house of size of 1.2 (1200 sq feet), the posterior median of the expected price is 94.5 thousand dollars, and the probability that the expected price falls between $69,800 and $121,000 is 90%.</p>
<pre><code>  Value        P05   P50   P95
  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 Size = 1.2  69.8  94.5  121
2 Size = 1.6 125  142   159
3 Size = 2   172  189   205
4 Size = 2.4 211  236   260</code></pre>
</section>
<section id="prediction-of-future-response" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="prediction-of-future-response"><span class="header-section-number">6.7.3</span> Prediction of future response</h3>
<p>Learning about the regression model and values of the expected response values focus on the deterministic linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(E[Y]\)</span> through the intercept <span class="math inline">\(\beta_0\)</span> and the slope <span class="math inline">\(\beta_1\)</span>, as shown in Equation (11.10). The variability among the fitted lines in Figure 11.5 and the variability among the simulated house price for fixed size in Figure 11.6 reflects the variability in the posterior draws of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>However, if one wants to predict future values for a house sale price <span class="math inline">\(Y\)</span> given its size <span class="math inline">\(x\)</span>, one needs to go one step further to incorporate the sampling model in the simulation process. <span class="math display">\[\begin{equation}
Y_i \mid \beta_0, \beta_1, \sigma \overset{ind}{\sim} \textrm{Normal}(\beta_0 + \beta_1 x_i, \sigma)
\label{eq:modelLik5}
\end{equation}\]</span> As shown in Equation (11.11), the sampling model of <span class="math inline">\(Y\)</span> is a Normal with a mean expressed as a linear combination of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and a standard deviation <span class="math inline">\(\sigma\)</span>. To obtain a predicted value of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x = x_i\)</span>, one first simulates the expected response from <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>, and then simulates the predicted value of <span class="math inline">\(Y_i\)</span> from the sampling model: <span class="math inline">\(Y_i \sim \textrm{Normal}(E[Y_i], \sigma)\)</span>. Below is a diagram for the prediction process for an observation where its house size is given as <span class="math inline">\(x\)</span>, and predicted value denoted as <span class="math inline">\(\tilde{y}^{(s)}\)</span> for iteration <span class="math inline">\(s\)</span>. Here the simulation size <span class="math inline">\(S\)</span> is 5000 as there are 5000 posterior samples of each of the three parameters.</p>
<p><span class="math display">\[\begin{eqnarray*}
\text{simulate}\,\, E[y]^{(1)} = \beta_0^{(1)} + \beta_1^{(1)} x &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(1)} \sim {\rm{Normal}}(E[y]^{(1)}, \sigma^{(1)})\\
\text{simulate}\,\, E[y]^{(2)} = \beta_0^{(2)} + \beta_1^{(2)} x &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(2)} \sim {\rm{Normal}}(E[y]^{(2)}, \sigma^{(2)})\\
&amp;\vdots&amp; \\
\text{simulate}\,\, E[y]^{(S)} = \beta_0^{(S)} + \beta_1^{(S)} x &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(S)} \sim {\rm{Normal}}(E[y]^{(S)}, \sigma^{(S)})\\
\end{eqnarray*}\]</span></p>
<p>The R function <code>one_predicted()</code> obtains a simulated sample of the predictive distribution of the house price given a value of the house size. First one uses the posterior sample of <span class="math inline">\((\beta_0, \beta_1)\)</span> to obtain a posterior sample of the “linear response” <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. Then it simulates draws of the future observation by simulating from a Normal distribution with mean <span class="math inline">\(\beta_0 + \beta_1 x\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, where draws of <span class="math inline">\(\sigma\)</span> are taken from its posterior distribution.</p>
<pre><code>one_predicted &lt;- function(x){
  lp &lt;- post[ , "beta0"] +  x * post[ , "beta1"]
  y &lt;- rnorm(5000, lp, post[, "sigma"])
  data.frame(Value = paste("Price =", x),
             Predicted_Price = y)
}</code></pre>
<p>This process is repeated for each of the house sizes <span class="math inline">\(x = 1.2, 1.6, 2.0, 2.4\)</span> (1200 sq feet, 1600 sq feet, 2000 sq feet, and 2400 sq feet). Figure 11.7 displays density estimates of these simulated samples from the predictive distributions of the house price. Comparing Figure 11.6 with Figure 11.7, note that the predictive distributions are much wider than the posterior distributions on the expected response. This is what one would anticipate, since the predictive distribution incorporates two types of uncertainty – the inferential uncertainty in the values of the regression line <span class="math inline">\(\beta_0 + \beta_1 x\)</span> and the predictive uncertainty expressed in the sampling density of the response <span class="math inline">\(y\)</span> with standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/PredictedResponse2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Density plots of the simulated draws of the predicted house price for four different values of the house size.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To reinforce this last point, the R output below displays the 5th, 50th, and 95th percentiles of the predictive distribution of the house price for each of the four values of the house size. One saw earlier that a 90% interval estimate for the expected price for a house with <span class="math inline">\(x = 1.2\)</span> was given by (69.8, 121). Below one sees that a 90% prediction interval for the price of the same house size is <span class="math inline">\((15.5, 175)\)</span>. The prediction interval is substantially wider than the posterior interval estimate. This is true since the predictive distribution incorporates the sizable uncertainty in the house price given the house size represented by the sampling standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<pre><code>  Value        P05   P50   P95
  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 Size = 1.2  15.5  94.4   175
2 Size = 1.6  64.5 142     219
3 Size = 2   110   189     266
4 Size = 2.4 157   234     315</code></pre>
</section>
<section id="posterior-predictive-model-checking" class="level3" data-number="6.7.4">
<h3 data-number="6.7.4" class="anchored" data-anchor-id="posterior-predictive-model-checking"><span class="header-section-number">6.7.4</span> Posterior predictive model checking</h3>
<p> <strong>Simulating replicated datasets</strong></p>
<p>The posterior predictive distribution is used to predict the value of a house’s price for a particular house size. It is also helpful in judging the suitability of the linear regression model. The basic idea is that the observed response values should be consistent with predicted responses generated from the fitted model.</p>
<p>In our example, one observed the house size <span class="math inline">\(x\)</span> and the house price <span class="math inline">\(y\)</span> for a sample of 24 houses. Suppose one simulates a sample of prices for a sample of 24 houses with the same sizes from the posterior predictive distribution. This is implemented in two steps.</p>
<ol type="1">
<li>Values of the parameters <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span> are simulated from the posterior distribution – call these simulated values <span class="math inline">\((\beta^*_0, \beta^*_1, \sigma^*)\)</span>.</li>
<li>A sample <span class="math inline">\(\{y_1^R, ..., y_n^R\}\)</span> is simulated where the sample size is <span class="math inline">\(n = 24\)</span> and <span class="math inline">\(y_i^R\)</span> is Normal(<span class="math inline">\(\mu^*_i, \sigma^*)\)</span>, where <span class="math inline">\(\mu^*_i = \beta^*_0 + \beta^*_1 x_i\)</span>.</li>
</ol>
<p>This is called a <strong>replicated</strong> sample from the posterior predictive distribution since one is using the same sample size and covariate values as the original dataset.</p>
<p>For our example, this simulation process was repeated eight times, where each iteration produces a sample <span class="math inline">\((x_i, y_i^R), i = 1, ..., 24\)</span>. Scatterplots of these eight replicated samples are displayed in Figure 11.8. The observed sample is also displayed in this figure.</p>
<p>The question one wants to ask is: Do the scatterplots of the simulated replicated samples resemble the scatterplot of the observed data? Since the <span class="math inline">\(x\)</span> values are the same for the observed and replicated datasets, one focuses on possible differences in the observed and replicated response values. Possibly, the sample prices display more variation than the replicated prices, or perhaps the sample prices have a particular outlier or other feature that is not present in the replicated prices.</p>
<p>In the examination of these scatterplots, the distribution of the observed responses does not seem markably different from the distribution of the response in the simulated replicated datasets. Therefore in this brief examination, one does not see any indication of model misfit – the observed <span class="math inline">\((x, y)\)</span> data seems consistent with replicated data generated from the posterior predictive distribution.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/regreplicated.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Scatterplots of observed and eight replicated datasets from the posterior predictive distribution.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>Predictive residuals</strong></p>
<p>In linear regression, one typically explores the residuals that are the deviations of the observations <span class="math inline">\(\{y_i\}\)</span> from the fitted regression model. The posterior prediction distribution is used to define a suitable Bayesian residual.</p>
<p>Consider the observed point (<span class="math inline">\(x_i, y_i\)</span>). One asks the question – is the observed response value <span class="math inline">\(y_i\)</span> consistent with predictions <span class="math inline">\(\tilde{y}_i\)</span> of this observation from the fitted model? One simulates predictions <span class="math inline">\(\tilde{y}_i\)</span> from the posterior predictive distribution in two steps:</p>
<ol type="1">
<li>One simulates <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span> from the posterior distribution.</li>
<li>One simulates <span class="math inline">\(\tilde{y}_i\)</span> from a Normal distribution with mean <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</li>
</ol>
<p>By repeating this process many times, one has a sample of values {<span class="math inline">\(\tilde{y}_i\)</span>} from the posterior predictive distribution.</p>
<p>To see how close the observed response <span class="math inline">\(y_i\)</span> is to the predictions {<span class="math inline">\(\tilde{y}_i\)</span>}, one computes the predictive residual <span class="math display">\[\begin{equation}
r_i = y_i - \tilde{y}_i.
\end{equation}\]</span> If this predictive residual is away from zero, that indicates that the observation is not consistent with the linear regression model. Remember that <span class="math inline">\(\tilde{y}_i\)</span>, and therefore the predictive residual <span class="math inline">\(r_i\)</span> is random. So one constructs a 90% interval estimate for the predictive residual <span class="math inline">\(r_i\)</span> and says that the observation is unusual if the predictive residual interval estimate does not include zero.</p>
<p>Figure 11.9 displays a graph of the 90% interval estimates for the predictive residuals {<span class="math inline">\(r_i\)</span>} plotted against the size variable. A horizontal line at the value 0 is displayed and we look for intervals that are located on one side of zero. One notices that a few of the intervals barely overlap zero – this indicates that the corresponding points <span class="math inline">\((x_i, y_i)\)</span> are somewhat inconsistent with the fitted regression model.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/ppresiduals.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Display of predictive residuals. Each line covers 90% of the probability of the predictive residual.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="informative-prior" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="informative-prior"><span class="header-section-number">6.8</span> Informative Prior</h2>
<p>One challenge in a Bayesian analysis is the construction of a prior that reflects beliefs about the parameters. In the usual linear function representation in Equation (11.7), thinking about prior beliefs can be difficult since the intercept <span class="math inline">\(\beta_0\)</span> does not have a meaningful interpretation. To make the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> easier to interpret, one considers standardizing the response and predictor variables. With this standardization, the task of constructing informative priors will be facilitated.</p>
<section id="standardization" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="standardization"><span class="header-section-number">6.8.1</span> Standardization</h3>
<p>Standardization is the process of putting different variables on similar scales. As we can see in Figure 11.2, the house size variable ranges from 1.0 to over 2.5 (in 1000 sq feet), while the price variable ranges from below 50 to over 350 (in $1000). The standardization process works as follows: for each variable, calculate the sample mean and the sample standard deviation, and then for each observed value of the variable, subtract the sample mean and divide by the sample standard deviation.</p>
<p>For example, let <span class="math inline">\(y_i\)</span> be the observed sale price and <span class="math inline">\(x_i\)</span> be the size of a house. Let <span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{x}\)</span> denote the sample means and <span class="math inline">\(s_y\)</span> and <span class="math inline">\(s_x\)</span> denote the sample standard deviations for the <span class="math inline">\(y_i\)</span>’s and <span class="math inline">\(x_i\)</span>’s, respectively. Then the standardized variables <span class="math inline">\(y_i^*\)</span> and <span class="math inline">\(x_i^*\)</span> are defined by the following formula. <span class="math display">\[\begin{equation}
y_i^* = \frac{y_i - \bar{y}}{s_y},  \, \, x_i^* = \frac{x_i - \bar{x}}{s_x}.
\end{equation}\]</span></p>
<p>In R, the function <code>scale()</code> performs standardization.</p>
<pre class="{r"><code>PriceAreaData$price_standardized &lt;- scale(PriceAreaData$price)
PriceAreaData$size_standardized &lt;- scale(PriceAreaData$newsize)</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/PriceAreaScatterplot_orig_and_standardized.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Two scatterplots of price against size of house sales: both variables unstandardized (top) and both variables standardized (bottom).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A standardized value represents the number of standard deviations that the value falls above or below the mean. For example, if <span class="math inline">\(x_i^* = -2\)</span>, then this house size is two standard deviations below the mean of all house sizes, and a value <span class="math inline">\(y_i = 1\)</span> indicates a sale price that is one standard deviation larger than the mean. Figure 11.10 constructs a scatterplot of the original <span class="math inline">\((x, y)\)</span> data (top) and the standardized <span class="math inline">\((x^*, y^*)\)</span> data (bottom). Note that the ranges of the standardized scores for the <span class="math inline">\(x^*\)</span> and <span class="math inline">\(y^*\)</span> are similar – both sets of standardized scores fall between <span class="math inline">\(-2\)</span> and 2. Also note that the association pattern of the two graphs agree which indicates that the standardization procedure has no impact on the relationship of house size with the sale price.</p>
<p>One advantage of standardization of the variables is that it provides more meaningful interpretations of the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. The linear regression model with the standardized variables is written as follows:</p>
<p><span class="math display">\[\begin{equation}
Y_i^* \mid \mu_i^*, \sigma \overset{ind}{\sim} \textrm{Normal}(\mu_i^*, \sigma),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\mu_i^* = \beta_0 + \beta_1 x_i^*.
\end{equation}\]</span></p>
<p>The intercept parameter <span class="math inline">\(\beta_0\)</span> now is the expected standardized sale price for a house where <span class="math inline">\(x_i^* = 0\)</span> corresponding to a house of average size. The slope <span class="math inline">\(\beta_1\)</span> gives the change in the expected standardized sale price <span class="math inline">\(\mu_i^*\)</span> when the standardized size <span class="math inline">\(x_i^*\)</span> increases by 1 unit, or when the size variable increases by one standard deviation. In addition, when the variables are standardized, the slope <span class="math inline">\(\beta_1\)</span> can be shown equal to the correlation between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>. So this slope provides a meaningful measure of the linear relationship between the standardized predictor <span class="math inline">\(x_i^*\)</span> and the expected standardized response <span class="math inline">\(\mu_i^*\)</span>. A positive value <span class="math inline">\(\beta_1\)</span> indicates a positive linear relationship between the two variables, and the absolute value of <span class="math inline">\(\beta_1\)</span> indicates the strength of the relationship.</p>
</section>
<section id="prior-distributions" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="prior-distributions"><span class="header-section-number">6.8.2</span> Prior distributions</h3>
<p>As in the weakly informative prior case, assume that the three parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma\)</span> are independent so the joint prior is factored into the marginal components. <span class="math display">\[\begin{equation*}
\pi(\beta_0, \beta_1, \sigma) = \pi(\beta_0) \pi(\beta_1) \pi(\sigma).
\end{equation*}\]</span> Then the task of assigning a joint prior simplifies to the task of assigning priors separately to each of the three parameters. The process of assigning an informative prior is described for each parameter.</p>
<p><strong>Prior on the intercept <span class="math inline">\(\beta_0\)</span></strong></p>
<p>After the data is standardized, recall that the intercept <span class="math inline">\(\beta_0\)</span> represents the expected standardized sale price given a house of average size (i.e.&nbsp;<span class="math inline">\(x_i^* = 0\)</span>). If one believes a house of average size will also have an average price, then a reasonable guess of <span class="math inline">\(\beta_0\)</span> is zero. One can give a Normal prior for <span class="math inline">\(\beta_0\)</span> with mean <span class="math inline">\(\mu_0 = 0\)</span> and standard deviation <span class="math inline">\(s_0\)</span>: <span class="math display">\[\begin{equation*}
\beta_0 \sim \textrm{Normal}(0, s_0).
\end{equation*}\]</span></p>
<p>The standard deviation <span class="math inline">\(s_0\)</span> in the Normal prior reflects how confident the person believes in the guess of <span class="math inline">\(\beta_0 = 0\)</span>. For example, if one specifies <span class="math inline">\(\beta_0 \sim \textrm{Normal}(0, 1)\)</span>, this indicates that a price of a house of average size could range from one standard deviation below to one standard deviation above the average price. Since this is a wide range, one is stating that he or she is unsure that a house of average size will have an average price. If one instead is very sure of the guess that <span class="math inline">\(\beta_0 = 0\)</span>, one could choose a smaller value of <span class="math inline">\(s_0\)</span>.</p>
<p><strong>Prior on the slope <span class="math inline">\(\beta_1\)</span></strong></p>
<p>For standardized data, the slope <span class="math inline">\(\beta_1\)</span> represents the correlation between the house size and the sale price. One represents one’s belief about the location of <span class="math inline">\(\beta_1\)</span> by means of a Normal prior. <span class="math display">\[\begin{equation*}
\beta_1 \sim \textrm{Normal}(\mu_1, s_1),
\end{equation*}\]</span> For this prior, <span class="math inline">\(\mu_1\)</span> represents one’s best guess of the correlation and <span class="math inline">\(s_1\)</span> represents the sureness of this guess. For example, if one lets <span class="math inline">\(\beta_1\)</span> be <span class="math inline">\(\textrm{Normal}(0.7, 0.15)\)</span>, this means that one’s “best guess” of the correlation is 0.7 and one is pretty certain that the correlation falls between <span class="math inline">\(0.7 - 0.15\)</span> and <span class="math inline">\(0.7 + 0.15\)</span>. If one is not very sure of the guess of 0.7, one could choose a larger value of <span class="math inline">\(s_1\)</span>.</p>
<p><strong>Prior on <span class="math inline">\(\sigma\)</span></strong></p>
<p>It is typically harder to specify informative beliefs about a standard deviation than a mean parameter such as <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. So it seems reasonable to assign a weakly informative prior for the sampling error standard deviation <span class="math inline">\(\sigma\)</span>. A Gamma prior for the precision parameter <span class="math inline">\(\phi = 1/\sigma^2\)</span> with small values of the shape and rate parameters, say <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b = 1\)</span>, can represent weak prior information in this regression setting. <span class="math display">\[\begin{equation*}
1/\sigma^2 \sim \textrm{Gamma}(1, 1).
\end{equation*}\]</span></p>
<p>To summarize, the informative prior distribution for (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\sigma\)</span>) is defined as follows. <span class="math display">\[\begin{equation}
\pi(\beta_0, \beta_1, \sigma) = \pi(\beta_0)
\pi(\beta_1) \pi(\sigma),
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\beta_0 \sim \textrm{Normal}(0, 1),
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\beta_1 \sim \textrm{Normal}(0.7, 0.15),
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
1/\sigma^2 \sim \textrm{Gamma}(1, 1).
\end{equation}\]</span></p>
</section>
<section id="posterior-analysis-1" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="posterior-analysis-1"><span class="header-section-number">6.8.3</span> Posterior Analysis</h3>
<p></p>
<p>One again uses the JAGS software to simulate from the posterior distribution of the parameters. The <code>modelString</code> is written in the same way as in Section 11.6.</p>
<p>Since the data have been standardized, one needs to do some initial preliminary work before the MCMC implementation.<br>
First, in R, one defines new variables <code>price_standardized</code> and <code>size_standardized</code> that are standardized versions of the original <code>price</code> and <code>newsize</code> variables.</p>
<pre><code>PriceAreaData$price_standardized &lt;- scale(PriceAreaData$price)
PriceAreaData$size_standardized &lt;- scale(PriceAreaData$newsize)</code></pre>
<p>Then the variables <code>y</code> and <code>x</code> in <code>modelString</code> now correspond to the standardized data. Also in the definition of the <code>the_data</code> list, we enter the mean and precision values of the informative priors placed on the regression intercept and slope. Remember that one needs to convert the prior standard deviations <span class="math inline">\(s_0\)</span> and <span class="math inline">\(s_1\)</span> to the corresponding precision values.</p>
<pre><code>y &lt;- as.vector(PriceAreaData$price_standardized)  
x &lt;- as.vector(PriceAreaData$size_standardized)
N &lt;- length(y)  
the_data &lt;- list("y" = y, "x" = x, "N" = N,
                 "mu0" = 0, "g0" = 1,
                 "mu1" = 0.7, "g1" = 44.4,
                 "a" = 1, "b" = 1)</code></pre>
<p>With the redefinition of the standardized variables <code>y</code> and <code>x</code>, the same JAGS script <code>modelString</code> is used to define the posterior distribution. As before, the <code>run.jags()</code> function is run, collecting a sample of 5000 draws from <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span>.</p>
<pre><code>posterior2 &lt;- run.jags(modelString,
                       n.chains = 1,
                       data = the_data,
                       monitor = c("beta0", "beta1", "sigma"),
                       adapt = 1000,
                       burnin = 5000,
                       sample = 5000)</code></pre>
<p><strong>Comparing posteriors for two priors</strong></p>
<p>To understand the influence of the informative prior, one can contrast this posterior distribution with a posterior using a weakly informative prior. Suppose one assumes that <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma\)</span> are independent with <span class="math inline">\(\beta_0 \sim \textrm{Normal}(0, 100)\)</span>, <span class="math inline">\(\beta_1 \sim \textrm{Normal}(0.7, 100)\)</span> and <span class="math inline">\(\phi = 1 / \sigma^2 \sim \textrm{Gamma}(1, 1)\)</span>. This prior differs from the informative prior in that large values are assigned to the standard deviations, reflecting weak information about the location of the regression intercept and slope.</p>
<pre><code>the_data &lt;- list("y" = y, "x" = x, "N" = N,
                 "mu0" = 0, "g0" = 0.0001,
                 "mu1" = 0.7, "g1" = 0.0001,
                 "a" = 1, "b" = 1)</code></pre>
<pre><code>posterior3 &lt;- run.jags(modelString,
                       n.chains = 1,
                       data = the_data,
                       monitor = c("beta0", "beta1", "sigma"),
                       adapt = 1000,
                       burnin = 5000,
                       sample = 5000)</code></pre>
<p>Figure 11.11 displays density estimates of the simulated posterior draws of the slope parameter <span class="math inline">\(\beta_1\)</span> under the informative and weakly informative prior distributions. Note that the “informative prior” posterior has less spread than the “weakly informative prior” posterior. This is to be expected since the informative prior adds more information about the location of the slope parameter. In addition, the “informative prior” posterior shifts the “weakly informative prior” posterior towards the prior belief that the slope is close to the value 0.7.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/twoposteriorslopes.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Density plots of posterior distributions of regression slope parameter using informative and weakly informative prior distributions.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>After viewing Figure 11.11, one would expect the posterior interval estimate for the slope <span class="math inline">\(\beta_1\)</span> to be shorter with the informative prior. We had earlier found that the 90% interval estimate for <span class="math inline">\(\beta_1\)</span> to be (0.551, 0.959) with the informative prior. The 90% interval for the slope with the weakly informative prior is (0.501, 1.08) which is about 40% longer than the interval using the informative prior.</p>
<pre><code>print(posterior2, digits = 3)
      Lower95   Median Upper95     Mean    SD Mode   MCerr 
beta0  -0.267 0.000358   0.276 0.000372 0.138   -- 0.00195   
beta1   0.551    0.751   0.959    0.749 0.104   -- 0.00147   
sigma   0.498     0.67   0.878    0.682 0.102   -- 0.00154</code></pre>
<pre><code>print(posterior3, digits = 3)
      Lower95   Median Upper95     Mean    SD Mode   MCerr 
beta0  -0.273 0.000362   0.281 0.000421 0.141   -- 0.00199    
beta1   0.501    0.794    1.08    0.792 0.146   -- 0.00207     
sigma   0.502    0.677   0.894    0.688 0.105   -- 0.00163  </code></pre>
</section>
</section>
<section id="a-conditional-means-prior" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="a-conditional-means-prior"><span class="header-section-number">6.9</span> A Conditional Means Prior</h2>
<p></p>
<p>In this chapter, we have illustrated two methods for constructing a prior on the parameters of a regression model. The first method reflects weakly informative prior beliefs about the parameters, and the second method assesses an informative prior on the regression parameters on a model on standardized data. In this section, a third method is described for representing prior beliefs on a regression model on the original data. This approach assesses a prior on <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span> indirectly by stating prior beliefs about the expected response value conditional on specific values of the predictor variable.</p>
<p><strong>Learning about a gas bill from the outside temperature</strong></p>
<p>A homeowner will typically have monthly payments on basic utilities such as water, natural gas, and electricity. One particular homeowner observes that her monthly natural gas bill seems to vary across the year. The bill is larger for colder months and smaller for warmer months. That raises the question: can one accurately predict one’s monthly natural gas bill from the outside temperature?</p>
<p>To address this question, the homeowner collects the monthly gas bill in dollars and the average monthly outside temperature for all twelve months in a particular year. Figure 11.12 displays a scatterplot of the temperatures and bill amounts. Note that the month bill appears to decrease as a function of the temperature. This motivates consideration of the linear regression model <span class="math display">\[\begin{equation}
Y_i \mid \beta_0, \beta_1, \sigma \sim {\rm Normal}(\beta_0 + \beta_1 x_i, \sigma),
\end{equation}\]</span> where <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are respectively the average temperature (degrees in Fahrenheit) and the bill amount (in dollars) in month <span class="math inline">\(i\)</span>, and <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span> are the unknown regression parameters.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/temp_gas_plot.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Scatterplot of average temperature and gas bill for twelve payments.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>A conditional means prior</strong></p>
<p>To construct a prior, first assume that one’s beliefs about the regression parameters <span class="math inline">\((\beta_0, \beta_1)\)</span> are independent of the beliefs on the standard deviation <span class="math inline">\(\sigma\)</span> and so the joint prior can be factored into the marginal densities: <span class="math display">\[\begin{equation*}
\pi(\beta_0, \beta_1, \sigma) = \pi(\beta_0, \beta_1) \pi(\sigma).
\end{equation*}\]</span> With the unstandardized data, it is difficult to think directly about plausible values of the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> and also how these regression parameters are related. But it may be easier to formulate prior opinion about the mean values <span class="math display">\[\begin{equation}
\mu_i^* = \beta_0 + \beta_1 x_i^*,
\end{equation}\]</span> for two specified values of the predictor <span class="math inline">\(x_1^*\)</span> and <span class="math inline">\(x_2^*\)</span>. The conditional means approach proceeds in two steps.</p>
<ol type="1">
<li>For the first predictor value <span class="math inline">\(x_1^*\)</span> construct a Normal prior for the mean value <span class="math inline">\(\mu_1^*\)</span>. Let the mean and standard deviation values of this prior be denoted by <span class="math inline">\(m_1\)</span> and <span class="math inline">\(s_1\)</span>, respectively.</li>
<li>Similarly, for the second predictor value <span class="math inline">\(x_2^*\)</span> construct a Normal prior for the mean value <span class="math inline">\(\mu_2^*\)</span> with respective mean and standard deviation <span class="math inline">\(m_2\)</span> and <span class="math inline">\(s_2\)</span>.</li>
</ol>
<p>If one assumes that one’s beliefs about the conditional means are independent, then the joint prior for the vector <span class="math inline">\((\mu_1^*, \mu_2^*)\)</span> has the form <span class="math display">\[\begin{equation*}
\pi(\mu_1^*, \mu_2^*) = \pi(\mu_1^*) \pi(\mu_2^*).
\end{equation*}\]</span></p>
<p>This prior on the two conditional means implies a Bivariate Normal prior on the regression parameters. The two conditional means <span class="math inline">\(\mu_1^*\)</span> and <span class="math inline">\(\mu_2^*\)</span> were written above as a function of the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. By solving these two equations for the regression parameters, one expresses each parameter as a function of the conditional means: <span class="math display">\[\begin{equation}
\beta_1 = \frac{\mu_2^* - \mu_1^*}{x_2 - x_1},
\label{eq:cmp:beta1}
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\beta_0 = \mu_1^* - x_1 \left(\frac{\mu_2^* - \mu_1^*}{x_2 - x_1}\right).
\label{eq:cmp:beta0}
\end{equation}\]</span> Note that both the slope <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are linear functions of the two conditional means <span class="math inline">\(\mu_1^*\)</span> and <span class="math inline">\(\mu_2^*\)</span> and this implies that <span class="math inline">\(\beta_0, \beta_1\)</span> will have a Bivariate Normal distribution.</p>
<p><strong>Regression analysis of the gas bill example</strong></p>
<p>The process of constructing a conditional means prior is illustrated for our gas bill example. Consider two different temperature values, say 40 degrees and 60 degrees, and, for each temperature, construct a Normal prior for the expected monthly bill. After some thought, the following priors are assigned.</p>
<ul>
<li><p>If <span class="math inline">\(x = 40\)</span>, the mean bill <span class="math inline">\(\mu_1^* = \beta_0 + \beta_1 (40)\)</span> is Normal with mean $100 and standard deviation $20. This statement indicates that one believes the average gas bill will be relatively high during a cold month averaging 40 degrees.</p></li>
<li><p>If <span class="math inline">\(x = 60\)</span>, the mean bill <span class="math inline">\(\mu_2^* = \beta_0 + \beta_1 (100)\)</span> is Normal with mean $50 and standard deviation $15. Here the month’s average temperature is warmer and one believes the gas cost will average $50 lower than in the first scenario.</p></li>
</ul>
<p>By assuming independence of our prior beliefs about the two means, we have <span class="math display">\[\begin{equation}
\pi(\mu_1^*, \mu_2^*) = \phi(\mu_1^*, 100, 20) \phi(\mu_2^*, 50, 15),
\end{equation}\]</span> where <span class="math inline">\(\phi(y, \mu, \sigma)\)</span> denotes the Normal density with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</p>
<p>The prior on the two means is an indirect way of assessing a prior on the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. One simulate pairs <span class="math inline">\((\beta_0, \beta_1)\)</span> from the prior distribution by simulating values of the means <span class="math inline">\(\mu_1^*\)</span> and <span class="math inline">\(\mu_2^*\)</span> from independent Normal distributions and applying Equation (11.22) and Equation (11.23).</p>
<p>Simulated draws from the prior are conveniently produced using the JAGS software. The prior is specified for the conditional means by two applications of the <code>dnorm()</code> function and the regression parameters are defined as functions of the conditional means. The prior standard deviations of beta0 and beta1 are 20 and 15 and so the corresponding precisions are 1 / 20 ^ 2 and 1 / 15 ^ 2. These precision values <code>s1</code> and <code>s2</code> (not the standard deviations) are used in the JAGS script.</p>
<pre><code>modelString = "
model{
beta1 &lt;- (mu2 - mu1) / (x2 - x1)
beta0 &lt;- mu1 - x1 * (mu2 - mu1) / (x2 - x1)
mu1 ~ dnorm(m1, s1)
mu2 ~ dnorm(m2, s2)
}"</code></pre>
<p>Figure 11.13 displays 1000 simulated draws of <span class="math inline">\((\beta_0, \beta_1)\)</span> from the the conditional means prior. It is interesting to note that although the conditional means <span class="math inline">\(\mu_1^*\)</span> and <span class="math inline">\(\mu_2^*\)</span> are independent, the implied prior on the regression coefficients indicates that <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are strongly negatively correlated.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter11/cond_means_prior.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Scatterplot of simulated draws of the regression parameters from the conditional means prior.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The conditional means approach is used to indirectly specify a prior on the regression vector <span class="math inline">\(\beta = (\beta_0, \beta_1)\)</span>. To complete the prior, one assigns the precision parameter <span class="math inline">\(\phi = 1 / \sigma^2\)</span> a Gamma prior with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Then the prior density on all parameters has the form <span class="math display">\[\begin{equation*}
\pi(\beta_0, \beta_1, \sigma)  = \pi_{CM}(\beta_0, \beta_1) \pi(\sigma),
\end{equation*}\]</span> where <span class="math inline">\(\pi_{CM}\)</span> is the conditional means prior.</p>
<p>Using this conditional means prior and the gas bill data, one also uses JAGS to simulate from the posterior distribution of <span class="math inline">\((\beta_0, \beta_1, \sigma)\)</span>. In the exercises, the reader will have the opportunity to perform inference about the regression line. In addition, there will be an opportunity to compare inferences using conditional means and weakly informative priors.</p>


</section>

</main> <!-- /main -->
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./hierarchical.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Hierarchical Modeling</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./multipleregression.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Bayesian Multiple Regression and Logistic Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>