<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.309">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian Modeling - 5&nbsp; Bayesian Hierarchical Modeling</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<link href="./regression.html" rel="next">
<link href="./mcmc.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Hierarchical Modeling</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Modeling</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proportion.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Learning About a Binomial Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Modeling Measurement and Count Data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mcmc.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Simulation by Markov Chain Monte Carlo</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hierarchical.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Hierarchical Modeling</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multipleregression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Bayesian Multiple Regression and Logistic Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./casestudies.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Case Studies</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"> <span class="header-section-number">5.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#observations-in-groups" id="toc-observations-in-groups" class="nav-link" data-scroll-target="#observations-in-groups"> <span class="header-section-number">5.1.1</span> Observations in groups</a></li>
  <li><a href="#example-standardized-test-scores" id="toc-example-standardized-test-scores" class="nav-link" data-scroll-target="#example-standardized-test-scores"> <span class="header-section-number">5.1.2</span> Example: standardized test scores</a></li>
  <li><a href="#separate-estimates" id="toc-separate-estimates" class="nav-link" data-scroll-target="#separate-estimates"> <span class="header-section-number">5.1.3</span> Separate estimates?</a></li>
  <li><a href="#combined-estimates" id="toc-combined-estimates" class="nav-link" data-scroll-target="#combined-estimates"> <span class="header-section-number">5.1.4</span> Combined estimates?</a></li>
  <li><a href="#a-two-stage-prior-leading-to-compromise-estimates" id="toc-a-two-stage-prior-leading-to-compromise-estimates" class="nav-link" data-scroll-target="#a-two-stage-prior-leading-to-compromise-estimates"> <span class="header-section-number">5.1.5</span> A two-stage prior leading to compromise estimates</a></li>
  </ul></li>
  <li><a href="#hierarchical-normal-modeling" id="toc-hierarchical-normal-modeling" class="nav-link" data-scroll-target="#hierarchical-normal-modeling"> <span class="header-section-number">5.2</span> Hierarchical Normal Modeling</a>
  <ul class="collapse">
  <li><a href="#example-ratings-of-animation-movies" id="toc-example-ratings-of-animation-movies" class="nav-link" data-scroll-target="#example-ratings-of-animation-movies"> <span class="header-section-number">5.2.1</span> Example: ratings of animation movies</a></li>
  <li><a href="#a-hierarchical-normal-model-with-random-sigma" id="toc-a-hierarchical-normal-model-with-random-sigma" class="nav-link" data-scroll-target="#a-hierarchical-normal-model-with-random-sigma"> <span class="header-section-number">5.2.2</span> A hierarchical Normal model with random <span class="math inline">\(\sigma\)</span></a></li>
  <li><a href="#inference-through-mcmc" id="toc-inference-through-mcmc" class="nav-link" data-scroll-target="#inference-through-mcmc"> <span class="header-section-number">5.2.3</span> Inference through MCMC</a></li>
  </ul></li>
  <li><a href="#hierarchical-beta-binomial-modeling" id="toc-hierarchical-beta-binomial-modeling" class="nav-link" data-scroll-target="#hierarchical-beta-binomial-modeling"> <span class="header-section-number">5.3</span> Hierarchical Beta-Binomial Modeling</a>
  <ul class="collapse">
  <li><a href="#example-deaths-after-heart-attack" id="toc-example-deaths-after-heart-attack" class="nav-link" data-scroll-target="#example-deaths-after-heart-attack"> <span class="header-section-number">5.3.1</span> Example: Deaths after heart attack</a></li>
  <li><a href="#a-hierarchical-beta-binomial-model" id="toc-a-hierarchical-beta-binomial-model" class="nav-link" data-scroll-target="#a-hierarchical-beta-binomial-model"> <span class="header-section-number">5.3.2</span> A hierarchical Beta-Binomial model</a></li>
  <li><a href="#inference-through-mcmc-1" id="toc-inference-through-mcmc-1" class="nav-link" data-scroll-target="#inference-through-mcmc-1"> <span class="header-section-number">5.3.3</span> Inference through MCMC</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bayesian Hierarchical Modeling</span></h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">5.1</span> Introduction</h2>
<section id="observations-in-groups" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="observations-in-groups"><span class="header-section-number">5.1.1</span> Observations in groups</h3>
<p>Chapters 7, 8, and 9 make an underlying assumption about the source of data: observations are assumed to be identically and independently distributed (i.i.d.) following a single distribution with one or more unknown parameters. In Chapter 7, the Binomial data model is based on the assumptions that a student’s chance of preferring dining out on Friday is the same for all students, and the dining preferences of different students are independent. To refresh your memory, recall the four conditions of a Binomial experiment: a fixed number of trials, only two outcomes, a fixed success probability, and independent trials. In Chapter 8, the Normal sampling model is based on the assumptions that Roger Federer’s time-to-serves are independent observations following a single Normal distribution with an unknown mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma\)</span>. That is, <span class="math inline">\(Y_i \overset{i.i.d.}{\sim} \textrm{Normal}(\mu, \sigma)\)</span>. Similarly in Chapter 9, the underlying assumption is that the snowfall amounts in Buffalo for the last 20 Januarys follow the same <span class="math inline">\(\textrm{Normal}(\mu, \sigma)\)</span> distribution with both parameters unknown.</p>
<p>In many situations, treating observations as i.i.d. from the same distribution with the same parameter(s) is not sensible. In our dining out example, dining preferences for students may be different from dining performances of senior citizens, so it would not make sense to use a single success probability for a combined group of students and senior citizens. In a similar fashion, if one considered time-to-serve data for a group of tennis players, then it would not be reasonable to use a single Normal distribution with a single mean to represent these data – the mean time-to-serve for a quick-serving player would likely be smaller than the mean time-to-serve for a slower player. For many applications, some observations share characteristics, such as age or player, that distinguish them from other observations, therefore multiple distinct groups are observed.</p>
</section>
<section id="example-standardized-test-scores" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="example-standardized-test-scores"><span class="header-section-number">5.1.2</span> Example: standardized test scores</h3>
<p>As a new example, consider a study in which students’ scores of a standardized test such as the SAT are collected from five different senior high schools in a given year. Suppose a researcher is interested in learning about the mean SAT score. Since five different schools participated in this study and students’ scores might vary from school to school, it makes sense for the researcher to learn about the mean SAT score for each school and compare students’ mean performance across schools.</p>
<p>To start modeling this education data, it is inappropriate to use <span class="math inline">\(Y_i\)</span> as the random variable for the SAT score of student <span class="math inline">\(i\)</span> (<span class="math inline">\(i = 1, \cdots, n\)</span>, where <span class="math inline">\(n\)</span> is the total number of students from all five schools) since this ignores the inherent grouping of the observations. Instead, the researcher adds a school label <span class="math inline">\(j\)</span> to <span class="math inline">\(Y_i\)</span> to reflect the grouping. Let <span class="math inline">\(Y_{ij}\)</span> denote the SAT score of student <span class="math inline">\(i\)</span> in school <span class="math inline">\(j\)</span>, where <span class="math inline">\(j = 1, \cdots, 5\)</span>, and <span class="math inline">\(i = 1, \cdots, n_j\)</span>, where <span class="math inline">\(n_j\)</span> is the number of students in school <span class="math inline">\(j\)</span>, and <span class="math inline">\(n = \sum_{j=1}^{5}n_j\)</span>.</p>
<p>Since SAT scores are continuous, the Normal sampling model is a reasonable choice for a data distribution. Within school <span class="math inline">\(j\)</span>, one assumes that SAT scores are i.i.d. from a Normal data model with a mean and standard deviation depending on the school. Specifically, one assumes a school-specific mean <span class="math inline">\(\mu_j\)</span> and a school-specific standard deviation <span class="math inline">\(\sigma_j\)</span> for the Normal data model for school <span class="math inline">\(j\)</span>. Combining the information for the five schools, one has</p>
<p><span class="math display">\[\begin{equation}
Y_{ij} \overset{i.i.d.}{\sim} \textrm{Normal}(\mu_j, \sigma_j),
\label{eq:introLik}
\end{equation}\]</span> where <span class="math inline">\(j = 1, \cdots, 5\)</span> and <span class="math inline">\(j = 1, \cdots, n_j\)</span>.</p>
</section>
<section id="separate-estimates" class="level3" data-number="5.1.3">
<h3 data-number="5.1.3" class="anchored" data-anchor-id="separate-estimates"><span class="header-section-number">5.1.3</span> Separate estimates?</h3>
<p>One approach for handling this group estimation problem is find separate estimates for each school. One focuses on the observations in school <span class="math inline">\(j\)</span>,<span class="math inline">\(\{Y_{1j}, Y_{2j}, \cdots, Y_{n_jj}\}\)</span>, choose a prior distribution <span class="math inline">\(\pi(\mu_j, \sigma_j)\)</span> for the mean and the standard deviation parameters, follow the Bayesian inference procedure in Chapter 9 and obtain posterior inference on <span class="math inline">\(\mu_j\)</span> and <span class="math inline">\(\sigma_j\)</span>.<br>
If one assumes that the prior distributions on the individual parameters for the schools are independent, one is essentially fitting five separate Bayesian models and one’s inferences about one particular school will be independent of the inferences on the remaining schools.</p>
<p>This “separate estimates” approach may be reasonable, especially if the researcher thinks the means and the standard deviations from the five Normal models are completely unrelated to each other. That is, one’s prior beliefs about the parameters of the SAT score distribution in one school are unrelated to the prior beliefs about the distribution parameters in another school.</p>
<p>To see if this assumption is reasonable, let us consider a thought experiment for the school testing example. Suppose you are interested in learning about the mean SAT score <span class="math inline">\(\mu_N\)</span> for school <span class="math inline">\(N\)</span>. You may not be familiar with the distribution of SAT scores and it would be difficult to construct an informative prior for <span class="math inline">\(\mu_N\)</span>. But suppose that you are told that the students from another school, call it school <span class="math inline">\(M\)</span>, average 1,200 on their SAT scores. That information would likely influence your prior on <span class="math inline">\(\mu_N\)</span>, since now you have some general idea about SAT scores. This means that your prior beliefs about the mean SAT scores <span class="math inline">\(\mu_N\)</span> and <span class="math inline">\(\mu_M\)</span> are not independent – some information about one school’s mean SAT scores would change your prior on the second school’s mean SAT score. So in many situations, this independence assumption would be questionable.</p>
</section>
<section id="combined-estimates" class="level3" data-number="5.1.4">
<h3 data-number="5.1.4" class="anchored" data-anchor-id="combined-estimates"><span class="header-section-number">5.1.4</span> Combined estimates?</h3>
<p>Another way to handle this group estimation problem is to ignore the fact that there is a grouping variable and estimate the parameters in the combined sample. In our school example, one ignores the school variable and simply assumes that the SAT scores <span class="math inline">\(Y_i's\)</span> are distributed from a single Normal population with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. Here, <span class="math inline">\(i = 1, \cdots, n\)</span> where <span class="math inline">\(n\)</span> is the total number of students from all five schools.</p>
<p>If ones ignores the grouping variable, then the inference procedure described in Chapter 9 can be used. One constructs a prior for the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and use Gibbs sampling to obtain a simulated sample from the posterior distribution of <span class="math inline">\((\mu, \sigma)\)</span>.</p>
<p>Using this approach, one is effectively ignoring any differences between the five schools. Although it is reasonable to assume some similarity in the SAT scores across different schools, one probably does not believe that the schools are indistinguishable. In fact, state officials assume the schools have distinct features such as student bodies with different socioeconomic statuses so that SAT scores from different schools can be substantially different. In some states in the United States, all schools are ranked on different criteria which reflects the belief that schools are different with respect to student achievement.</p>
</section>
<section id="a-two-stage-prior-leading-to-compromise-estimates" class="level3" data-number="5.1.5">
<h3 data-number="5.1.5" class="anchored" data-anchor-id="a-two-stage-prior-leading-to-compromise-estimates"><span class="header-section-number">5.1.5</span> A two-stage prior leading to compromise estimates</h3>
<p>If one applies the “separate estimates” approach, one performs separate analyses on the different groups, and one ignores any prior knowledge about the similarity between the groups. On the other extreme, the “combined estimates” approach ignores the grouping variable and assumes that the groups are identical with respect to the response variable SAT score. Is there an alternative approach that compromises between the separate and combined estimate methods?</p>
<p>Let us return to the model <span class="math inline">\(\textrm{Normal}(\mu_j, \sigma_j)\)</span> where <span class="math inline">\(\mu_j\)</span> is the parameter representing the mean SAT score of students in school <span class="math inline">\(j\)</span>. For simplicity of discussion it is assumed the standard deviation <span class="math inline">\(\sigma_j\)</span> of the <span class="math inline">\(j\)</span>-th school is known. Consider the collection of five mean parameters, <span class="math inline">\(\{\mu_1, \mu_2, \mu_3, \mu_4, \mu_5\}\)</span> representing the means of the five schools’ SAT scores. One believes that the <span class="math inline">\(\mu_j\)</span>’s are distinct, because each <span class="math inline">\(\mu_j\)</span> depends on the characteristics of school <span class="math inline">\(j\)</span>, such as size and socioeconomic status. But one also believes that the mean parameters are similar in size. Imagine if you were given some information about the location of one mean, say <span class="math inline">\(\mu_j\)</span>, then this information would influence your beliefs about the location of another mean <span class="math inline">\(\mu_k\)</span>. One wishes to construct a prior distribution for the five mean parameters that reflects the belief that <span class="math inline">\(\mu_1, \mu_2, \mu_3, \mu_4,\)</span> and <span class="math inline">\(\mu_5\)</span> are related or similar in size. This type of “similarity” prior ] allows one to combine the SAT scores of the five schools in the posterior distribution in such a way to obtain compromise estimates of the separate mean parameters.</p>
<p>The prior belief in similarity of the means is constructed in two stages.</p>
<ol type="1">
<li><p>[Stage 1] The prior distribution for the <span class="math inline">\(j\)</span>-th mean, <span class="math inline">\(\mu_j\)</span> is Normal, where the mean and standard deviation parameters are shared among all <span class="math inline">\(\mu_j\)</span>’s: <span class="math display">\[\begin{equation}
\mu_j \mid \mu, \tau \sim \textrm{Normal}(\mu, \tau), \, \, j = 1, ..., 5.
\label{eq:introPrior}
\end{equation}\]</span></p></li>
<li><p>[Stage 2] In the Stage 1 specification, the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are unknown. So this stage assigns the parameters a prior density <span class="math inline">\(\pi\)</span>. <span class="math display">\[\begin{equation}
\mu, \tau \sim \pi(\mu, \tau).
\end{equation}\]</span></p></li>
</ol>
<p>Several comments can be made about this two-stage prior.</p>
<ul>
<li><p>Specifying the same prior distribution for all <span class="math inline">\(\mu_j\)</span>’s at Stage 1 does not say that the <span class="math inline">\(\mu_j\)</span>’s are the same value. Instead, Stage1 indicates that the <span class="math inline">\(\mu_j\)</span>’s a priori are related and come from the same distribution. If the prior distribution <span class="math inline">\(\textrm{Normal}(\mu, \tau)\)</span> has a large standard deviation (that is, if <span class="math inline">\(\tau\)</span> is large), the <span class="math inline">\(\mu_j\)</span>’s can be very different from each other a priori. On the other hand, if the standard deviation <span class="math inline">\(\tau\)</span> is small, the <span class="math inline">\(\mu_j\)</span>’s will be very similar in size.</p></li>
<li><p>To follow up the previous comment, if one considers the limit of the Stage 1 prior as the standard deviation <span class="math inline">\(\tau\)</span> approaches zero, the group means <span class="math inline">\(\mu_j\)</span> will be identical. Then one is in the “combined groups” situation where one is pooling the SAT data to learn about a single population. At the other extreme, if one allows the standard deviation <span class="math inline">\(\tau\)</span> of the Stage 1 prior to approach infinity, then one is saying that the group means <span class="math inline">\(\mu_1, ..., \mu_5\)</span> are unrelated and that leads to the separate estimates situation.</p></li>
<li><p>In the school testing example, this prior <span class="math inline">\(\textrm{Normal}(\mu, \tau)\)</span> distribution is a model about all <span class="math inline">\(\mu_j\)</span>’s in the U.S., i.e.&nbsp;the population of SAT score means corresponding to all schools in the United States. The five schools in the dataset represent a sample from all schools in the U.S.</p></li>
<li><p>Since <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are parameters in the prior distribution, they are called hyperparameters. Learning about <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> provides information about the population of <span class="math inline">\(\mu_j\)</span>’s. Naturally in Bayesian inference, one learns about <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> by specifying a hyperprior distribution and performing inference based on the posterior distribution. In this example, inferences about <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> tell us about the location and spread of the population of mean SAT scores of schools in the U.S.</p></li>
</ul>
<p>To recap, one models continuous outcomes in groups through the school-specific sampling density in Equation (10.1) and the common Normal prior distribution in Equation (10.2) for the mean parameters. An important and appealing feature of this approach is learning simultaneously about each school (group) and learning about the population of schools (groups). Specifically in the current setup, the model simultaneously estimates the means for the schools (the <span class="math inline">\(\mu_j\)</span>’s) and the variation among the means (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>). It will be seen that the hierarchical model posterior estimates for one school borrows information from other schools. This process is often called “partial pooling” information among groups.</p>
<p>From the structural point of view, due to the two stages of the model, this approach is called hierarchical or multilevel modeling. In essence, hierarchical modeling takes into account information from multiple levels, acknowledging differences and similarities among groups. In the posterior analysis, one learns simultaneously about each group and learns about the population of groups by pooling information across groups.</p>
<p>In this chapter, hierarchical modeling is described in two situations that extend the Bayesian models for one proportion and one Normal mean described in Chapters 7 and 8, respectively. Section 10.2 introduces hierarchical Normal modeling using a sample of ratings of animation movies released in 2010; and Section 10.3 describes hierarchical Beta-Binomial modeling with an example of deaths after heart attack. In each section, we motivate the consideration of hierarchical models, outline the model structure, and implement model inference through Markov chain Monte Carlo simulation.</p>
</section>
</section>
<section id="hierarchical-normal-modeling" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="hierarchical-normal-modeling"><span class="header-section-number">5.2</span> Hierarchical Normal Modeling</h2>
<section id="example-ratings-of-animation-movies" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="example-ratings-of-animation-movies"><span class="header-section-number">5.2.1</span> Example: ratings of animation movies</h3>
<p>MovieLens is a website which provides personalized movie recommendations from users who create accounts and rate movies that they have seen. Based on such information, MovieLens works to build a custom preference profile for each user and provide movie recommendations. MovieLens is run by GroupLens Research, a research lab at the University of Minnesota, who has made MovieLens rating datasets available to the public. GroupLens Research regularly updates these datasets on their website and the datasets are useful for new research, education and development initiatives.</p>
<p>In one study, a sample from the MovieLens database was collected on movie ratings for eight different animation movies released in 2010. There are a total of 55 movie ratings, where a rating is is for a particular animation movie completed by a MovieLens user. The ratings are likely affected by the quality of the movie itself, as some movies are generally favored by the audience while others might be less favored. Therefore there exists a natural grouping of the 55 ratings by the movie title.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/ratingsplot.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Jittered dotplot of the ratings for the eight animation movies.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Figure 10.1 displays a jittered dotplot of the ratings grouped by movie title and Table 10.1 lists the sample mean, sample standard deviation, and the number of ratings for each title. Note the variability in the sample sizes – “Toy Story 3’ received 16 ratings and”Legend of the Guardians” and “Batman: Under the Red Hood” only received a single rating. For a movie with only one observed rating, such as “Legend of the Guardians” and “Batman: Under the Red Hood”, it would be difficult to learn much about its mean rating. Here it is desirable to improve the estimate of its mean rating by using rating information from similar movies.</p>
<p>Table 10.1. The movie title, the mean rating, the standard deviation of the ratings, and the number of ratings.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Movie Title</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">SD</th>
<th style="text-align: right;">N</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Batman: Under the Red Hood</td>
<td style="text-align: right;">5.00</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Despicable Me</td>
<td style="text-align: right;">3.72</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">How to Train Your Dragon</td>
<td style="text-align: right;">3.41</td>
<td style="text-align: right;">0.86</td>
<td style="text-align: right;">11</td>
</tr>
<tr class="even">
<td style="text-align: left;">Legend of the Guardians</td>
<td style="text-align: right;">4.00</td>
<td style="text-align: right;"></td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Megamind</td>
<td style="text-align: right;">3.38</td>
<td style="text-align: right;">1.31</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">Shrek Forever After</td>
<td style="text-align: right;">4.00</td>
<td style="text-align: right;">1.32</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Tangled</td>
<td style="text-align: right;">4.20</td>
<td style="text-align: right;">0.89</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="even">
<td style="text-align: left;">Toy Story 3</td>
<td style="text-align: right;">3.81</td>
<td style="text-align: right;">0.96</td>
<td style="text-align: right;">16</td>
</tr>
</tbody>
</table>
</section>
<section id="a-hierarchical-normal-model-with-random-sigma" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="a-hierarchical-normal-model-with-random-sigma"><span class="header-section-number">5.2.2</span> A hierarchical Normal model with random <span class="math inline">\(\sigma\)</span></h3>
<p>In this situation it is reasonable to develop a model for the movie ratings where the grouping variable is the movie title. We index a ratings by two subscripts, where <span class="math inline">\(Y_{ij}\)</span> denotes the <span class="math inline">\(i\)</span>-th rating for the <span class="math inline">\(j\)</span>-th movie title (<span class="math inline">\(j = 1, \cdots, 8\)</span>).</p>
<p>What sampling model should be used for the movie ratings? Since the ratings are continuous, it is reasonable to use the Normal data model described in Chapter 8. Recall that a Normal model has two parameters, the mean and the standard deviation. Based on previous reasoning, the mean parameter is assumed to be movie-specific, so <span class="math inline">\(\mu_j\)</span> will represent the mean of the ratings for movie <span class="math inline">\(j\)</span>. Thinking about the standard deviation parameter, should the standard deviation also be movie-specific, where <span class="math inline">\(\sigma_j\)</span> represents the standard deviation of the ratings for movie <span class="math inline">\(j\)</span>? Or can we assume a common value of the standard deviation, say <span class="math inline">\(\sigma\)</span>, across movies? For simplicity and ease of illustration, a common and shared unknown standard deviation <span class="math inline">\(\sigma\)</span> is assumed for all Normal models. This is a simplified version of random <span class="math inline">\(\sigma_j\)</span>’s — the more flexible hierarchical model with random <span class="math inline">\(\sigma_j\)</span>’s will be left as an end-of-chapter exercise.</p>
<p>One begins by writing down the sampling distributions for the ratings of the eight movies. Recall that <span class="math inline">\(Y_{ij}\)</span> denotes the <span class="math inline">\(i\)</span>-th rating of movie <span class="math inline">\(j\)</span>, where <span class="math inline">\(\mu_j\)</span> denote the mean of the Normal model for movie <span class="math inline">\(j\)</span>, and <span class="math inline">\(\sigma\)</span> denote the shared standard deviation of the Normal models across different movies. In our notation, <span class="math inline">\(n_j\)</span> represents the number of ratings for movie <span class="math inline">\(j\)</span>.</p>
<ul>
<li>Sampling, for <span class="math inline">\(j = 1, \cdots, 8\)</span> and <span class="math inline">\(i = 1, \cdots, n_j\)</span>: <span class="math display">\[\begin{equation}
Y_{ij} \mid \mu_j, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu_j, \sigma).
\end{equation}\]</span></li>
</ul>
<p>The next task is to set up a prior distribution for the eight mean parameters, <span class="math inline">\(\{\mu_1, \mu_2, \cdots, \mu_8\}\)</span> and the shared standard deviation parameter <span class="math inline">\(\sigma\)</span>. Focus first on the prior distribution for the mean parameters. Since these movies are all animations, it is reasonable to believe that the mean ratings are similar across movies. So one assigns each mean rating the same Normal prior distribution at the first stage:</p>
<ul>
<li>Prior for <span class="math inline">\(\mu_j\)</span>, <span class="math inline">\(j = 1, \cdots, 8\)</span>: <span class="math display">\[\begin{equation}
\mu_j \mid \mu, \tau \sim \textrm{Normal}(\mu, \tau).
\end{equation}\]</span></li>
</ul>
<p>As discussed in Section 10.1, this prior allows for a flexible method for pooling information across movies. If the prior distribution has a large standard deviation (e.g.&nbsp;a large value of <span class="math inline">\(\tau\)</span>), the <span class="math inline">\(\mu_j\)</span>’s are very different from each other a priori, and one would have modest pooling of the eight sets of ratings. If instead this prior has a small standard deviation (e.g.&nbsp;a small value of <span class="math inline">\(\tau\)</span>), the <span class="math inline">\(\mu_j\)</span>’s are very similar a priori and one would essentially be pooling the ratings to get an estimate at each of the <span class="math inline">\(\mu_j\)</span>. This shared prior <span class="math inline">\(\textrm{Normal}(\mu, \tau)\)</span> distribution among the <span class="math inline">\(\mu_j\)</span>’s simultaneously estimates both a mean for each movie (the <span class="math inline">\(\mu_j\)</span>’s) and also lets us learn about variation among the movies by the parameter <span class="math inline">\(\tau\)</span>.</p>
<p>The hyperparameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are treated as random since we are unsure about the degree of pooling of the eight sets of ratings. In typical practice, one specifies weakly informative hyperprior distributions for these “second-stage” parameters, indicating that one has little prior knowledge about the locations of these parameters. After observing data, inference is performed about <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> based on their posterior distributions. The posterior on the mean parameter <span class="math inline">\(\mu\)</span> is informative about an “average” mean rating and the posterior on <span class="math inline">\(\tau\)</span> lets one know about the variation among the <span class="math inline">\(\mu_j\)</span>’s in the posterior.</p>
<p>Treating <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> as random, one arrives at the following hierarchical model.</p>
<ul>
<li><p>Sampling: for <span class="math inline">\(j = 1, \cdots, 8\)</span> and <span class="math inline">\(i = 1, \cdots, n_j\)</span>: <span class="math display">\[\begin{equation}
Y_{ij} \mid \mu_j, \sigma \overset{i.i.d.}{\sim} \textrm{Normal}(\mu_j, \sigma).
\label{eq:NormalLik}
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(\mu_j\)</span>, Stage 1: <span class="math inline">\(\mu_j\)</span>, <span class="math inline">\(j = 1, \cdots, 8\)</span>: <span class="math display">\[\begin{equation}
\mu_j \mid \mu, \tau \sim \textrm{Normal}(\mu, \tau).
\label{eq:NormalMuPrior}
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(\mu_j\)</span>, Stage 2: <span class="math display">\[\begin{equation}
\mu, \tau \sim \pi(\mu, \tau).
\label{eq:NormalMuHyperprior}
\end{equation}\]</span></p></li>
</ul>
<p>In our model <span class="math inline">\(\pi(\mu, \tau)\)</span> denotes an arbitrary joint hyperprior distribution for the “Stage 2” hyperparameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>. When the MovieLens ratings dataset is analyzed, the specification of this hyperprior distribution will be described.</p>
<p>To complete the model, one needs to specify a prior distribution for the standard deviation parameter, <span class="math inline">\(\sigma\)</span>. As discussed in Chapter 9, when making inference about the standard deviation in a Normal model, one uses a Gamma prior on the precision (the inverse of the variance), for example,</p>
<ul>
<li>Prior for <span class="math inline">\(\sigma\)</span>: <span class="math display">\[\begin{eqnarray}
1/\sigma^2 \mid a_{\sigma}, b_{\sigma}  &amp;\sim&amp; \textrm{Gamma}(a_{\sigma}, b_{\sigma})
\label{eq:NormalSigmaPrior}
\end{eqnarray}\]</span></li>
</ul>
<p>One assigns a known Gamma prior distribution for <span class="math inline">\(1/\sigma^2\)</span>, with fixed hyperparameter values <span class="math inline">\(a_{\sigma}\)</span> and <span class="math inline">\(b_{\sigma}\)</span>. In some situations, one may consider the situation where <span class="math inline">\(a_{\sigma}\)</span> and <span class="math inline">\(b_{\sigma}\)</span> are random and assign hyperprior distributions for these unknown hyperparameters.</p>
<p>Before continuing to the graphical representation and simulation by MCMC using JAGS, it is helpful to contrast the two-stage prior distribution for {<span class="math inline">\(\mu_j\)</span>} and the one-stage prior distribution for <span class="math inline">\(\sigma\)</span>. The hierarchical model specifies a common prior for the means <span class="math inline">\(\mu_j\)</span>’s which induces sharing of information across ratings from different movies. On the other hand, the model uses a shared <span class="math inline">\(\sigma\)</span> for all movies which also induces sharing of information, though different from the sharing induced by the two-stage prior distribution for {<span class="math inline">\(\mu_j\)</span>}.</p>
<p>What is the difference between the two types of sharing? For the means {<span class="math inline">\(\mu_j\)</span>}, we have discussed that specifying a common prior distribution for different <span class="math inline">\(\mu_j\)</span>’s pools information across the movies. One is simultaneously estimating both a mean for each movie (the <span class="math inline">\(\mu_j\)</span>’s) and the variation among the movies (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>). For the standard deviation <span class="math inline">\(\sigma\)</span>, the hierarchical model also pools information across movies. However, all of the observations are combined in the estimation of <span class="math inline">\(\sigma\)</span>. Since separate values of <span class="math inline">\(\sigma_j\)</span>’s are not assumed, one cannot learn about the differences and similarities among the <span class="math inline">\(\sigma_j\)</span>’s. If one is interested in pooling information across movies for the <span class="math inline">\(\sigma_j\)</span>’s, one needs to allow random <span class="math inline">\(\sigma_j\)</span>’s, and specify a two-stage prior distribution for these parameters. Interested readers are encouraged to try out this approach as an end-of-chapter exercise.</p>
<p><strong>Graphical representation of the hierarchical model</strong></p>
<p>An alternative way of expressing this hierarchical model uses the following graphical representation.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figures/chapter10/treediagram1.png" class="img-fluid" width="500"></p>
</div>
</div>
<p>In the middle section of the graph, <span class="math inline">\(\{Y_{ij}\}\)</span> represents the collection of random variables for all ratings of movie <span class="math inline">\(j\)</span>, and the label to the left indicates the assumed Normal sampling distribution. The two parameters in the Normal sampling density, <span class="math inline">\(\mu_j\)</span> and <span class="math inline">\(\sigma\)</span>, are connected from above and below, with arrows pointing from the parameters to the random variables.</p>
<p>The upper section of the graph focuses on the <span class="math inline">\(\mu_j\)</span>’s. All means follow the same prior, a Normal distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\tau\)</span>. Therefore, arrows come from the common hyperparameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> to each <span class="math inline">\(\mu_j\)</span>. Since <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are random, these second-stage parameters are associated with the prior label <span class="math inline">\(\pi(\mu, \tau)\)</span>.</p>
<p>The lowest section of the graph is about <span class="math inline">\(\sigma\)</span>, or to be precise, <span class="math inline">\(1/\sigma^2\)</span>. If one wants to allow hyperparameters <span class="math inline">\(a_{\sigma}\)</span> and <span class="math inline">\(b_{\sigma}\)</span> to be random as well, the lower part of the graph grows further, in a similar manner as the upper section for <span class="math inline">\(\mu_j\)</span>.</p>
<p><strong>Second-stage prior</strong></p>
<p>The hierarchical Normal model presented in Equations (10.6) through (10.9) has not specified the hyperprior distribution <span class="math inline">\(\pi(\mu, \tau)\)</span>. How does one construct a prior on these second-stage hyperparameters?</p>
<p>Recall that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are parameters for the Normal prior distribution for {<span class="math inline">\(\mu_j\)</span>} the collection of eight different Normal sampling means. The mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\tau\)</span> in this Normal prior distribution reflect respectively the mean and spread of the mean ratings across eight different movies.</p>
<p>Following the discussion in Section 9.5.3, a typical approach for Normal models is to assign two independent prior distributions — a Normal distribution for the mean <span class="math inline">\(\mu\)</span> and a Gamma distribution for the precision <span class="math inline">\(1 / \tau^2\)</span>. Such a specification facilitates the use of the Gibbs sampling due to the availability of the conditional posterior distributions of both parameters (see the details of this work in Section 9.5.3). Using this approach, the density <span class="math inline">\(\pi(\mu, \tau)\)</span> is replaced by the two hyperprior distributions below.</p>
<ul>
<li>The hyperprior for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>: <span class="math display">\[\begin{equation}
\mu \mid \mu_0, \gamma_0 \sim  \textrm{Normal}(\mu_0, \gamma_0)
\end{equation}\]</span> <span class="math display">\[\begin{equation}
1/\tau^2 \mid a, b  \sim \textrm{Gamma}(a_{\tau}, b_{\tau})
\end{equation}\]</span></li>
</ul>
<p>The task of choosing a prior for <span class="math inline">\((\mu, \tau)\)</span> reduces to the problem of choosing values for the four hyperparameters <span class="math inline">\(\mu_0, \gamma_0, a_{\tau}\)</span>, and <span class="math inline">\(b_{\tau}\)</span>. If one believes that <span class="math inline">\(\mu\)</span> is located around the value of 3 and she is not very confident of this choice, the set of values <span class="math inline">\(\mu_0 = 3\)</span> and <span class="math inline">\(\gamma_0 = 1\)</span> could be chosen. As for <span class="math inline">\(\tau\)</span>, one chooses a weakly informative prior with <span class="math inline">\(a_{\tau} = b_{\tau} = 1\)</span>, as <span class="math inline">\(\textrm{Gamma}(1, 1)\)</span>. Moreover, to choose a prior for <span class="math inline">\(\sigma\)</span>, let <span class="math inline">\(a_{\sigma} = b_{\sigma} = 1\)</span> to have the weakly informative <span class="math inline">\(\textrm{Gamma}(1, 1)\)</span> prior.</p>
</section>
<section id="inference-through-mcmc" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="inference-through-mcmc"><span class="header-section-number">5.2.3</span> Inference through MCMC</h3>
<p>With the specification of the prior, the complete hierarchical model is described as follows:</p>
<ul>
<li><p>Sampling: for <span class="math inline">\(j = 1, \cdots, 8\)</span> and <span class="math inline">\(i = 1, \cdots, n_j\)</span>: <span class="math display">\[\begin{equation}
Y_{ij} \mid \mu_j, \sigma_j \overset{i.i.d.}{\sim} \textrm{Normal}(\mu_j, \sigma_j)
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(\mu_j\)</span>, Stage 1: for <span class="math inline">\(j = 1, \cdots, 8\)</span>: <span class="math display">\[\begin{equation}
\mu_j \mid \mu, \tau \sim \textrm{Normal}(\mu, \tau)
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(\mu_j\)</span>, Stage 2: the hyperpriors: <span class="math display">\[\begin{equation}
\mu \sim \textrm{Normal}(3, 1)
\end{equation}\]</span> <span class="math display">\[\begin{equation}
1/\tau^2  \sim \textrm{Gamma}(1, 1)
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(\sigma\)</span>: <span class="math display">\[\begin{equation}
1/\sigma^2  \sim \textrm{Gamma}(1, 1)
\end{equation}\]</span></p></li>
</ul>
<p>If one uses JAGS for simulation by MCMC, one writes out the model section by following the model structure above closely. Review Section 9.7 for an introduction and a description of several examples of JAGS.</p>
<p><strong>Describe the model by a script</strong></p>
<p>The first step in using the JAGS software is to write the following script defining the hierarchical model. The model is saved in the character string <code>modelString</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>modelString <span class="ot">&lt;-</span><span class="st">"</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="st">## sampling</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="st">for (i in 1:N){</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">   y[i] ~ dnorm(mu_j[MovieIndex[i]], invsigma2)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="st">## priors</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="st">for (j in 1:J){</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="st">   mu_j[j] ~ dnorm(mu, invtau2)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="st">invsigma2 ~ dgamma(a_s, b_s)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="st">sigma &lt;- sqrt(pow(invsigma2, -1))</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="st">## hyperpriors</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="st">mu ~ dnorm(mu0, g0)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="st">invtau2 ~ dgamma(a_t, b_t)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">tau &lt;- sqrt(pow(invtau2, -1))</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="st">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the sampling part of the script, note that the loop goes from <code>1</code> to <code>N</code>, where <code>N</code> is the number of observations with index <code>i</code>. However, because now <code>N</code> observations are grouped according to movies, indicated by <code>j</code>, one needs to create one vector, <code>mu_j</code> of length eight, and use <code>MovieIndex[i]</code> to grab the corresponding <code>mu_j</code> based on the movie index.</p>
<p>In the priors part of the script, the loop goes from <code>1</code> to <code>J</code>, and <code>J</code> = 8 in the current example. Inside the loop, the first line corresponds to the prior distribution for <code>mu_j</code>. Due to a commonly shared <code>sigma</code>, <code>invsigma2</code> follows <code>dgamma(a_g, b_g)</code> outside of the loop. In addition, <code>sigma &lt;- sqrt(pow(invsigma2, -1))</code> is added to help track <code>sigma</code> directly.</p>
<p>Finally in the hyperpriors section of the script, one specifies the Normal hyperprior for <code>mu</code>, a Gamma hyperprior for <code>invtau2</code>. Keep in mind that the arguments in the <code>dnorm</code> in JAGS are the mean and the precision. If one is interested instead in the standard deviation parameter <code>tau</code>, one could return it in the script by using <code>tau &lt;- sqrt(pow(invtau2, -1))</code>, enabling the tracking of its MCMC chain in the posterior inferences.</p>
<p><strong>Define the data and prior parameters</strong></p>
<p>After one has defined the model script, the next step is to provide the data and values for parameters of the prior. In the R script below, a list <code>the_data</code> contains the vector of observations, the vector of movie indices, the number of observations, and the number of movies. It also contains the Normal hyperparameters <code>mu0</code> and <code>g0</code>, and two sets of Gamma hyperparameters (<code>a_t</code> and <code>b_t</code>) for <code>invtau2</code>, and (<code>a_s</code> and <code>b_s</code>) for <code>invsigma2</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ProbBayes)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> animation_ratings<span class="sc">$</span>rating      </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>MovieIndex <span class="ot">&lt;-</span> animation_ratings<span class="sc">$</span>Group_Number      </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">length</span>(y)  </span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>J <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">unique</span>(MovieIndex)) </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>the_data <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="st">"y"</span> <span class="ot">=</span> y, <span class="st">"MovieIndex"</span> <span class="ot">=</span> MovieIndex, </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"N"</span> <span class="ot">=</span> N, <span class="st">"J"</span> <span class="ot">=</span> J,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"mu0"</span> <span class="ot">=</span> <span class="dv">3</span>, <span class="st">"g0"</span> <span class="ot">=</span> <span class="dv">1</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"a_t"</span> <span class="ot">=</span> <span class="dv">1</span>, <span class="st">"b_t"</span> <span class="ot">=</span> <span class="dv">1</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                 <span class="st">"a_s"</span> <span class="ot">=</span> <span class="dv">1</span>, <span class="st">"b_s"</span> <span class="ot">=</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>One uses the <code>run.jags()</code> function in the <code>runjags</code> R package to generate posterior samples by using the MCMC algorithms in JAGS. The script below runs one MCMC chain with 1000 iterations in the adapt period (preparing for MCMC), 5000 iterations of burn-in and an additional set of 5000 iterations to be run and collected for inference. By using <code>monitor = c("mu", "tau", "mu_j", "sigma")</code>, one collects the values of all parameters in the model. In the end, the output variable <code>posterior</code> contains a matrix of simulated draws.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>posterior <span class="ot">&lt;-</span> <span class="fu">run.jags</span>(modelString,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">n.chains =</span> <span class="dv">1</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data =</span> the_data,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">monitor =</span> <span class="fu">c</span>(<span class="st">"mu"</span>, <span class="st">"tau"</span>, <span class="st">"mu_j"</span>, <span class="st">"sigma"</span>),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">adapt =</span> <span class="dv">1000</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">burnin =</span> <span class="dv">5000</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">sample =</span> <span class="dv">5000</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>MCMC diagnostics and summarization</strong></p>
<p>In any implementation of MCMC sampling, diagnostics are crucial to perform to ensure convergence. To perform some MCMC diagnostics in our example, one uses the <code>plot()</code> function, specifying the variable to be checked by the <code>vars</code> argument. For example, the script below returns four diagnostic plots (trace plot, empirical PDF, histogram, and autocorrelation plot) in Figure 10.2 for the hyperparameter <span class="math inline">\(\tau\)</span>. Note that the trace plot only includes 5000 iterations in sample, although its index starts from adapt (1000 adapt + 5000 burn-in). The trace plot and autocorrelation plot suggest good mixing of the chain, therefore indicating convergence of the MCMC chain for <span class="math inline">\(\tau\)</span>.</p>
<pre><code>plot(posterior, vars = "tau")</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/Normal_tau.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Diagnostic plots of simulated draws of <span class="math inline">\(\tau\)</span> using the JAGS software with the <code>runjags</code> package.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In practice MCMC diagnostics should be performed for all parameters to justify the overall MCMC convergence. In our example, the above diagnostics should be implemented for each of the eleven parameters in the model: <span class="math inline">\(\mu, \tau, \mu_1, \mu_2, \cdots, \mu_8\)</span>, and <span class="math inline">\(\sigma\)</span>. Once diagnostics are done, one reports posterior summaries of the parameters using <code>print()</code>. Note that these summaries are based on the 5000 iterations from the sample period, excluding the adapt and burn-in iterations.</p>
<pre><code>print(posterior, digits = 3)                                                                                     
        Lower95 Median Upper95  Mean     SD Mode   MCerr 
mu         3.19   3.78    4.34  3.77  0.286   -- 0.00542     
tau       0.357  0.638    1.08 0.677    0.2   -- 0.00365  
mu_j[1]    2.96   3.47    3.99  3.47  0.262   -- 0.00376  
mu_j[2]    3.38   3.81    4.25  3.82  0.221   -- 0.00313    
mu_j[3]    3.07   3.91    4.75  3.91  0.425   -- 0.00677    
mu_j[4]    3.21   3.74    4.31  3.74  0.285   -- 0.00428 
mu_j[5]    3.09   4.15    5.43  4.18  0.588   --  0.0115   
mu_j[6]     2.7   3.84    4.99  3.85  0.576   -- 0.00915   
mu_j[7]    2.74   3.53    4.27  3.51  0.388   -- 0.00595  
mu_j[8]    3.58   4.12    4.66  4.12  0.276   -- 0.00423  
sigma     0.763   0.92    1.12  0.93 0.0923   -- 0.00142 </code></pre>
<p>One performs various inferential summaries and inferences based on the output. For example, the movies “How to Train Your Dragon” (corresponding to <span class="math inline">\(\mu_1\)</span>) and “Megamind” (corresponding to <span class="math inline">\(\mu_7\)</span>) have the lowest average ratings with short 90% credible intervals, (2.96, 3.99) and (2.74, 4.27) respectively, whereas “Legend of the Guardians: The Owls of Ga’Hoole” (corresponding to <span class="math inline">\(\mu_6\)</span>) also has a low average rating but with a wider 90% credible interval (2.70, 4.99). The differences in the width of the credible intervals stem from the sample sizes: there are eleven ratings for “How to Train Your Dragon”, four ratings for “Megamind”, and only a single rating for “Legend of the Guardians: The Owls of Ga’Hoole”. The smaller the sample size, the larger the variability in the inference, even if one pools information across groups.</p>
<p>Among the movies with high average ratings, “Batman: Under the Red Hood” (corresponding to <span class="math inline">\(\mu_5\)</span>) is worth noting. This movie’s average rating <span class="math inline">\(\mu_5\)</span> has the largest median value among all <span class="math inline">\(\mu_j\)</span>’s, at 4.15, and also a wide 90% credible interval, (3.09, 5.43). “Batman: Under the Red Hood” also received one rating in the sample resulting in a wide credible interval.</p>
<p><strong>Shrinkage</strong></p>
<p>Recall that the two-stage prior in Equations (10.7) to (10.8) specifies a shared prior <span class="math inline">\(\textrm{Normal}(\mu, \tau)\)</span> for all <span class="math inline">\(\mu_j\)</span>’s which facilitates simultaneous estimation of the movie mean ratings (the <span class="math inline">\(\mu_j\)</span>’s), and estimation of the variation among the movie mean ratings through the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span>. The posterior mean of the rating for a particular movie <span class="math inline">\(\mu_j\)</span> shrinks the observed mean rating towards an average rating.<br>
Figure 10.3 displays a “shrinkage plot” which illustrates the movement of the observed sample mean ratings towards an average rating.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/MovieLensPoolingb.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Shrinkage plot of sample means and posterior means of movie ratings for eight movies.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The left side of Figure 10.3 plots the sample movie rating means and lines connect the sample means to the corresponding posterior means (i.e.&nbsp;means of the posterior draws of <span class="math inline">\(\mu_j\)</span>). The shrinkage effect is obvious for the movie “Batman: Under the Red Hood” which corresponds to the dot at the value 5.0 on the left. This movie only received one rating of 5.0 and its mean rating <span class="math inline">\(\mu_5\)</span> shrinks to the value 4.178 on the right, which is still the highest posterior mean among the nine movie posterior means. A large shrinkage is desirable for a movie with a small number of ratings such as “Batman: Under the Red Hood”. For a movie with a small sample size, information about other ratings of similar movies helps to produce a more reasonable estimate at the “true” average movie rating. The amount of shrinkage is more modest for movies with larger sample sizes. Furthermore, by pooling ratings across movies, one is able to estimate the standard deviation <span class="math inline">\(\sigma\)</span> of the ratings. Without this pooling, one would be unable to estimate the standard deviation for a movie with only one rating.</p>
<p><strong>Sources of variability</strong></p>
<p>As discussed in Section 10.1, the prior distribution <span class="math inline">\(\textrm{Normal}(\mu, \tau)\)</span> is shared among the means <span class="math inline">\(\mu_j\)</span>’s of all groups in a hierarchical Normal model, and the hyperparameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> provide information about the population of <span class="math inline">\(\mu_j\)</span>’s. Specifically, the standard deviation <span class="math inline">\(\tau\)</span> measures the variability among the <span class="math inline">\(\mu_j\)</span>’s. When the hierarchical model is estimated through MCMC, summaries from the simulation draws from the posterior of <span class="math inline">\(\tau\)</span> provide information about this source of variation after analyzing the data.</p>
<p>There are actually two sources for the variability among the observed <span class="math inline">\(Y_{ij}\)</span>’s. At the sampling level of the model, the standard deviation <span class="math inline">\(\sigma\)</span> measures variability of the <span class="math inline">\(Y_{ij}\)</span> within the groups. In contrast, the parameter <span class="math inline">\(\tau\)</span> measures the variability in the measurements between the groups. When the hierarchical model is fit through MCMC, summaries from the marginal posterior distributions of <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span> provide information about the two sources of variability.</p>
<p><span class="math display">\[\begin{equation}
Y_{ij} \overset{i.i.d.}{\sim} \textrm{Normal}(\mu_j, \sigma) \,\,\, \text{[within-group variability]}
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\mu_j \mid \mu, \tau \sim \textrm{Normal}(\mu, \tau) \,\,\, \text{[between-group variability]}
\end{equation}\]</span></p>
<p>The Bayesian posterior inference in the hierarchical model is able to compare these two sources of variability, taking into account the prior belief and the information from the data. One initially provides prior beliefs about the values of the standard deviations <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span> through Gamma distributions. In the MovieLens ratings application, weakly informative priors of <span class="math inline">\(\textrm{Gamma}(1, 1)\)</span> are assigned to both <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span>. These prior distributions assume a priori the within-group variability, measured by <span class="math inline">\(\sigma\)</span>, is believed to be the same size as the between-group variability measured by <span class="math inline">\(\tau\)</span>.</p>
<p>What can be said about these two sources of variability after the estimation of the hierarchical model? As seen in the output of <code>print(posterior, digits = 3)</code>, the 90% credible interval for <span class="math inline">\(\sigma\)</span> is (0.763, 1.12) and the 90% credible interval for <span class="math inline">\(\tau\)</span> is (0.357, 1.08). After observing the data, the within-group variability in the measurements is estimated to be larger than the between-group variability.</p>
<p>To compare these two sources of variation one computes the fraction <span class="math inline">\(R = \frac{\tau^2}{\sigma^2 + \tau^2}\)</span> from the posterior samples of <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\tau\)</span>. The interpretation of <span class="math inline">\(R\)</span> is that it represents the fraction of the total variability in the movie ratings due to the differences between groups. If the value of <span class="math inline">\(R\)</span> is close to 1, most of the total variability is attributed to the between-group variability. On the other side, if <span class="math inline">\(R\)</span> is close to 0, most of the variation is within groups and there is little significant differences between groups.</p>
<p>Sample code shown below computes simulated values of <span class="math inline">\(R\)</span> from the MCMC output. A density plot of <span class="math inline">\(R\)</span> is shown in Figure 10.4.</p>
<pre><code>tau_draws &lt;- as.mcmc(posterior, vars = "tau")
sigma_draws &lt;- as.mcmc(posterior, vars = "sigma")
R &lt;- tau_draws ^ 2 / (tau_draws ^ 2 + sigma_draws ^ 2)</code></pre>
<p>A 95% credible interval for <span class="math inline">\(R\)</span> is (0.149, 0.630). Since much of the posterior probability of <span class="math inline">\(R\)</span> is located below the value 0.5, this confirms that the variation between the mean movie rating titles is smaller than the variation of the ratings within the movie titles in this example.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/Normal_R.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Density plot of the ratio <span class="math inline">\(R\)</span> from the posterior samples of tau and sigma.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="hierarchical-beta-binomial-modeling" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="hierarchical-beta-binomial-modeling"><span class="header-section-number">5.3</span> Hierarchical Beta-Binomial Modeling</h2>
<section id="example-deaths-after-heart-attack" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="example-deaths-after-heart-attack"><span class="header-section-number">5.3.1</span> Example: Deaths after heart attack</h3>
<p>The New York State (NYS) Department of Health collects and releases data on mortality after Acute Myocardial Infarction (AMI), commonly known as a heart attack. Their 2015 report was the initial public data release by the NYS Department of Health on risk-adjusted mortality outcomes for AMI patients at hospitals across the state. We focus on 13 hospitals in Manhattan, New York City, with the goal of learning about the percentages of resulted deaths from heart attack for hospitals in this sample. Table 10.2 records for each hospital the number of heart attack cases, the corresponding number of resulted deaths, and their computed percentage of resulted deaths.</p>
<p>Table 10.2. The number of heart attack cases, the number of resulted deaths, and the percentage of resulted deaths of 13 hospitals in New York City - Manhattan in 2015. NYP stands for New York Presbyterian.</p>
<table class="table">
<colgroup>
<col style="width: 70%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 11%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Hospital</th>
<th style="text-align: right;">Cases</th>
<th style="text-align: right;">Deaths</th>
<th style="text-align: right;">Death %</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bellevue Hospital Center</td>
<td style="text-align: right;">129</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">3.101</td>
</tr>
<tr class="even">
<td style="text-align: left;">Harlem Hospital Center</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">2.857</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Lenox Hill Hospital</td>
<td style="text-align: right;">228</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">7.894</td>
</tr>
<tr class="even">
<td style="text-align: left;">Metropolitan Hospital Center</td>
<td style="text-align: right;">84</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">8.333</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mount Sinai Beth Israel</td>
<td style="text-align: right;">291</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">8.247</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mount Sinai Hospital</td>
<td style="text-align: right;">270</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">5.926</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mount Sinai Roosevelt</td>
<td style="text-align: right;">46</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">13.043</td>
</tr>
<tr class="even">
<td style="text-align: left;">Mount Sinai St.&nbsp;Luke’s</td>
<td style="text-align: right;">293</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">6.485</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NYU Hospitals Center</td>
<td style="text-align: right;">241</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">6.224</td>
</tr>
<tr class="even">
<td style="text-align: left;">NYP Hospital - Allen Hospital</td>
<td style="text-align: right;">105</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">12.381</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NYP Hospital - Columbia Presbyterian Center</td>
<td style="text-align: right;">353</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">7.082</td>
</tr>
<tr class="even">
<td style="text-align: left;">NYP Hospital - New York Weill Cornell Center</td>
<td style="text-align: right;">250</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">4.400</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NYP/Lower Manhattan Hospital</td>
<td style="text-align: right;">41</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">9.756</td>
</tr>
</tbody>
</table>
</section>
<section id="a-hierarchical-beta-binomial-model" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="a-hierarchical-beta-binomial-model"><span class="header-section-number">5.3.2</span> A hierarchical Beta-Binomial model</h3>
<p>Treating “cases” as trials and “deaths” as successes, the Binomial sampling model is a natural choice for this data, and the objective is to learn about the death probability <span class="math inline">\(p\)</span> of the hospitals. If one looks at the actual death percentages in Table <span class="math inline">\(\ref{table:DeathData}\)</span>, some hospitals have much higher death rates than other hospitals. For example, the highest death rate belongs to Mount Sinai Roosevelt, at 13.043% which is more than four times the rate of Harlem Hospital Center at 2.857%. If one assumes a common probability <span class="math inline">\(p\)</span> for all thirteen hospitals, this model does not allow for possible differences between the death rates among these hospitals.</p>
<p>On the other hand, if one creates thirteen separate Binomial sampling models, one for each hospital, and conducts separate inferences, one loses the ability to use potential information about the death rate from hospital <span class="math inline">\(j\)</span> when making inference about that of a different hospital <span class="math inline">\(i\)</span>. Since these are all hospitals in Manhattan, New York City, they may share attributes in common related to death rates from heart attack. The separate modeling approach does not allow for the sharing of information across hospitals.</p>
<p>A hierarchical model provides a compromise between the combined and separate modeling approaches. In Section 10.2, a hierarchical Normal density was used to model mean rating scores from different movies. In this setting, one builds a hierarchical model by assuming the hospital death rate parameters <strong>a priori</strong> come from a common distribution. Specifically, one builds a hierarchical model based on a common Beta distribution that generalizes the Beta-Binomial conjugate model described in Chapter 7. This modeling setup provides posterior estimates that partially pool information among hospitals</p>
<p>Let <span class="math inline">\(Y_i\)</span> denote the number of resulted deaths from heart attack, <span class="math inline">\(n_i\)</span> the number of heart attack cases, and <span class="math inline">\(p_i\)</span> the death rate for hospital <span class="math inline">\(i\)</span>. The sampling density for <span class="math inline">\(Y_i\)</span> for hospital <span class="math inline">\(i\)</span> is a Binomial distribution with <span class="math inline">\(n_i\)</span> and <span class="math inline">\(p_i\)</span>, as in Equation (10.19). Suppose that the proportions {<span class="math inline">\(p_i\)</span>} independently follow the same conjugate Beta prior distribution, as in Equation (10.20). So the sampling and first stage of the prior of our model is written as follows:</p>
<ul>
<li><p>Sampling, for <span class="math inline">\(i, \cdots, 13\)</span>: <span class="math display">\[\begin{equation}
Y_i \sim \textrm{Binomial}(n_i, p_i)
\label{eq:BetaBinomialLik_v1}
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(p_i\)</span>, <span class="math inline">\(i = 1, \cdots, 13\)</span>: <span class="math display">\[\begin{equation}
p_i \sim \textrm{Beta}(a, b)
\label{eq:BetaBinomialPrior_v1}
\end{equation}\]</span></p></li>
</ul>
<p>Note that the hyperparameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are shared among all hospitals. If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are known values, then the posterior inference for <span class="math inline">\(p_i\)</span> of hospital <span class="math inline">\(i\)</span> is simply another Beta distribution by conjugacy (review material in Chapter 7 if needed): <span class="math display">\[\begin{equation}
p_i \mid y_i \sim \textrm{Beta}(a + y_i, b + n_i - y_i).
\end{equation}\]</span></p>
<p>In the general situation where the hyperparameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are unknown, a second stage of the prior <span class="math inline">\(\pi(a, b)\)</span> needs to specified for these hyperparameters. With this specification, one arrives at the hierarchical model below.</p>
<ul>
<li><p>Sampling, for <span class="math inline">\(i, \cdots, 13\)</span>: <span class="math display">\[\begin{equation}
Y_i \sim \textrm{Binomial}(n_i, p_i)
\label{eq:BetaBinomialLik_v2}
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(p_i\)</span>, Stage1: for <span class="math inline">\(i = 1, \cdots, 13\)</span>: <span class="math display">\[\begin{equation}
p_i \sim \textrm{Beta}(a, b)
\label{eq:BetaBinomialPrior_v2}
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(p_i\)</span>, Stage 2: the hyperprior: <span class="math display">\[\begin{eqnarray}
a, b \sim \pi(a, b)
\label{eq:BetaBinomialHyperprior_v1}
\end{eqnarray}\]</span></p></li>
</ul>
<p>Wee use <span class="math inline">\(\pi(a, b)\)</span> to denote an arbitrary distribution for the joint hyperprior distribution for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. When we start analyzing the New York State heart attack death rate dataset, the specification of this hyperprior distribution <span class="math inline">\(\pi(a, b)\)</span> will be described.</p>
<p><strong>Graphical representations of the hierarchical model</strong></p>
<p>Below is a sketch of a graphical representation of the hierarchical Beta-Binomial model.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figures/chapter10/treediagram2.png" class="img-fluid" width="500"></p>
</div>
</div>
<p>Focusing on the graph on the right, one sees that the upper section of the graph represents the sampling density, with the arrow directing from <span class="math inline">\(p_i\)</span> to <span class="math inline">\(Y_i\)</span>. Here the start of the arrow is the parameter and the end of the arrow is the random variable. The lower section of the graph represents the prior, with arrows directing from <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> to <span class="math inline">\(p_i\)</span>. In this case, the start of the arrow is the hyperparameter and the end of the arrow is the parameter.<br>
On the left side of the display, the sampling density, prior and hyperprior distributional expressions are written next to the graphical representation.</p>
<p>In the situation where the Beta parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are known constants, the graphical representation changes to the Beta-Binomial conjugate model displayed below.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figures/chapter10/treediagram3.png" class="img-fluid" width="500"></p>
</div>
</div>
<p>To illustrate another graphical representation, we display below the one for the separate models approach in the hospitals death rate application where a fully specified Beta prior is specified for each death rate. The separate models are represented by thirteen graphs, one for each hospital. This graphical structure shows clearly the separation of the subsamples and the resulting separation of the corresponding Bayesian posterior distributions.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="figures/chapter10/treediagram4.png" class="img-fluid" width="500"></p>
</div>
</div>
<p>In comparing graphical representations for hierarchical models, the interested reader might notice that the structure for the hierarchical Beta-Binomial model looks different from the ones in Section 10.2 for the hierarchical Normal models. In this chapter, one is dealing with one-parameter models (recall that Beta-Binomial is an example of one-parameter models; other examples include Gamma-Poisson), whereas the Normal models in Section 10.2 involve two parameters. Typically, when working with one-parameter models, one starts from the top with the sampling density, then next writes down the priors and continues with the hyperpriors. When there are multiple parameters, one needs to be careful in describing the graphical structure. In fact, for a large number of parameters, a good graphical representation might not be feasible. In that case, one writes a representation that focuses on the key parts of the model.</p>
<p>Also note that there is no unique way of sketching a graphical representation, as long as the representation is clear and shows the relationship among the random variables, parameters and hyperparameters with the arrows in the correct directions.</p>
</section>
<section id="inference-through-mcmc-1" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="inference-through-mcmc-1"><span class="header-section-number">5.3.3</span> Inference through MCMC</h3>
<p>In this section the application of JAGS script for simulation by MCMC is illustrated for the hierarchical Beta-Binomial models for the New York State heart attach death rate dataset. Before this is done, we discuss the specification of the hyperprior density <span class="math inline">\(\pi(a, b)\)</span> for the hyperparameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for the common Beta prior distribution for the proportions <span class="math inline">\(p_i\)</span>’s.</p>
<p><strong>Second-stage prior</strong></p>
<p>In Chapter 7, the task was to specify the values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for a single Beta curve <span class="math inline">\(\textrm{Beta}(a, b)\)</span> and the Beta shape parameter values were selected by trial-and-error using the <code>beta.select()</code> function in the <code>ProbBayes</code> package. In this hierarchical model setting, the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are random and the goal is learn about these parameters from its posterior distribution.</p>
<p>In this prior construction, it is helpful to review some facts on Beta curves from Chapter 7. For a <span class="math inline">\(\textrm{Beta}(a, b)\)</span> prior distribution for a proportion <span class="math inline">\(p\)</span>, one considers the parameter <span class="math inline">\(a\)</span> as the prior count of “successes”, the parameter <span class="math inline">\(b\)</span> as the prior count of “failures”, and the sum <span class="math inline">\(a + b\)</span> represents the prior sample size. Also the expectation of <span class="math inline">\(\textrm{Beta}(a, b)\)</span> is <span class="math inline">\(\frac{a}{a + b}\)</span>. From these facts, a more natural parameterization of the hyperprior distribution <span class="math inline">\(\pi(a, b)\)</span> is <span class="math inline">\(\pi(\mu, \eta)\)</span>, where <span class="math inline">\(\mu = \frac{a}{a+b}\)</span> is the hyperprior mean and <span class="math inline">\(\eta = a + b\)</span> is the hyperprior sample size. One rewrites the hyperprior distribution in terms of the new parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\eta\)</span> as follows: <span class="math display">\[\begin{equation}
\mu, \eta \sim \pi(\mu, \eta),
\label{eq:BetaBinomialHyperprior_v2}
\end{equation}\]</span> where <span class="math inline">\(a = \mu\eta\)</span> and <span class="math inline">\(b = (1-\mu)\eta\)</span>. These expressions are useful in writing the JAGS script for the hierarchical Beta-Binomial Bayesian model.</p>
<p>A hyperprior is constructed from the <span class="math inline">\((\mu, \eta)\)</span> representation. Assume <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\eta\)</span> are independent which means that one’s beliefs about the prior mean are independent of the beliefs about the prior sample size. The hyperprior expectation <span class="math inline">\(\mu\)</span> is the mean measure for <span class="math inline">\(p_i\)</span>, the average death rate across 13 hospitals. If one has little prior knowledge about the expectation <span class="math inline">\(\mu\)</span>, one assigns this parameter a Uniform prior which is equivalent to a <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> prior.</p>
<p>To motivate the prior choice for the hyperparameter sample size <span class="math inline">\(\eta\)</span>, consider the case where the hyperparameter values are known. If <span class="math inline">\(y^*\)</span> and <span class="math inline">\(n^*\)</span> are respectively the number of deaths and number of cases for one hospital, then the posterior mean of death rate parameter <span class="math inline">\(p^*\)</span> is given by <span class="math display">\[\begin{equation}
E(p^* \mid y^*) = \frac{y^* + \mu \eta }{n^* + \eta}.
\end{equation}\]</span> With a little algebra, the posterior mean is rewritten as <span class="math display">\[\begin{equation}
E(p^* \mid y^*) = (1 - \lambda) \frac{y^*}{n^*} + \lambda \mu,
\end{equation}\]</span> where <span class="math inline">\(\lambda\)</span> is the shrinkage fraction <span class="math display">\[\begin{equation}
\lambda = \frac{\eta}{n^* + \eta}.
\end{equation}\]</span> The parameter <span class="math inline">\(\lambda\)</span> falls in the interval (0, 1) and represents the degree of shrinkage of the posterior mean away from the sample proportion <span class="math inline">\(y^* / n^*\)</span> towards the prior mean <span class="math inline">\(\mu\)</span>.</p>
<p>Suppose one believes a priori that, for a representative sample size <span class="math inline">\(n^*\)</span>, the shrinkage <span class="math inline">\(\lambda\)</span> is Uniformly distributed on (0, 1). By performing a transformation, this implies that the prior density for the prior sample size <span class="math inline">\(\eta\)</span> has the form <span class="math display">\[\begin{equation}
\pi(\eta) = \frac{n^*}{(n^* + \eta)^2}, \, \, \eta &gt; 0.
\end{equation}\]</span> Equivalently, the logarithm of <span class="math inline">\(\eta\)</span>, <span class="math inline">\(\theta = \log \eta\)</span>, has a Logistic distribution with location <span class="math inline">\(\log n^*\)</span> and scale 1. We represent this distribution as <span class="math inline">\(\textrm{Logistic}(\log n^*, 1)\)</span>, with pdf: <span class="math display">\[\begin{equation}
\pi(\theta) = \frac{e^{-(\theta - \log n^*)}}
{(1 + e^{-(\theta - \log n^*)})^2}.
\end{equation}\]</span></p>
<p>With this specification of the hyperparameter distribution, one writes down the complete hierarchical model as follows:</p>
<ul>
<li><p>Sampling, for <span class="math inline">\(i, \cdots, 13\)</span>: <span class="math display">\[\begin{equation}
Y_i \sim \textrm{Binomial}(n_i, p_i)
\label{eq:BetaBinomialLik_v3}
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(p_i\)</span>, Stage 1: for <span class="math inline">\(i = 1, \cdots, 13\)</span>: <span class="math display">\[\begin{equation}
p_i \sim \textrm{Beta}(a, b)
\label{eq:BetaBinomialPrior_v3}
\end{equation}\]</span></p></li>
<li><p>Prior for <span class="math inline">\(p_i\)</span>, Stage 2: <span class="math display">\[\begin{equation}
\mu \sim \textrm{Beta}(1, 1),
\end{equation}\]</span> <span class="math display">\[\begin{equation}
\log \eta \sim \textrm{Logistic}(\log n^*, 1)
\end{equation}\]</span> where <span class="math inline">\(a = \mu\eta\)</span> and <span class="math inline">\(b = (1-\mu)\eta\)</span>.</p></li>
</ul>
<p><strong>Writing the JAGS script</strong></p>
<p>Following this model structure above, one writes out the model section of the JAGS script for the hierarchical Beta-Binomial model. The model script is saved in <code>modelString</code>.</p>
<pre><code>modelString &lt;-"
model {
## likelihood
for (i in 1:N){
   y[i] ~ dbin(p[i], n[i])
}
## priors
for (i in 1:N){
   p[i] ~ dbeta(a, b)
}
## hyperpriors
a &lt;- mu*eta
b &lt;- (1-mu)*eta
mu ~ dbeta(mua, mub)
eta &lt;- exp(logeta)
logeta ~ dlogis(logn, 1)
}
"</code></pre>
<p>In the sampling part of the script, the loop goes from <code>1</code> to <code>N</code>, where <code>N</code> is the total number of observations, with index <code>i</code>. Another loop going from <code>1</code> to <code>N</code> is needed for the priors as each <code>p[i]</code> follows the same <code>dbeta(a, b)</code> distribution. The hyperpriors section uses the new parameterization of the <span class="math inline">\(Beta(a, b)\)</span> distribution in terms of <code>mu</code> and <code>eta</code>. Here one expresses the hyperparameters <code>a</code> and <code>b</code> in terms of the new hyperparameters <code>mu</code> and <code>eta</code>, and then assigns to the parameters <code>mu</code> and <code>logeta</code> the independent distributions <code>dbeta(mua, mub)</code> and <code>dlogist(logn, 1)</code>, respectively. One also needs to transform <code>logeta</code> to <code>eta</code>. The values of <code>mua</code>, <code>mub</code>, and <code>logn</code> are assigned together with the data in the setup of JAGS, following Equation (10.33) and Equation (10.34).</p>
<p><strong>Define the data and prior parameters</strong></p>
<p>Following the usual implementation of JAGS, the next step is to define the data and provide values for the parameters of the prior. In the script below, a list <code>the_data</code> contains the vector of death counts in <code>y</code>, the vector of hearth attack cases in <code>n</code>, the number of observations <code>N</code>, the values of <code>mua</code>, <code>mub</code>, and <code>logn</code>. Note that we are setting <span class="math inline">\(\log n^* = \log(100)\)</span> which indicates that a priori we believe the shrinkage <span class="math inline">\(\lambda = \eta / (\eta + 100)\)</span> is Uniformly distributed on (0, 1).</p>
<pre><code>y &lt;- deathdata$Deaths     
n &lt;- deathdata$Cases      
N &lt;- length(y) 
the_data &lt;- list("y" = y, "n" = n, "N" = N, 
                 "mua" = 1, "mub" = 1, 
                 "logn" = log(100))</code></pre>
<p><strong>Generate samples from the posterior distribution</strong></p>
<p>The <code>run.jags()</code> function is used to generate samples by MCMC in JAGS following the sample script below. It runs one MCMC chain with 1000 iterations in the adapt period, 5000 iterations of burn-in and an additional set of 5000 iterations to be run and collected for inference. One keeps tracks of all parameters in the model by using the argument <code>monitor = c("p", "mu", "logeta")</code>. The output of the MCMC runs is the variable <code>posterior</code> containing a matrix of simulated draws.</p>
<pre><code>posterior &lt;- run.jags(modelString,
                      n.chains = 1,
                      data = the_data,
                      monitor = c("p", "mu", "logeta"),
                      adapt = 1000,
                      burnin = 5000,
                      sample = 5000)</code></pre>
<p><strong>MCMC diagnostics and summarization</strong></p>
<p>As usual, it is important to perform MCMC diagnostics to ensure convergence of the simulated sample. The <code>plot()</code> function returns diagnostics plots of a designated parameter. For brevity, the diagnostics for <span class="math inline">\(\log \eta\)</span> are performed and results shown in Figure 10.5. Readers should implement MCMC diagnostics for all parameters in the model.</p>
<pre><code>plot(posterior, vars = "logeta")</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/BetaBinomial_logeta.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Diagnostic plots of simulated draws of log eta using the JAGS software with the <code>run.jags</code> package.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>After the diagnostics are performed, one reports posterior summaries of the parameters using <code>print()</code>. Note that these summaries are based on the 5000 iterations from the sampling period (excluding the adapt and burn-in periods).</p>
<pre><code>print(posterior, digits = 3)
       Lower95 Median Upper95   Mean      SD Mode    MCerr 
p[1]    0.0314 0.0602  0.0847 0.0593  0.0138   -- 0.000619   
p[2]    0.0312  0.066   0.095 0.0654  0.0156   -- 0.000496   
p[3]    0.0515 0.0731     0.1 0.0741  0.0122   -- 0.000398    
p[4]     0.044 0.0726   0.105  0.074  0.0155   -- 0.000486     
p[5]    0.0553 0.0756     0.1 0.0765  0.0116   -- 0.000348       
p[6]    0.0435 0.0655  0.0871 0.0655  0.0111   --  0.00042     
p[7]    0.0466 0.0765   0.119 0.0797  0.0191   -- 0.000717     
p[8]    0.0473 0.0683  0.0889 0.0683  0.0104   -- 0.000277    
p[9]    0.0442 0.0669  0.0879 0.0671  0.0111   -- 0.000301     
p[10]   0.0544 0.0811   0.122 0.0845  0.0178   -- 0.000732     
p[11]   0.0521 0.0704  0.0934 0.0711  0.0103   -- 0.000279    
p[12]   0.0369   0.06  0.0818 0.0596  0.0116   -- 0.000504     
p[13]   0.0444 0.0729   0.113 0.0752  0.0176   -- 0.000593     
mu      0.0576 0.0705  0.0881 0.0714 0.00788   -- 0.000375     
logeta    3.63   5.84    8.38   6.01    1.26   --    0.107     </code></pre>
<p>From the posterior output, one evaluates the effect of information pooling in the hierarchical model. See Figure 10.6 displays a shrinkage plot showing how the sample proportions are shrunk towards the overall death rate. Two of the lines in the figure are labelled corresponding to the death rates for the hospitals Mount Sinai Roosevelt and NYP - Allen Hospital. Mount Sinai Roosevelt’s death rate of <span class="math inline">\(6/46 = 0.13043\)</span> exceeds the rate of NYP - Allen of <span class="math inline">\(13 / 105 = 0.12381\)</span>, but the figure shows the posterior death rate of NYP - Allen exceeds the posterior death rate of Mount Sinai Roosevelt. Due to the relatively small sample size, one has less confidence in the 0.13043 death rate of Mount Sinai and this rate is shrunk significantly towards the overall death rate in the hierarchical posterior analysis.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/BetaBinomialPooling.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Shrinkage plot of sample proportions and posterior means of proportions of resulted heart attack deaths of 13 hospitals. The death rates of two particular hospitals are labeled. Due to the varying sample sizes, Mt Sinai Roosevelt has a higher observed death rate than NYP - Allen, but NYP - Allen has a higher posterior proportion than Mt Sinai Roosevelt.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To compare the posterior densities of the different <span class="math inline">\(p_i\)</span>, one displays the density estimates in a single graph as in Figure 10.7. Because of the relatively large number of parameters, such plots are difficult to read. Combining the graph and the output above, one sees that <span class="math inline">\(p_7\)</span> and and <span class="math inline">\(p_{10}\)</span> have the largest median values with large standard deviations. One makes inferential statements such as Mount Sinai Roosevelt’s (corresponding to <span class="math inline">\(p_7\)</span>) death rate of heart attack cases has a posterior 90% credible interval of (0.0466, 0.119), the highest among the 13 hospitals in the dataset.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/BetaBinomial_p.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Density plots of simulated draws of proportions using the JAGS software with the <code>run.jags</code> package.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><strong>Comparison of hospitals</strong></p>
<p>One uses this MCMC output to compare the death rates of two hospitals directly, for example, NYP Hospital - Columbia Presbyterian Center and NYP Hospital - New York Weill Cornell Center corresponding respectively to <span class="math inline">\(p_{11}\)</span> and <span class="math inline">\(p_{12}\)</span>. One collects the vector of simulated values of the difference of the death rates (<span class="math inline">\(\delta = p_{11} - p_{12}\)</span>) by subtracting the sets of simulated proportion draws. From the simulated values of the difference in proportions <code>diff</code>, one estimates the probability that <span class="math inline">\(p_{11} &gt; p_{12}\)</span> is positive.</p>
<pre><code>p11draws &lt;- as.mcmc(posterior, vars = "p[11]")
p12draws &lt;- as.mcmc(posterior, vars = "p[12]")
diff = p11draws - p12draws
sum(diff &gt; 0)/5000
[1] 0.7872</code></pre>
<p>A 78.72% posterior probability of <span class="math inline">\(p_{11} &gt; p_{12}\)</span> indicates strong posterior evidence that the death rate of NYP Hospital - Columbia Presbyterian Center is higher than that of NYP Hospital - New York Weill Cornell Center.</p>
<p>Generally, when one presents a table such as Table 10.2, one is interested in ranking the 13 hospitals from best (smallest death rate) to worst (largest death rate). A particular hospital, say Bellevue Hospital Center, is interested in its rank among the 13 hospitals. The probability Bellevue has rank 1 is the posterior probability <span class="math display">\[\begin{equation}
P(p_1 &lt; p_2, ..., p_1 &lt; p_{13} | y),
\end{equation}\]</span> and this probability is approximated by collecting the posterior draws where the simulated value of <span class="math inline">\(p_1\)</span> is the smallest among the 13 simulated proportions. Likewise, one computes from the MCMC output the probability that Bellevue has rank 2 through 13. These rank probabilities are displayed in Figure 10.8 for two hospitals. The probability that Bellevue is the best hospital with respect to death rate is 0.25 and by summing several probabilities, the probability that Bellevue is ranked among the top three hospitals is 0.54. In contrast, from Figure 10.8, the rank of Harlem Hospital is less certain since the probability distribution is relatively flat across the 13 possible rank values. This is not surprising since this particular hospital had only 35 cases, compared to 129 cases at Bellevue.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/rankplot2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Posterior probabilities of rank for two hospitals.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>From a patient’s perspective, she would be interested in learning the identity of the hospital that is ranked best among the 13. For each simulation draw of <span class="math inline">\(p_1, ..., p_{13}\)</span>, one identifies the hospital with the smallest simulated value. By collecting this information over the 5000 draws, one computes the posterior probability that each hospital is ranked first. These probability probabilities are displayed in Figure 10.9. The identity of the best hospital is not certain, but the top three hospitals are Bellevue, NYP, NY Weill Corner, and Harlem with respective probabilities 0.250, 0.220, and 0.137 of being the best.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter10/rankplot3.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Posterior probabilities of the hospital that was ranked first.</figcaption><p></p>
</figure>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./mcmc.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Simulation by Markov Chain Monte Carlo</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./regression.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Simple Linear Regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>