<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.309">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian Modeling - 1&nbsp; Learning About a Binomial Probability</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<link href="./mean.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Learning About a Binomial Probability</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian Modeling</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./proportion.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Learning About a Binomial Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mean.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-thinking-about-a-proportion-subjectively" id="toc-introduction-thinking-about-a-proportion-subjectively" class="nav-link active" data-scroll-target="#introduction-thinking-about-a-proportion-subjectively"> <span class="header-section-number">1.1</span> Introduction: Thinking About a Proportion Subjectively</a></li>
  <li><a href="#bayesian-inference-with-discrete-priors" id="toc-bayesian-inference-with-discrete-priors" class="nav-link" data-scroll-target="#bayesian-inference-with-discrete-priors"> <span class="header-section-number">1.2</span> Bayesian Inference with Discrete Priors</a>
  <ul class="collapse">
  <li><a href="#example-students-dining-preference" id="toc-example-students-dining-preference" class="nav-link" data-scroll-target="#example-students-dining-preference"> <span class="header-section-number">1.2.1</span> Example: students’ dining preference</a></li>
  <li><a href="#discrete-prior-distributions-for-proportion-p" id="toc-discrete-prior-distributions-for-proportion-p" class="nav-link" data-scroll-target="#discrete-prior-distributions-for-proportion-p"> <span class="header-section-number">1.2.2</span> Discrete prior distributions for proportion <span class="math inline">\(p\)</span></a></li>
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood"> <span class="header-section-number">1.2.3</span> Likelihood</a></li>
  <li><a href="#posterior-distribution-for-proportion-p" id="toc-posterior-distribution-for-proportion-p" class="nav-link" data-scroll-target="#posterior-distribution-for-proportion-p"> <span class="header-section-number">1.2.4</span> Posterior distribution for proportion <span class="math inline">\(p\)</span></a></li>
  <li><a href="#inference-students-dining-preference" id="toc-inference-students-dining-preference" class="nav-link" data-scroll-target="#inference-students-dining-preference"> <span class="header-section-number">1.2.5</span> Inference: students’ dining preference</a></li>
  <li><a href="#discussion-using-a-discrete-prior" id="toc-discussion-using-a-discrete-prior" class="nav-link" data-scroll-target="#discussion-using-a-discrete-prior"> <span class="header-section-number">1.2.6</span> Discussion: using a discrete prior</a></li>
  </ul></li>
  <li><a href="#continuous-priors" id="toc-continuous-priors" class="nav-link" data-scroll-target="#continuous-priors"> <span class="header-section-number">1.3</span> Continuous Priors</a>
  <ul class="collapse">
  <li><a href="#the-beta-distribution-and-probabilities" id="toc-the-beta-distribution-and-probabilities" class="nav-link" data-scroll-target="#the-beta-distribution-and-probabilities"> <span class="header-section-number">1.3.1</span> The Beta distribution and probabilities</a></li>
  <li><a href="#choosing-a-beta-density-to-represent-prior-opinion" id="toc-choosing-a-beta-density-to-represent-prior-opinion" class="nav-link" data-scroll-target="#choosing-a-beta-density-to-represent-prior-opinion"> <span class="header-section-number">1.3.2</span> Choosing a Beta density to represent prior opinion</a></li>
  </ul></li>
  <li><a href="#updating-the-beta-prior" id="toc-updating-the-beta-prior" class="nav-link" data-scroll-target="#updating-the-beta-prior"> <span class="header-section-number">1.4</span> Updating the Beta Prior</a>
  <ul class="collapse">
  <li><a href="#bayes-rule-calculation" id="toc-bayes-rule-calculation" class="nav-link" data-scroll-target="#bayes-rule-calculation"> <span class="header-section-number">1.4.1</span> Bayes’ rule calculation</a></li>
  <li><a href="#from-beta-prior-to-beta-posterior" id="toc-from-beta-prior-to-beta-posterior" class="nav-link" data-scroll-target="#from-beta-prior-to-beta-posterior"> <span class="header-section-number">1.4.2</span> From Beta prior to Beta posterior</a></li>
  </ul></li>
  <li><a href="#bayesian-inferences-with-continuous-priors" id="toc-bayesian-inferences-with-continuous-priors" class="nav-link" data-scroll-target="#bayesian-inferences-with-continuous-priors"> <span class="header-section-number">1.5</span> Bayesian Inferences with Continuous Priors</a>
  <ul class="collapse">
  <li><a href="#bayesian-hypothesis-testing" id="toc-bayesian-hypothesis-testing" class="nav-link" data-scroll-target="#bayesian-hypothesis-testing"> <span class="header-section-number">1.5.1</span> Bayesian hypothesis testing</a></li>
  <li><a href="#bayesian-credible-intervals" id="toc-bayesian-credible-intervals" class="nav-link" data-scroll-target="#bayesian-credible-intervals"> <span class="header-section-number">1.5.2</span> Bayesian credible intervals</a></li>
  <li><a href="#bayesian-prediction" id="toc-bayesian-prediction" class="nav-link" data-scroll-target="#bayesian-prediction"> <span class="header-section-number">1.5.3</span> Bayesian prediction</a></li>
  </ul></li>
  <li><a href="#predictive-checking" id="toc-predictive-checking" class="nav-link" data-scroll-target="#predictive-checking"> <span class="header-section-number">1.6</span> Predictive Checking</a>
  <ul class="collapse">
  <li><a href="#comparing-bayesian-models" id="toc-comparing-bayesian-models" class="nav-link" data-scroll-target="#comparing-bayesian-models"> <span class="header-section-number">1.6.1</span> Comparing Bayesian models</a></li>
  <li><a href="#posterior-predictive-checking" id="toc-posterior-predictive-checking" class="nav-link" data-scroll-target="#posterior-predictive-checking"> <span class="header-section-number">1.6.2</span> Posterior predictive checking</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="proportion" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Learning About a Binomial Probability</span></span></h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell">

</div>
<section id="introduction-thinking-about-a-proportion-subjectively" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="introduction-thinking-about-a-proportion-subjectively"><span class="header-section-number">1.1</span> Introduction: Thinking About a Proportion Subjectively</h2>
<p>In previous chapters, we have seen many examples involving drawing color balls from a box. In those examples, we are given the numbers of balls of various colors in the box, and we consider questions related to calculating probabilities. For example, there are 40 white and 20 red balls in a box. If you draw two balls at random, what is the probability that both balls are white?</p>
<p>Here we consider a new scenario where we do not know the proportions of color balls in the box. That is, in the previous example, we only know that there are two kinds of color balls in the box, but we don’t know 40 out of 60 of the balls are white (proportion of white = <span class="math inline">\(2/3\)</span>) and 20 out of the 60 of the balls are red (proportion of red = <span class="math inline">\(1/3\)</span>). How can we learn about the proportions of white and red balls? Since counting 60 balls can be tedious, how can we infer those proportions by drawing a sample of balls out of the box and observe the colors of balls in the sample? This becomes an inference question, because we are trying to infer the proportion <span class="math inline">\(p\)</span> of the population, based on a sample from the population.</p>
<p>Let’s continue discussing the scenario where we are told that there are 60 balls in total in a box, and the balls are either white or red. We do not know the count of balls of each of the two colors. We are given the opportunity to take a random sample of 10 balls out of these 60 balls. We are interested in the quantity <span class="math inline">\(p\)</span>, the proportion of red balls in the 60 balls. How can we infer <span class="math inline">\(p\)</span>, the proportion of red balls in the population (i.e.&nbsp;the 60 balls), based on the numbers of red and white balls we observe in the sample (i.e.&nbsp;the 10 balls)?</p>
<p>Proportions are like probabilities. Recall in Chapter 1 three views of a probability were discussed. We briefly review them here, and state the specific requirements to obtain each view.</p>
<ol type="1">
<li><p>The classical view: one needs to write down the sample space where each outcome is equally likely.</p></li>
<li><p>The frequency view: one needs to repeat the random experiments many times under identical conditions.</p></li>
<li><p>The subjective view: one needs to express one’s opinion about the likelihood of a one-time event.</p></li>
</ol>
<p>The classical view does not seem to work here, because we only know there are two kinds of color balls and the total number of balls is 60. Even if we take a sample of 10 balls, we are only going to observe the proportion of red balls in the sample. There does not seem to be a way for us to write down the sample space where each outcome is equally likely.</p>
<p>The frequency view would work here. One could treat the process of obtaining a sample (i.e.&nbsp;taking a random sample of 10 balls from the box) as an experiment, and obtain a sample proportion <span class="math inline">\(\hat{p}\)</span> from the experiment. One then could repeat the experiment many times under the same condition, get many sample proportions <span class="math inline">\(\hat{p}\)</span>, and summarize all the <span class="math inline">\(\hat{p}\)</span>. When one repeats the experiment enough times (a large number), one gets a good sense about the proportion <span class="math inline">\(p\)</span> of red balls in the population of 60 balls in the box. This process is doable, but tedious, time-consuming, and prone to errors.</p>
<p>The subjective view perceives the unknown proportion <span class="math inline">\(p\)</span> subjectively. It does require one to express his or her opinion about the value of <span class="math inline">\(p\)</span>, and he or she could be skeptical and unconfident about the opinion. In Chapter 1, a calibration experiment was introduced to help one sharpen an opinion about the likelihood of an event by comparisons with opinion about the likelihood of other events. In this chapter and the chapters to follow, we introduce the key ideas and practice about thinking subjectively about unknowns and quantify one’s opinions about the values of these unknowns using probability distributions.</p>
<p>As an example, let’s think about plausible values for the proportion <span class="math inline">\(p\)</span> of red balls. As <span class="math inline">\(p\)</span> is a proportion, it can take any possible value between 0 and 1. In the calibration experiment introduced in Chapter 1, we focus on the scenario where only one value of <span class="math inline">\(p\)</span> is of interest. For example, when one thinks that <span class="math inline">\(p\)</span> is 0.5, it is saying that one’s opinion about the probability of the value <span class="math inline">\(p=0.5\)</span> is one. When we phrase it this way (``one’s opinion about the probability of <span class="math inline">\(p=0.5\)</span> is one”), it sounds like a very strong opinion, because one only allows <span class="math inline">\(p\)</span> to take one possible value, and gives probability one of that happening. Since one typically has no thought about the exact value of the proportion <span class="math inline">\(p\)</span>, setting one possible value for the proportion with probability one seems too strong.</p>
<p>Instead suppose that the proportion <span class="math inline">\(p\)</span> can take multiple values between 0 and 1. In particular, let’s consider two scenarios, in both <span class="math inline">\(p\)</span> can take 10 different values, denoted by set <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray}
A = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}
\end{eqnarray}\]</span></p>
<p>Though <span class="math inline">\(p\)</span> can take the same 10 multiple values in both scenarios, we assign different probabilities to each possible value.</p>
<ul>
<li>Scenario 1: <span class="math display">\[\begin{eqnarray}
f_1(A) = (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)
\end{eqnarray}\]</span></li>
<li>Scenario 2: <span class="math display">\[\begin{eqnarray}
f_2(A) = (0.05, 0.05, 0.05, 0.175, 0.175, 0.175, 0.175, 0.05, 0.05, 0.05) \nonumber \\
\end{eqnarray}\]</span></li>
</ul>
<p>To visually compare the values of two probability distributions <span class="math inline">\(f_1(A)\)</span> and <span class="math inline">\(f_2(A)\)</span>, we plot <span class="math inline">\(f_1(A)\)</span> and <span class="math inline">\(f_2(A)\)</span> on the same graph.</p>
<div class="cell">

</div>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/RedBallProb_prob.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">The same ten possible values of <span class="math inline">\(p\)</span>, but two sets of probabilities.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Figure 7.1 labels the <span class="math inline">\(x\)</span>-axis as the values of <span class="math inline">\(p\)</span> (range from 0 to 1), <span class="math inline">\(y\)</span>-axis as the probabilities (range from 0 to 1). For both panels, there are ten bars, each representing the possible values of <span class="math inline">\(p\)</span> in the set <span class="math inline">\(A = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}\)</span>.</p>
<p>The probability assignment in <span class="math inline">\(f_1(A)\)</span> is called a discrete Uniform distribution, where each possible value of the proportion <span class="math inline">\(p\)</span> is equally likely. Since there are ten possible values of <span class="math inline">\(p\)</span>, each value gets assigned a probability of <span class="math inline">\(1/10 = 0.1\)</span>. This assignment expresses the opinion that <span class="math inline">\(p\)</span> can be any value from the set <span class="math inline">\(A = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}\)</span>, and each value has a probability of <span class="math inline">\(0.1\)</span>.</p>
<p>The probability assignment in <span class="math inline">\(f_2(A)\)</span> is also discrete, however, we do not see a Uniform distribution pattern of the probabilities across the board. What we see is that the probabilities of the first three values (0.1, 0.2, and 0.3) and last three (0.8, 0.9, and 1.0) values of <span class="math inline">\(p\)</span> are each <span class="math inline">\(1/3.5\)</span> of that of the middle four (0.4, 0.5, 0.6, and 0.7) values. The shape of the bins reflects the opinion that the middle values of <span class="math inline">\(p\)</span> are 3.5 times as likely as the extreme values of <span class="math inline">\(p\)</span>.</p>
<p>Both sets of probabilities follow the three probability axioms in Chapter 1. One sees that within each set,</p>
<ol type="1">
<li><p>Each probability is nonnegative;</p></li>
<li><p>The sum of the probabilities is 1;</p></li>
<li><p>The probability of mutually exclusive values is the sum of probability of each value, e.g.&nbsp;probability of <span class="math inline">\(p = 0.1\)</span> or <span class="math inline">\(p = 0.2\)</span> is <span class="math inline">\(0.1 + 0.1\)</span> in <span class="math inline">\(f_1(A)\)</span>, and <span class="math inline">\(0.05 + 0.05\)</span> in <span class="math inline">\(f_2(A)\)</span>.</p></li>
</ol>
<p>In this introduction, we have presented a way to think about proportions subjectively. We have introduced a way to allow multiple values of <span class="math inline">\(p\)</span>, and perform probability assignments that follow the three probability axioms. One probability distribution expresses a unique opinion about the proportion <span class="math inline">\(p\)</span>.</p>
<p>To answer our inference question “what is the proportion of red balls in the box”, we will take a random sample of 10 balls, and use the observed proportion of red balls in that sample to sharpen and update our belief about <span class="math inline">\(p\)</span>. Bayesian inference is a formal method for implementing this way of thinking and problem solving, including three general steps.</p>
<ul>
<li><p>Step 1: <strong>Prior</strong>: express an opinion about the location of the proportion <span class="math inline">\(p\)</span> before sampling.</p></li>
<li><p>Step 2: <strong>Data/Likelihood</strong>: take the sample and record the observed proportion of red balls.</p></li>
<li><p>Step 3: <strong>Posterior</strong>:use Bayes’ rule to sharpen and update the previous opinion about <span class="math inline">\(p\)</span> given the information from the sample.</p></li>
</ul>
<p>As indicated in the parentheses, the first step “Prior” constructs <strong>prior</strong> opinion about the quantity of interest, and a probability distribution is used (like <span class="math inline">\(f_1(A)\)</span> and <span class="math inline">\(f_2(A)\)</span> earlier) to quantify the prior opinion. The name “prior” indicates that the opinion should be formed before collecting any data.</p>
<p>The second step “Data” is the process of data collection, where the quantity of interest is observed in the collected data. For example, if our 10-ball sample contains 4 red balls and 6 white balls, the observed proportion of red balls is <span class="math inline">\(4/10 = 0.4\)</span>. Informally, how does this information help us sharpen one’s opinion about <span class="math inline">\(p\)</span>? Intuitively one would give more probability to <span class="math inline">\(p = 0.4\)</span>, but it is unclear how the probabilities would be redistributed among the 10 values in <span class="math inline">\(A\)</span>. Since the sum of all probabilities is 1, is it possible that some of the larger proportion values, such as <span class="math inline">\(p = 0.9\)</span> and <span class="math inline">\(p = 1.0\)</span>, will receive probabilities of zero? To address these questions, the third step is needed.</p>
<p>The third step “Posterior” combines one’s prior opinion and the collected data to update one’s opinion about the quantity of interest. Just like the example of observing 4 red balls in the 10-ball sample, one needs a structured way of updating the opinion from prior to posterior.</p>
<p>Throughout this chapter, the entire inference process will be described for learning about a proportion <span class="math inline">\(p\)</span>. This chapter will discuss how to express prior opinion that matches with one’s belief, how to extract information from the data/likelihood, and how to update our opinion to its posterior.</p>
</section>
<section id="bayesian-inference-with-discrete-priors" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="bayesian-inference-with-discrete-priors"><span class="header-section-number">1.2</span> Bayesian Inference with Discrete Priors</h2>
<section id="example-students-dining-preference" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="example-students-dining-preference"><span class="header-section-number">1.2.1</span> Example: students’ dining preference</h3>
<p>Let’s start our Bayesian inference for proportion <span class="math inline">\(p\)</span> with discrete prior distributions with a students’ dining preference example. A popular restaurant in a college town has been in business for about 5 years. Though the business is doing well, the restaurant owner wishes to learn more about his customers. Specifically, he is interested in learning about the dining preferences of the students. The owner plans to conduct a survey by asking students “what is your favorite day for eating out?” In particular, he wants to find out what percentage of students prefer to dine on Friday, so he can plan ahead for ordering supplies and giving promotions.</p>
<p>Let <span class="math inline">\(p\)</span> denote the proportion of all students whose answer is Friday.</p>
</section>
<section id="discrete-prior-distributions-for-proportion-p" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="discrete-prior-distributions-for-proportion-p"><span class="header-section-number">1.2.2</span> Discrete prior distributions for proportion <span class="math inline">\(p\)</span></h3>
<p>Before giving out the survey, let’s pause and think about the possible values for the proportion <span class="math inline">\(p\)</span>. Not only does one want to know about possible values, but also the probabilities associated with the values. A probability distribution provides a measure of belief for the proportion and it ultimately will help the restaurant owner improve his business.</p>
<p>One might not know much about students’ dining preference, but it is possible to come up with a list of plausible values for the proportion. There are seven days a week. If each day was equally popular, then one would expect 1/7 or approximately 15% of all students to choose Friday. The owner recognizes that Friday is the start of the weekend, therefore there should be a higher chance of being students’ preferred day of dining out. So perhaps <span class="math inline">\(p\)</span> starts with 0.3. Then what about the largest plausible value? Letting this largest value be 1 seems unrealistic, as there are six other days in the week. Suppose that one chooses 0.8 to be the largest plausible value, and then comes up with the list of values of <span class="math inline">\(p\)</span> to be the six values going from 0.3 to 0.8 with an increment of 0.1.</p>
<p><span class="math display">\[\begin{eqnarray}
p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}
\label{eq:dining:p}
\end{eqnarray}\]</span></p>
<p>Next one needs to assign probabilities to the list of plausible values of <span class="math inline">\(p\)</span>. Since one may not know much about the location of the probabilities <span class="math inline">\(p\)</span>, a good place to start is a discrete Uniform prior (recall the discrete Uniform prior distribution for <span class="math inline">\(p\)</span>, the proportion of red balls, in Section 7.1). A discrete Uniform prior distribution expresses the opinion that all plausible values of <span class="math inline">\(p\)</span> are equally likely. In the current students’ dining preference example, if one decides on six plausible values of <span class="math inline">\(p\)</span> as in Equation (7.1), each of the six values gets a prior probability of 1/6. One labels this prior as <span class="math inline">\(\pi_l\)</span>, where <span class="math inline">\(l\)</span> stands for laymen (for all of us who are not in the college town restaurant business). Note that in the notation <span class="math inline">\(f_l(p)\)</span>, the first <span class="math inline">\(p\)</span> stands for probability, and the <span class="math inline">\(p\)</span> in the parenthesis is our quantity of interest, the proportion <span class="math inline">\(p\)</span> of students preferring to dine out on Friday.</p>
<p><span class="math display">\[\begin{eqnarray}
\pi_l(p)= (1/6, 1/6, 1/6, 1/6, 1/6, 1/6)
\label{eq:dining:laymenprior}
\end{eqnarray}\]</span></p>
<p>With five years of experience of running his restaurant in this college town, the restaurant owner might have different opinions about likely values of <span class="math inline">\(p\)</span>. Suppose he agrees with us that <span class="math inline">\(p\)</span> could take the 6 plausible values from 0.3 to 0.8, but he assigns a different prior distribution for <span class="math inline">\(p\)</span>. In particular, the restaurant owner thinks that values of 0.5 and 0.6 are most likely – each of these values is twice as likely as the other values. His prior is labelled as <span class="math inline">\(\pi_e\)</span>, where <span class="math inline">\(e\)</span> stands for expert.</p>
<p><span class="math display">\[\begin{eqnarray}
\pi_e(p)= (0.125, 0.125, 0.250, 0.250, 0.125, 0.125)
\label{eq:dining:expertprior}
\end{eqnarray}\]</span></p>
<p>To obtain <span class="math inline">\(\pi_e(p)\)</span> efficiently, one can use the <code>ProbBayes</code> R package. First a data frame is created by providing the list of plausible values of <span class="math inline">\(p\)</span> and corresponding weights assigned to each value using the function <code>data.frame()</code>. As one can see here, one does not have to calculate the probability – one only needs to give the weights (e.g.&nbsp;giving <span class="math inline">\(p = 0.3, 0.4, 0.7, 0.8\)</span> weight 1 and giving <span class="math inline">\(p = 0.5, 0.6\)</span> weight 2, to reflect the owner’s opinion ``0.5 and 0.6 are twice as likely as the other values”).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>bayes_table <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">p =</span> <span class="fu">seq</span>(.<span class="dv">3</span>, .<span class="dv">8</span>, <span class="at">by=</span>.<span class="dv">1</span>),</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>                          <span class="at">Prior =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>bayes_table</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    p Prior
1 0.3     1
2 0.4     1
3 0.5     2
4 0.6     2
5 0.7     1
6 0.8     1</code></pre>
</div>
</div>
<p>One uses the function <code>mutate()</code> to normalize these weights to obtain the prior probabilities in the Prior column.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>bayes_table <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">Prior =</span> Prior <span class="sc">/</span> <span class="fu">sum</span>(Prior)) <span class="ot">-&gt;</span> bayes_table</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>bayes_table</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    p Prior
1 0.3 0.125
2 0.4 0.125
3 0.5 0.250
4 0.6 0.250
5 0.7 0.125
6 0.8 0.125</code></pre>
</div>
</div>
<p>One conveniently plots the restaurant owner’s prior distribution by use of <code>ggplot2</code> functions. This distribution is displayed in Figure 7.2.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/prior.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">The restaurant owner’s prior distribution for the proportion <span class="math inline">\(p\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>It is left as an exercise for the reader to compute and plot the laymen’s prior <span class="math inline">\(\pi_l(p)\)</span> in Equation (7.2). For the rest of this section, we will work with the expert’s prior <span class="math inline">\(\pi_e(p)\)</span>.</p>
</section>
<section id="likelihood" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="likelihood"><span class="header-section-number">1.2.3</span> Likelihood</h3>
<p>The next step in the inference process is the data collection. The restaurant owner gives a survey to 20 student diners at the restaurant. Out of the 20 student respondents, 12 say that their favorite day for eating out is Friday. Recall the quantity of interest is proportion <span class="math inline">\(p\)</span> of the population of students choosing Friday.</p>
<p>The likelihood is a function of the quantity of interest, which is the proportion <span class="math inline">\(p\)</span>. The owner has conducted an experiment 20 times, where each experiment involves a “yes” or “no” answer from the respondent to the rephrased question “whether Friday is your preferred day to dine out”. Then the proportion <span class="math inline">\(p\)</span> is the probability a student answers ``yes”.</p>
<p>Does this ring a bell of what we have seen before? Indeed, in Chapter 4, one has seen this type of experiment, a Binomial experiment, similar to the dining survey. Recall that a Binomial experiment needs to satisfy four conditions:</p>
<ul>
<li>One is repeating the same basic task or trial many times – let the number of trials be denoted by <span class="math inline">\(n\)</span>.</li>
<li>On each trial, there are two possible outcomes called <code>success" or</code>failure”.</li>
<li>The probability of a success, denoted by <span class="math inline">\(p\)</span>, is the same for each trial.</li>
<li>The results of outcomes from different trials are independent.</li>
</ul>
<p>If one recognizes an experiment as being Binomial, then all one needs to know is <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> to determine probabilities for the number of successes <span class="math inline">\(Y\)</span>. The probability of <span class="math inline">\(y\)</span> successes in a Binomial experiment is given by</p>
<p><span class="math display">\[\begin{eqnarray}
Prob(Y=y) = {n \choose y} p^y (1 - p)^{n - y}, y = 0, \cdots, n.
\end{eqnarray}\]</span></p>
<p>Assuming the dining survey is a random sample (thus independent outcomes), this is the result of a Binomial experiment. The likelihood is the chance of 12 successes in 20 trials viewed as a function of the probability of success <span class="math inline">\(p\)</span>: <span class="math display">\[\begin{eqnarray}
Likelihood = L(p) = {20 \choose 12} p ^ {12} (1 - p) ^ 8.
\label{eq:dining:likelihood}
\end{eqnarray}\]</span></p>
<p>Generally one uses <span class="math inline">\(L\)</span> to denote a likelihood function — one sees in Equation (7.5), <span class="math inline">\(L\)</span> is a function of <span class="math inline">\(p\)</span>. Note that the value of <span class="math inline">\(n\)</span>, the total number of trials, is known and the number of successes <span class="math inline">\(Y\)</span> is observed to be 12. The proportion <span class="math inline">\(p\)</span>, is the parameter of the Binomial experiment and the likelihood is a function of the proportion <span class="math inline">\(p\)</span>.</p>
<p>The likelihood function <span class="math inline">\(L(p)\)</span> is efficiently computed using the <code>dbinom()</code> function in R. In order to use this function, we need to know the sample size <span class="math inline">\(n\)</span> (20 in the dining survey), the number of successes <span class="math inline">\(y\)</span> (12 in the dining survey), and <span class="math inline">\(p\)</span> (the list of 6 plausible values created in Section 7.2.2; <span class="math inline">\(p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}\)</span>). Note that we only need the plausible values of <span class="math inline">\(p\)</span>, not yet the assigned probabilities in the prior distribution. The prior will be used in the third step to update the opinion of <span class="math inline">\(p\)</span> to its posterior.</p>
<p>Below is the example R code of finding the probability of 12 successes in a sample of 20 for each value of the proportion <span class="math inline">\(p\)</span>. The values are placed in the <code>Likelihood</code> column of the <code>bayes_table</code> data frame.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>bayes_table<span class="sc">$</span>Likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(<span class="dv">12</span>, <span class="at">size=</span><span class="dv">20</span>, <span class="at">prob=</span>bayes_table<span class="sc">$</span>p)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>bayes_table</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    p Prior  Likelihood
1 0.3 0.125 0.003859282
2 0.4 0.125 0.035497440
3 0.5 0.250 0.120134354
4 0.6 0.250 0.179705788
5 0.7 0.125 0.114396740
6 0.8 0.125 0.022160877</code></pre>
</div>
</div>
</section>
<section id="posterior-distribution-for-proportion-p" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="posterior-distribution-for-proportion-p"><span class="header-section-number">1.2.4</span> Posterior distribution for proportion <span class="math inline">\(p\)</span></h3>
<p>The posterior probabilities are found as an application of Bayes’ rule. This recipe will be illustrated first through a step-by-step calculation process. Next the process is demonstrated with the <code>bayesian_crank()</code> function in the <code>ProbBayes</code> R package, which implements the Bayes’ rule calculation and outputs the posterior probabilities.</p>
<p>Let <span class="math inline">\(\pi(p)\)</span> to be the prior distribution of <span class="math inline">\(p\)</span>, let <span class="math inline">\(L(p)\)</span> denote the likelihood function, and <span class="math inline">\(\pi(p \mid y)\)</span> to be the posterior distribution of <span class="math inline">\(p\)</span> after observing the number of successes <span class="math inline">\(y\)</span>. For discrete parameters, such as the proportion <span class="math inline">\(p\)</span> in our case, one is able to enumerate the list of plausible values and assign prior probabilities to the values. If <span class="math inline">\(p_i\)</span> represents a particular value of <span class="math inline">\(p\)</span>, Bayes’ rule for a discrete parameter has the form <span class="math display">\[\begin{eqnarray}
\pi(p_i \mid y)  = \frac{\pi(p_i) \times L(p_i)} {\sum_j \pi(p_j) \times L(p_j)},
\label{eq:Discrete:bayesrule}
\end{eqnarray}\]</span> where <span class="math inline">\(\pi(p_i)\)</span> is the prior probability of <span class="math inline">\(p = p_i\)</span>, <span class="math inline">\(L(p_i)\)</span> is the likelihood function evaluated at <span class="math inline">\(p = p_i\)</span>, and <span class="math inline">\(\pi(p_i \mid y)\)</span> is the posterior probability of <span class="math inline">\(p = p_i\)</span> given the number of successes <span class="math inline">\(y\)</span>. By the <strong>Law of Total Probability</strong>, the denominator gives the marginal distribution of the observation <span class="math inline">\(y\)</span>.</p>
<p>Bayes’ rule can also be expressed as ``prior times likelihood”: <span class="math display">\[\begin{eqnarray}
\pi(p_i \mid y)  \propto \pi(p_i) \times L(p_i)
\label{eq:Discrete:bayesruleProp}
\end{eqnarray}\]</span> Equation (7.7) ignores the denominator and states that the posterior is proportional to the product of the prior and the likelihood. As one will see soon, the value of the denominator is a constant, meaning that its purpose is to normalize the numerator. It is convenient to work with Bayes’ rule as in Equation (7.7) in later chapters. However, it is instructive to show the exact calculation of Equation (7.6), because one has a finite sum in the denominator and it is possible to obtain the analytical solution. In the case where the prior is continuous, it will be more difficult to analytically compute the normalizing constant.</p>
<p>Returning to the students’ dining preference example, the list of plausible values of the proportion is <span class="math inline">\(p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}\)</span> and according to the restaurant owner’s expert prior, the assigned probabilities are <span class="math inline">\(\pi_e(p)= (0.125, 0.125, 0.250, 0.250, 0.125, 0.125)\)</span> (recall Figure 7.2). After observing the number of successes, the likelihood values are calculated for the models using <code>dbinom()</code> function, as presented in Section 7.2.3.</p>
<p>The denominator is the sum of the products of the prior and the likelihood at each possible <span class="math inline">\(p_i\)</span>, which, given the Law of Total Probability, is equal to the marginal probability of the data <span class="math inline">\(f(y)\)</span>. One can think of the above formula as reweighing or normalizing the probability of <span class="math inline">\(\pi(p_i \mid y)\)</span> by all possible values of <span class="math inline">\(p\)</span>. In the case of discrete models like this, the marginal probability of the likelihood is computed through <span class="math inline">\(\sum_j f(p_j) \times L(p_j)\)</span>.</p>
<p>In this setup, the computation of the posterior probabilities of different <span class="math inline">\(p_i\)</span> values is straightforward. First, one calculates the denominator and denote the value as <span class="math inline">\(D\)</span>. <span class="math display">\[\begin{eqnarray*}
D &amp;=&amp; \pi(0.3) \times L(0.3) + \pi(0.4) \times L(0.4) + \cdots + \pi(0.8) \times L(0.8) \\
&amp;=&amp; 0.125 \times {20 \choose 12}(0.3)^{12}(1-0.3)^{8} + \cdots + 0.125 \times {20 \choose 12}(0.8)^{12}(1-0.8)^{8} \nonumber \\ &amp;\approx&amp; 0.0969.
\end{eqnarray*}\]</span> Then the posterior probability of <span class="math inline">\(p = 0.3\)</span> is given by <span class="math display">\[\begin{eqnarray*}
\pi(p = 0.3 \mid 12) &amp;=&amp; \frac{\pi(0.3) \times L(0.3)}{D} \\
&amp;=&amp; \frac{0.125 \times {20 \choose 12}(0.3)^{12}(1-0.3)^{8}}{D}  \\
&amp;\approx&amp; 0.005.
\end{eqnarray*}\]</span> In a similar fashion, the posterior probability of <span class="math inline">\(p=0.5\)</span> is calculated as <span class="math display">\[\begin{eqnarray*}
\pi(p = 0.5 \mid 12) &amp;=&amp; \frac{\pi(0.5) \times L(0.5)}{D} \\
&amp;=&amp; \frac{0.125 \times {20 \choose 12}(0.5)^{12}(1-0.5)^{8}}{D} \\
&amp;\approx&amp; 0.310.
\end{eqnarray*}\]</span> One sees that the denominator is the same for the posterior probability calculation of every value of <span class="math inline">\(p\)</span>. This calculation gets tedious for a large number of possible values of <span class="math inline">\(p\)</span>. Relying on statistical software such as <code>R</code> helps us simplify the tasks.</p>
<p>To use the <code>bayesian_crank()</code> function, recall that we have already created a data frame with variables <code>p</code>, <code>Prior</code>, and <code>Likelihood</code>. Then the <code>bayesian_crank()</code> function is used to compute the posterior probabilities.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">bayesian_crank</span>(bayes_table) <span class="ot">-&gt;</span> bayes_table</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>bayes_table</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    p Prior  Likelihood      Product   Posterior
1 0.3 0.125 0.003859282 0.0004824102 0.004975901
2 0.4 0.125 0.035497440 0.0044371799 0.045768032
3 0.5 0.250 0.120134354 0.0300335884 0.309786454
4 0.6 0.250 0.179705788 0.0449264469 0.463401326
5 0.7 0.125 0.114396740 0.0142995925 0.147495530
6 0.8 0.125 0.022160877 0.0027701096 0.028572757</code></pre>
</div>
</div>
<p>As one sees in the <code>bayes_table</code> output, the <code>bayesian_crank()</code> function computes the product of <code>Prior</code> and <code>Likelihood</code> and stores the values in the column <code>Product</code>, then normalizes each product with the sum of all products to produce the posterior probabilities, stored in the column <code>Posterior</code>.</p>
<p>Figure 7.3 compares the prior probabilities in the bottom panel with the posterior probabilities in the top panel. Notice the difference in the two distributions. After observing the survey results (i.e.&nbsp;the data/likelihood), the owner is more confident that <span class="math inline">\(p\)</span> is equal to 0.5 or 0.6, and it is unlikely for <span class="math inline">\(p\)</span> to be 0.3, 0.4, 0.7, and 0.8. Recall that the data gives an observed proportion 12/20 = 0.6. Since the posterior is a combination of prior and data/likelihood, it is not surprising that the data/likelihood helps the owner to sharpen his belief about proportion <span class="math inline">\(p\)</span> and place a larger posterior probability around 0.6.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/priorpost2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Prior and posterior distributions on the proportion <span class="math inline">\(p\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="inference-students-dining-preference" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="inference-students-dining-preference"><span class="header-section-number">1.2.5</span> Inference: students’ dining preference</h3>
<p>Let’s revisit the posterior distribution table to perform some inference. What is the posterior probability that over half of the students prefer eating out on Friday? One is interested in the probability that <span class="math inline">\(p &gt;\)</span> 0.5, in the posterior. Looking at the table, this posterior probability is equal to <span class="math display">\[\begin{eqnarray*}
Prob(p &gt; 0.5) \approx 0.463 + 0.147 + 0.029 = 0.639.
\end{eqnarray*}\]</span> This means the owner is reasonably confident (with probability 0.639) that over half of the college students prefer to eat out on Friday.</p>
<p>One easily obtains the probability from the R output, for example.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(bayes_table<span class="sc">$</span>Posterior[bayes_table<span class="sc">$</span>p <span class="sc">&gt;</span> <span class="fl">0.5</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.6394696</code></pre>
</div>
</div>
</section>
<section id="discussion-using-a-discrete-prior" class="level3" data-number="1.2.6">
<h3 data-number="1.2.6" class="anchored" data-anchor-id="discussion-using-a-discrete-prior"><span class="header-section-number">1.2.6</span> Discussion: using a discrete prior</h3>
<p>Specifying a discrete prior has two steps: (1) specifying a list of plausible values of the parameter of interest, and (2) assigning probabilities to the plausible values. It is important to remember the three probability axioms when specifying a discrete prior.</p>
<p>After the prior specification, the next component is the data/likelihood, which can also be broken up into two steps. First, one constructs a suitable experiment that works for the particular scenario. Here one has a Binomial experiment for a survey to a fixed number of respondents, the answers are classified into <code>yes" and</code>no” or <code>success" and</code>failure”, the outcome of interest is the number of successes and trials are independent. From the Binomial distribution, one obtains the likelihood function which is evaluated at each possible value of the parameter of interest. In our example, the <code>dbinom()</code> R function was used to calculate the likelihood function.</p>
<p>Last, the posterior probabilities are calculated using Bayes’ rule. In particular for the discrete case, follow Equation (7.6). The calculation of the denominator is tedious, however practice with the Bayes’ rule calculation enhances one’s understanding of Bayesian inference. R functions such as <code>bayesian_crank()</code> are helpful for implementing the Bayes’ rule calculations. Bayesian inference follows from a suitable summarization of the posterior probabilities. In our example, inference was illustrated by calculating the probability that over half of the students prefer eating out on Friday.</p>
<p>Let’s revisit the list of plausible values of proportion <span class="math inline">\(p\)</span> of students preferring Friday in dining out in the example. Although <span class="math inline">\(p = 1.0\)</span>, that is, everyone prefers Friday, is very unlikely, one might not want to eliminate this proportion value from consideration. As one observes in the Bayes’ rule calculation process shown in Sections 7.2.3 and 7.2.4, if one does not include <span class="math inline">\(p = 1.0\)</span> as one of the plausible values in the prior distribution in Section 7.2.2, this value will also be given a probability of zero in the posterior.</p>
<p>Alternatively, one could choose the alternative set of values <span class="math display">\[\begin{eqnarray*}
p = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\},
\end{eqnarray*}\]</span> and assign a very small prior probability (e.g.&nbsp;0.05 or even smaller) for <span class="math inline">\(p = 1.0\)</span> to express the opinion that <span class="math inline">\(p = 1.0\)</span> is very unlikely. One may assign small prior probabilities for other large values of <span class="math inline">\(p\)</span> such as <span class="math inline">\(p = 0.9\)</span>.</p>
<p>This comment illustrates a limitation of specifying a discrete prior for a proportion <span class="math inline">\(p\)</span>. If a plausible value is not specified in the prior distribution (e.g.&nbsp;<span class="math inline">\(p = 1.0\)</span> is not in the restaurant owner’s prior distribution), it will be assigned a probability of zero in the posterior (e.g.&nbsp;<span class="math inline">\(p = 1.0\)</span> is not in the restaurant owner’s posterior distribution).</p>
<p>It generally is more desirable to have <span class="math inline">\(p\)</span> to be any value in [0, 1] including less plausible values such as <span class="math inline">\(p = 1.0\)</span>. To make this happen, the proportion <span class="math inline">\(p\)</span> should be allowed to take any value between 0 and 1, which means <span class="math inline">\(p\)</span> will be a continuous variable. In this situation, it is necessary to construct a continuous prior distribution for <span class="math inline">\(p\)</span>. A popular class of continuous prior distributions for proportion is the Beta distribution which is the subject of the next section.</p>
</section>
</section>
<section id="continuous-priors" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="continuous-priors"><span class="header-section-number">1.3</span> Continuous Priors</h2>
<p>Let’s continue our students’ dining preference example. A restaurant owner is interested in learning about the proportion <span class="math inline">\(p\)</span> of students whose favorite day for eating out is Friday.</p>
<p>The proportion <span class="math inline">\(p\)</span> should be a value between 0 and 1. Previously, we used a discrete prior for <span class="math inline">\(p\)</span>, representing the belief that <span class="math inline">\(p\)</span> only takes the six different values 0.3, 0.4, 0.5, 0.6, 0.7, and 0.8. An obvious limitation of this assumption is, what if the true <span class="math inline">\(p\)</span> is 0.55? If the value 0.55 is not specified in the prior distribution of <span class="math inline">\(p\)</span> (that is, a zero probability is assigned to the value <span class="math inline">\(p\)</span> = 0.55), then by the Bayes’ rule calculation (either by hand or by the useful <code>bayesian_crank()</code> function) there will be zero posterior probability assigned to 0.55. It is therefore preferable to specify a prior that allows <span class="math inline">\(p\)</span> to be any value in the interval [0, 1].</p>
<p>To represent such a prior belief, it is assumed that <span class="math inline">\(p\)</span> is continuous on [0, 1]. Suppose again that one is a layman unfamiliar with the pattern of dining during a week. Then one possible choice of a continuous prior for <span class="math inline">\(p\)</span> is the continuous Uniform distribution, which expresses the opinion that <span class="math inline">\(p\)</span> is equally likely to take any value between 0 and 1.</p>
<p>Formally, the probability density function of the continuous Uniform on the interval <span class="math inline">\((a, b)\)</span> is <span class="math display">\[\begin{eqnarray}
\pi(p) =
\begin{cases}
  \frac{1}{b - a} &amp; \text{for }a \le p \le b,\\    
  0             &amp; \text{for }p &lt; a \,\, \text{or } p &gt; b.
\end{cases}
\label{eq:Binomial:Continuous:Uniform}
\end{eqnarray}\]</span> In our situation <span class="math inline">\(p\)</span> is a continuous Uniform random variable on [0, 1], we have <span class="math inline">\(\pi(p) = 1\)</span> for <span class="math inline">\(p \in [0, 1]\)</span>, and <span class="math inline">\(\pi(p) = 0\)</span> everywhere else.</p>
<p>What about other possible continuous prior distributions for <span class="math inline">\(p\)</span> on [0, 1]? Consider a prior distribution for the restaurant owner who has some information about the location (i.e.&nbsp;value) of <span class="math inline">\(p\)</span>. This owner would be interested in a continuous version of the discrete prior distribution where values of <span class="math inline">\(p\)</span> between 0.3 and 0.8 are more likely than the values at the two ends.</p>
<p>The Beta family of continuous distributions is useful for representing prior knowledge in this situation. A Beta distribution, denoted by Beta(<span class="math inline">\(a, b)\)</span>, represents probabilities for a random variable falling between 0 and 1. This distribution has two shape parameters, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, with probability density function given by <span class="math display">\[\begin{eqnarray}
\pi(p) = \frac{1}{B(a, b)} p^{a - 1} (1 - p)^{b - 1}, \, \, 0 \le p \le 1,
\end{eqnarray}\]</span> where <span class="math inline">\(B(a, b)\)</span> is the Beta function defined by <span class="math inline">\(B(a, b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\)</span>, where <span class="math inline">\(\Gamma\)</span> is the Gamma function. For future reference, it is useful to know that if <span class="math inline">\(p \sim {\rm Beta}(a, b)\)</span>, its mean <span class="math inline">\(E[p] = \frac{a}{a+b}\)</span> and its variance <span class="math inline">\(V(p) = \frac{ab}{(a+b)^2(a+b+1)}\)</span>. The continuous Uniform in Equation (7.8) is a special case of the Beta distribution: <span class="math inline">\(\textrm{Uniform}(0, 1) = \textrm{Beta}(1, 1)\)</span>.</p>
<p>For the remainder of this section, Section 7.3.1 introduces the Beta distribution and Beta probabilities, and Section 7.3.2 focuses on several ways of choosing a Beta prior that reflects one’s opinion about the location of a proportion.</p>
<section id="the-beta-distribution-and-probabilities" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="the-beta-distribution-and-probabilities"><span class="header-section-number">1.3.1</span> The Beta distribution and probabilities</h3>
<p>The two shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> control the shape of the Beta density curve. Figure 7.4 shows density curves of Beta distributions for several choices of the shape parameters. One observes from this figure that the Beta density curve displays vastly different shapes for varying choices of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. For example, <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> represents the prior belief that extreme values of <span class="math inline">\(p\)</span> are likely and <span class="math inline">\(p=0.5\)</span> is the least probable value. In the students’ dining preference example, specifying a <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> would reflect the owner’s belief that the proportion of students dining out on Friday is either very high (near one) or very low (near one) and not likely to be moderate values.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/Beta-priors.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Illustration of nine Beta density curves.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>As the Beta is a common continuous distribution, R functions are available for Beta distribution calculations. We provide a small example of “Beta” functions for <code>Beta(1, 1)</code>, where the two shape parameters 1 and 1 are the second and third arguments of the functions.</p>
<p>Recall the following useful results from previous material: (1) a <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> distribution is a Uniform density on (0, 1), (2) the density of <span class="math inline">\(\textrm{Uniform}(0, 1)\)</span> is <span class="math inline">\(\pi(p) = 1\)</span> on [0, 1], and (3) if <span class="math inline">\(p \sim \textrm{Uniform}(0, 1)\)</span>, then the cdf <span class="math inline">\(F(x) = Prob(p \leq x) = x\)</span> for <span class="math inline">\(x \in [0, 1]\)</span>.</p>
<ul>
<li><code>dbeta()</code>: the probability density function for a <span class="math inline">\(\textrm{Beta}(a, b)\)</span> which takes a value of the random variable as its input and outputs the probability density function at that value.</li>
</ul>
<p>For example, we evaluate the density function of <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> at the values <span class="math inline">\(p = 0.5\)</span> and <span class="math inline">\(p = 0.8\)</span>, which should be both 1, and 0 at <span class="math inline">\(p = 1.2\)</span> which should be 0 since this value is outside of [0, 1].</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dbeta</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>, <span class="fl">1.2</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 1 0</code></pre>
</div>
</div>
<ul>
<li><code>pbeta()</code>: the distribution function of a Beta(a; b) random variable, which takes a value x and gives the value of the random variable at that value, F(x).</li>
</ul>
<p>For example, suppose one wishes to evaluate the distribution function of <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> at <span class="math inline">\(p = 0.5\)</span> and <span class="math inline">\(p = 0.8\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pbeta</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5 0.8</code></pre>
</div>
</div>
<p>One calculates the probability of <span class="math inline">\(p\)</span> between 0.5 and 0.8, i.e.&nbsp;<span class="math inline">\(Prob(0.5 \le p \le 0.8)\)</span> by taking the difference of the cdf at the two values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pbeta</span>(<span class="fl">0.8</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">-</span> <span class="fu">pbeta</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.3</code></pre>
</div>
</div>
<ul>
<li><code>qbeta()</code>: the quantile function of a <span class="math inline">\(\textrm{Beta}(a, b)\)</span>, which inputs a probability value <span class="math inline">\(p\)</span> and outputs the value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F(x) = p\)</span>.</li>
</ul>
<p>For example, suppose one wishes to calculate the quantile of <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> at <span class="math inline">\(p = 0.5\)</span> and <span class="math inline">\(p = 0.8\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5 0.8</code></pre>
</div>
</div>
<ul>
<li><code>rbeta()</code>: the random number generator for <span class="math inline">\(\textrm{Beta}(a, b)\)</span>, which inputs the size of a random sample and gives a vector of the simulated random variates.</li>
</ul>
<p>For example, suppose one is interested in simulating a sample of size five from <span class="math inline">\(\textrm{Beta}(1, 1)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rbeta</span>(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.014428665 0.706480894 0.007722286 0.014802991 0.620159572</code></pre>
</div>
</div>
<p>There are additional functions in the <code>ProbBayes</code> R package that aid in visualizing Beta distribution calculations. For example, suppose one has a <span class="math inline">\(\textrm{Beta}(7, 10)\)</span> curve and we want to find the chance that <span class="math inline">\(p\)</span> is between 0.4 and 0.8. Looking at Figure 7.5, this probability corresponds to the area of the shaded region. The special function <code>beta_area()</code> will compute and illustrate this probability. Note the use of the vector <code>c(7, 10)</code> to input the two shape parameters.</p>
<pre><code>beta_area(0.4, 0.8, c(7, 10))</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/betaprob1.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Area represents the probability that a Beta(7, 10) variable lies between 0.4 and 0.8</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>One could also find the chance that <span class="math inline">\(p\)</span> is between 0.4 and 0.8 by subtracting two <code>pbeta()</code> functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pbeta</span>(<span class="fl">0.8</span>, <span class="dv">7</span>, <span class="dv">10</span>) <span class="sc">-</span> <span class="fu">pbeta</span>(<span class="fl">0.4</span>, <span class="dv">7</span>, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5269265</code></pre>
</div>
</div>
<p>The function <code>beta_quantile()</code> works in the same way as <code>qbeta()</code>, the quantile function. However, <code>beta_quantile()</code> automatically produces a plot with the shaded probability area. Figure 7.6 plots and computes the quantile to be 0.408. The chance that <span class="math inline">\(p\)</span> is smaller than 0.408 is 0.5.</p>
<pre><code>beta_quantile(0.5, c(7, 10))</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/betaprob2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Illustration of a 0.5 quantile for a Beta(7, 10) variable.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Alternatively, use the <code>qbeta()</code> function without returning a plot.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbeta</span>(<span class="fl">0.5</span>, <span class="dv">7</span>, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4082265</code></pre>
</div>
</div>
</section>
<section id="choosing-a-beta-density-to-represent-prior-opinion" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="choosing-a-beta-density-to-represent-prior-opinion"><span class="header-section-number">1.3.2</span> Choosing a Beta density to represent prior opinion</h3>
<p>One wants to use a <span class="math inline">\(\textrm{Beta}(a, b)\)</span> density curve to represent one’s prior opinion about the values of the proportion <span class="math inline">\(p\)</span> and their associated probabilities. It is difficult to guess at values of the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> directly. However, there are indirect ways of guessing their values. We present two general methods here.</p>
<p>The first method is to consider the shape parameter <span class="math inline">\(a\)</span> as the prior count of “successes” and the other shape parameter <span class="math inline">\(b\)</span> as the prior count of “failures”. Subsequently, the value <span class="math inline">\(a + b\)</span> represents the <strong>prior sample size</strong> comparable to <span class="math inline">\(n\)</span>, the <strong>data sample size</strong>. Following this setup, one could specify a Beta prior with shape parameter <span class="math inline">\(a\)</span> expressing the number of successes in one’s prior opinion, and the other shape parameter <span class="math inline">\(b\)</span> expressing the number of failures in one’s prior opinion. For example, if one believes that <strong>a priori</strong> there should be about 4 successes and 4 failures, then one could use <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> as the prior distribution for the proportion <span class="math inline">\(p\)</span>.</p>
<p>How can we check if <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> looks like what we believe <strong>a priori</strong>? Recall that <code>rbeta()</code> generates a random sample from a Beta distribution. The R script below generates a random sample of size 1000 from <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> and we plot a histogram and an overlapping density curve. (See top panel of Figure 7.7.) By an inspection of this graph, one decides if this prior is a reasonable approximation to one’s beliefs about the proportion.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/ChooseBeta1_new.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Histograms of 1000 samples of two Beta density curves Beta(4, 4) and Beta(2, 9).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>As a second example, consider a belief that <strong>a priori</strong> there are 2 successes and 9 failures, corresponding to the <span class="math inline">\(\textrm{Beta}(2, 9)\)</span> prior? One can use the <code>rbeta()</code> function take a random sample of 1000 from this prior.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>Beta29samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Comparing the two distributions, note from Figure 7.7 that <span class="math inline">\(\textrm{Beta}(2, 9)\)</span> favors smaller proportion values than <span class="math inline">\(\textrm{Beta}(4, 4)\)</span>.</p>
<p>To further check the quantiles of the prior, one can use the <code>quantile()</code> function on the simulated draws from the prior. For example, if one wishes to check the middle 50% range of values of <span class="math inline">\(p\)</span> from the random sample of values from <span class="math inline">\(\textrm{Beta}(4, 4)\)</span>, one types</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>Beta44samples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="dv">1000</span>, <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(Beta44samples, <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="fl">0.75</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      25%       75% 
0.3866210 0.6249832 </code></pre>
</div>
</div>
<p>This tells us that the probability that <span class="math inline">\(p \leq 0.366\)</span> is 0.25 and the probability that <span class="math inline">\(p \geq 0.616\)</span> is also 0.25. These probability statements should be checked against one’s prior belief about <span class="math inline">\(p\)</span>. If these quantiles do not seem reasonable, one should make adjustments to the values of the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> .</p>
<p>On the surface the two priors <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> and <span class="math inline">\(\textrm{Beta}(40, 40)\)</span> seem similar in that they both have a mean of <span class="math inline">\(0.5\)</span> and represent similar breakdowns of the success and failure counts. However, the aforementioned concept of prior sample size tells us that <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> has a prior sample size of 8 while that of <span class="math inline">\(\textrm{Beta}(40, 40)\)</span> is 80. As we will see in Section 7.4, the prior sample size determines the strength of the prior (i.e.&nbsp;the confidence level in the prior) and so the <span class="math inline">\(\textrm{Beta}(40, 40)\)</span> prior represents a much stronger belief that <span class="math inline">\(p\)</span> is close to the value 0.5.</p>
<p>A second indirect method of determining a Beta prior is by specification of quantiles of the distribution. Specifically, one determines the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> by first specifying two quantiles of the Beta density curve, and then finding the Beta density curve that matches these quantiles. Suppose the restaurant owner uses his knowledge to specify the 0.5 and 0.9 quantiles of the proportion <span class="math inline">\(p\)</span> as follows.</p>
<ol type="1">
<li><p>First, the restaurant owner thinks of a value <span class="math inline">\(p_{50}\)</span> such that the proportion <span class="math inline">\(p\)</span> is equally likely to be smaller or larger than <span class="math inline">\(p_{50}\)</span>. After some thought, he thinks that <span class="math inline">\(p_{50}\)</span> = 0.55.</p></li>
<li><p>Next, the owner thinks of a value <span class="math inline">\(p_{90}\)</span> that he is pretty sure (with probability 0.90) that the proportion <span class="math inline">\(p\)</span> is smaller than <span class="math inline">\(p_{90}\)</span>. After more thought, he decides <span class="math inline">\(p_{90}\)</span> = 0.80.</p></li>
</ol>
<p> One then uses the <code>beta.select()</code> function in the <code>ProbBayes</code> package to find shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> of the Beta density curve that match this information. Each quantile is specified by a list with values <span class="math inline">\(x\)</span> and <span class="math inline">\(p\)</span>. From the output, we see <span class="math inline">\(\textrm{Beta}(3.06, 2.56)\)</span> curve represents the owner’s prior beliefs.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">beta.select</span>(<span class="fu">list</span>(<span class="at">x =</span> <span class="fl">0.55</span>, <span class="at">p =</span> <span class="fl">0.5</span>),</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>            <span class="fu">list</span>(<span class="at">x =</span> <span class="fl">0.80</span>, <span class="at">p =</span> <span class="fl">0.9</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.06 2.56</code></pre>
</div>
</div>
<p>The owner’s Beta density curve is shown here. To make sure this prior is reasonable, the owner should compute several probabilities and quantiles for his prior distribution and see if these values correspond to his opinion.<br>
To illustrate this checking process, Figure 7.8 shows the middle 50% area of the prior distribution. This graph shows that the probability that <span class="math inline">\(p \leq 0.402\)</span> is 0.25 and the probability that <span class="math inline">\(p \geq 0.692\)</span> is also 0.25. If these calculations do not correspond to the owner’s opinion, then maybe some change in the prior distribution would be appropriate.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/betaprob3.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Illustration of the middle 50% of a Beta(3.06, 2.56) curve.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="updating-the-beta-prior" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="updating-the-beta-prior"><span class="header-section-number">1.4</span> Updating the Beta Prior</h2>
<p>In the previous section, we have seen that the restaurant owner thinks that a Beta curve with shape parameters 3.06 and 2.56 is a reasonable reflection of his prior opinion about the proportion of students <span class="math inline">\(p\)</span> whose favorite day for eating out is Friday. Therefore, we work with <span class="math inline">\(\textrm{Beta}(3.06, 2.56)\)</span> as the prior distribution for <span class="math inline">\(p\)</span>.</p>
<p>Now we have the survey results – the survey was administered to 20 students and 12 say that their favorite day for eating out is Friday. As before in Section 7.2, the likelihood, that is the chance of getting this data if the probability of success is <span class="math inline">\(p\)</span> is given by the Binomial formula, <span class="math display">\[\begin{eqnarray*}
Likelihood = L(p) = {20 \choose 12} p ^ {12 }(1 - p) ^ 8.
\end{eqnarray*}\]</span></p>
<p>In this section, the Bayes’ rule calculation of the posterior is presented for the continuous prior case and one discovers an interesting result: if one starts with a Beta prior for a proportion <span class="math inline">\(p\)</span>, and the data is Binomial, then the posterior will also be a Beta distribution. The Beta posterior is a natural combination of the information contained in the Beta prior and the Binomial sampling, as one would expect in typical Bayesian inference. This is an illustration of the use of a conjugate prior where the prior and posterior densities are in the same family of distributions.</p>
<section id="bayes-rule-calculation" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="bayes-rule-calculation"><span class="header-section-number">1.4.1</span> Bayes’ rule calculation</h3>
<p>First we demonstrate the Bayes’ rule calculation of the posterior of <span class="math inline">\(p\)</span> through the proportional statement: <span class="math display">\[\begin{eqnarray}
\pi(p \mid y) \propto  \pi(p) \times L(p).
\end{eqnarray}\]</span></p>
<p>The prior distribution of <span class="math inline">\(p\)</span>, with density <span class="math inline">\(\pi(p)\)</span>, is Beta with shape parameters <span class="math inline">\(3.06\)</span> and <span class="math inline">\(2.56\)</span> <span class="math display">\[\begin{eqnarray*}
p \sim \textrm{Beta}(3.06, 2.56).
\end{eqnarray*}\]</span> The symbol “<span class="math inline">\(\sim\)</span>” is read “follows”, meaning that the random variable before the symbol follows the distribution after the symbol.</p>
<p>For the data/likelihood, we introduce proper notation. Let <span class="math inline">\(Y\)</span> be the random variable of the number of students say that their favorite day for eating out is Friday. We know that the sampling distribution for <span class="math inline">\(Y\)</span> is a Binomial distribution with number of trials <span class="math inline">\(20\)</span> and success probability <span class="math inline">\(p\)</span>. Using the notation of ``<span class="math inline">\(\sim\)</span>“, we have <span class="math display">\[\begin{eqnarray*}
Y \sim \textrm{Binomial}(20, p).
\end{eqnarray*}\]</span> After the value <span class="math inline">\(Y = y\)</span> is observed, <span class="math inline">\(L(p) = f(y \mid p)\)</span> denotes the likelihood, which is the probability of observing this sample value <span class="math inline">\(y\)</span> viewed as a function of the proportion <span class="math inline">\(p\)</span>. (Note that a small letter <span class="math inline">\(y\)</span> is used to denote the actual data observed, as opposed to the random variable <span class="math inline">\(Y\)</span>.) From the dining survey, we know that <span class="math inline">\(y = 12\)</span>.</p>
<p>Now we have the following prior density and the likelihood function.</p>
<ul>
<li><p>The prior distribution: <span class="math display">\[\begin{eqnarray*}
\pi(p) = \frac{1}{B(3.06, 2.56)}p^{3.06-1}(1-p)^{2.56-1}.
\end{eqnarray*}\]</span></p></li>
<li><p>The likelihood: <span class="math display">\[\begin{eqnarray*}
f(Y =12 \mid p) = L(p) = {20 \choose 12}p^{12}(1-p)^{8}.
\end{eqnarray*}\]</span></p></li>
</ul>
<p>By Bayes’ rule, the posterior density <span class="math inline">\(\pi(p \mid y)\)</span> is proportional to the product of the prior and the likelihood.<br>
<span class="math display">\[\begin{eqnarray*}
\pi(p \mid y) \propto \pi(p) \times L(p).
\end{eqnarray*}\]</span></p>
<p>Substituting the current prior and likelihood, one can perform the algebra for the posterior density. <span class="math display">\[\begin{eqnarray}
\pi(p \mid Y = 12) &amp;\propto&amp; \pi(p) \times f(Y = 12 \mid p) \nonumber \\
&amp;=&amp;  \frac{1}{B(3.06, 2.56)}p^{3.06-1}(1-p)^{2.56-1} \times \nonumber \\
&amp;&amp; {20 \choose 12}p^{12}(1-p)^{8} \nonumber \\
\texttt{[drop the constants]} &amp;\propto&amp; p^{12}(1-p)^{8}p^{3.06-1}(1-p)^{2.56-1} \nonumber \\
\texttt{[combine the powers]} &amp;=&amp; p^{15.06-1}(1-p)^{10.56-1}. \nonumber \\
\end{eqnarray}\]</span> One observes that the posterior density of <span class="math inline">\(p\)</span> given <span class="math inline">\(Y = 12\)</span> is, up to a proportionality constant, <span class="math display">\[\begin{eqnarray*}
\pi(p \mid Y = 12) \propto p^{15.06-1}(1-p)^{10.56-1}.
\end{eqnarray*}\]</span></p>
<p> Note that in the posterior derivation, the constants <span class="math inline">\({20 \choose 12}\)</span> and <span class="math inline">\(\frac{1}{B(3.06, 2.56)}\)</span> are dropped due to the proportional sign “<span class="math inline">\(\propto\)</span>”. That is, the expression of <span class="math inline">\(\pi(p \mid Y = 12)\)</span> is computed up to some constant. In this case, Appendix A demonstrates the calculation of the constant.</p>
<p>Next, one recognizes if the posterior distribution of <span class="math inline">\(p\)</span> is recognizable as a member of a familiar family of distributions. In the computation of the posterior, we have intentionally kept the expression of ``<span class="math inline">\(-1\)</span>” in the powers of <span class="math inline">\(p\)</span> and <span class="math inline">\(1-p\)</span> terms, instead of using <span class="math inline">\(14.06\)</span> and <span class="math inline">\(9.56\)</span> directly. By doing this, one recognizes that the posterior density has the familiar form <span class="math display">\[\begin{eqnarray*}
p^{a-1}(1-p)^{b-1}.
\end{eqnarray*}\]</span> As the reader might have guessed, the posterior distribution turns out to be a Beta distribution with updated shape parameters. That is, the posterior distribution of <span class="math inline">\(p\)</span> given <span class="math inline">\(Y = 12\)</span> is Beta with parameters 15.06 and 10.56.</p>
</section>
<section id="from-beta-prior-to-beta-posterior" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="from-beta-prior-to-beta-posterior"><span class="header-section-number">1.4.2</span> From Beta prior to Beta posterior</h3>
<p>The results about a proportion <span class="math inline">\(p\)</span> from the Bayes’ rule calculation performed in Section 7.4.1 can be generalized. Suppose one works with the following prior distribution and sampling density:</p>
<ul>
<li><p>The prior distribution: <span class="math display">\[\begin{eqnarray*}
p \sim \textrm{Beta}(a, b)
\end{eqnarray*}\]</span></p></li>
<li><p>The sampling density: <span class="math display">\[\begin{eqnarray*}
Y \sim \textrm{Binomial}(n, p)
\end{eqnarray*}\]</span></p></li>
</ul>
<p>One observes the count <span class="math inline">\(Y = y\)</span>, the number of successes in the collected data. Then the posterior distribution of <span class="math inline">\(p\)</span> is another Beta distribution with shape parameters <span class="math inline">\(a + y\)</span> and <span class="math inline">\(b + n - y\)</span>.</p>
<ul>
<li>The posterior distribution: <span class="math display">\[\begin{eqnarray}
p \mid Y = y \sim \textrm{Beta}(a + y, b + n - y)
(\#eq:betaposterior2)
\end{eqnarray}\]</span></li>
</ul>
<p>The two shape parameters of the Beta posterior distribution, <span class="math inline">\(a + y\)</span> and <span class="math inline">\(b + n - y\)</span>, are the sums of the prior and data/likelihood counts of successes and failures, respectively. We algebraically combine the shape parameters of the Beta prior and the Binomial likelihood to obtain the shape parameters of the posterior Beta distribution.</p>
<p>Table 7.1 demonstrates this process with three rows labelled Prior, Data/Likelihood and Posterior. The Prior row contains the shape parameters of the Beta prior <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the Successes and Failures columns, respectively. The Data/Likelihood row contains the number of successes <span class="math inline">\(y\)</span> and the number of failures <span class="math inline">\(n - y\)</span>. The shape parameters of the Beta posterior are found by adding the prior parameter values and the data values.</p>
<p>Table 7.1. Updating the Beta prior.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Successes</th>
<th style="text-align: center;">Failures</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Prior</td>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">(b)</td>
</tr>
<tr class="even">
<td style="text-align: center;">Data/Likelihood</td>
<td style="text-align: center;">(y)</td>
<td style="text-align: center;">(n-y)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">Posterior</td>
<td style="text-align: center;">(a + y)</td>
<td style="text-align: center;">(b + n - y)</td>
</tr>
</tbody>
</table>
<p>In the following R script we update the Beta shape parameters. We see that the owner’s posterior distribution for <span class="math inline">\(p\)</span> is Beta with shape parameters 15.06 and 10.56.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>ab <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">3.06</span>, <span class="fl">2.56</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>yny <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">12</span>, <span class="dv">8</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>(ab_new <span class="ot">&lt;-</span> ab <span class="sc">+</span> yny)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 15.06 10.56</code></pre>
</div>
</div>
<p>The function <code>beta_prior_post()</code> in the <code>ProbBayes</code> R package plots the prior and posterior Beta curves together on one graph, see Figure 7.9.</p>
<pre><code>beta_prior_post(ab, ab_new)</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/betapriorpost.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Prior and posterior curves for the proportion of students who prefer to dine out on Friday.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Comparing the two Beta curves, several observations can be made.</p>
<ul>
<li><p>One can compare the prior and posterior Beta curves using the respective means. The mean of a <span class="math inline">\(\textrm{Beta}(a, b)\)</span> distribution is <span class="math inline">\(\frac{a}{a+b}\)</span>. Using this formula, the posterior mean of <span class="math inline">\(p\)</span> is 15.06 / (15.06 + 10.56) = 0.588 which is slightly larger than the prior mean 3.06 / (30.6 + 2.56) = 0.544. Recall that the sample proportion from the survey results is <span class="math inline">\(12/20 = 0.6\)</span>. The posterior mean lies between the prior mean and sample mean and it is closer to the sample mean.</p></li>
<li><p>Next one compares the spreads of the two curves. One sees a much wider spread of the prior Beta curve (dashed line) than that of the posterior Beta curve (solid line). Initially the owner was unsure about the proportion of students favoring Friday to dine out. After observing the results of the survey, the solid posterior curve indicates that he is more certain that <span class="math inline">\(p\)</span> is between 0.5 and 0.7. This sheds light on a general feature of Bayesian inference: the data helps sharpen the belief about the parameter of interest, producing a posterior distribution with a smaller spread than the prior distribution.</p></li>
</ul>
<p>The attractive combination of a Beta prior and a Binomial sampling density to obtain a posterior motivates a definition of conjugate priors. If the prior distribution and the posterior distribution come from the same family of distributions, the prior is then called a conjugate prior. Here a Beta is a conjugate prior for a success probability <span class="math inline">\(p\)</span>, since the posterior distribution for <span class="math inline">\(p\)</span> is also in the Beta family. Conjugate priors are specific to the choice of sampling density. For example, a Beta prior is conjugate with Binomial sampling, but not to Normal sampling which is popular for continuous outcome. In Chapter 8 we will discover the conjugate prior distribution for a Normal sampling distribution.</p>
<p>Conjugate priors are desirable because they simplify the Bayesian inference procedure. In the dining preference example, when a <span class="math inline">\({\rm Beta}(3.06, 2.56)\)</span> prior is assigned to <span class="math inline">\(p\)</span>, the posterior is <span class="math inline">\({\rm Beta}(15.06, 10.56)\)</span> and inference about <span class="math inline">\(p\)</span> is made in a straightforward way. One can easily plot the he prior and posterior Beta distributions as in Figure 7.9. One can also make precise comparative statements about the locations of the prior and posterior distribution using quantiles of a Beta curve.</p>
<p>Although conjugate priors are convenient and straightforward to use, they may not be appropriate for use in a Bayesian analysis. One should choose a prior that fits one’s belief, not one that is convenient to use. In some situations it may be appropriate to choose a prior distribution that does not provide conjugacy. In Chapter 9, we will describe computational methods to facilitate posterior inferences when non-conjugate priors are used. Modern Bayesian posterior computations accommodate a wide variety of choices of prior and sampling distributions. Therefore it is more important to choose a prior that matches one’s prior belief than choosing a prior that is computationally convenient.</p>
</section>
</section>
<section id="bayesian-inferences-with-continuous-priors" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="bayesian-inferences-with-continuous-priors"><span class="header-section-number">1.5</span> Bayesian Inferences with Continuous Priors</h2>
<p>We will continue with the dining preference example to illustrate different types of Bayesian inference. The restaurant owner has taken his dining survey and the posterior distribution <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span> reflects his opinion about the proportion <span class="math inline">\(p\)</span> of students whose favorite day for eating out is Friday.</p>
<p>All Bayesian inferences about the proportion <span class="math inline">\(p\)</span> are based on various summaries of this posterior Beta distribution. The summary we compute from the posterior will depend on the type of inference. We will focus on three types of inference: (1) testing problems where one is interested in assessing the likelihood of some values of <span class="math inline">\(p\)</span>, (2) interval estimations where one wants to find an interval that is likely to contain <span class="math inline">\(p\)</span>, and (3) Bayesian prediction where one wants to learn about new observation(s) in the future.</p>
<p>Simulation will be incorporated for all three types of Bayesian inference problems. Since one has a conjugate prior distribution, one can derive the exact posterior distribution (a Beta) and inferences are performed with the exact posterior Beta distribution. In other situations when conjugacy is not available, meaning that no exact representation of the posterior is available, inferences through simulation are much more widely used. It is instructive to present the exact solutions and the approximated simulation-based solutions together, so one learns through practice and prepare for future use of simulation in other settings.</p>
<p>There is nothing magic about simulation. In fact, simulation has been used earlier, when the <code>rbeta()</code> function was used to generate simulated samples from <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> and <span class="math inline">\(\textrm{Beta}(2, 9)\)</span> and check the appropriateness of the chosen Beta prior (review Section 7.3.2} as needed). Information on simulation and the relevant R code will be introduced in the description of each inferential problem.</p>
<section id="bayesian-hypothesis-testing" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="bayesian-hypothesis-testing"><span class="header-section-number">1.5.1</span> Bayesian hypothesis testing</h3>
<p>Suppose one of the restaurant workers claims that at least 75% of the students prefer to eat out on Friday. Is this a reasonable claim?</p>
<p>In traditional classical statistics, one might be interested in testing the hypothesis <span class="math inline">\(H: p \ge 0.75\)</span>.<br>
From a Bayesian viewpoint, it is straightforward to implement this test. Since the hypothesis is an interval of values, one finds the posterior probability that <span class="math inline">\(p \ge 0.75\)</span> and makes a decision based on the value of this probability. If the probability is small, one rejects this claim.</p>
<p> First the exact solution will be presented. Since the posterior distribution is <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span>, the owner’s posterior density is graphed and the area under the curve for values of <span class="math inline">\(p\)</span> between 0.75 and 1 is found. The <code>beta_area()</code> function is used to display and show the area; see Figure 7.10. Since the probability is only about 4%, one rejects the worker’s claim that <span class="math inline">\(p\)</span> is at least 0.75.</p>
<pre><code>beta_area(lo = 0.75, hi = 1.0,
          shape_par = c(15.06, 10.56))</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/betapost1.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Probability of the hypothesis from the Beta posterior density.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>This computation can be implemented using simulation. Since the posterior distribution is <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span>, one generates a large number of random values from this Beta distribution, then summarizes the sample of simulated draws to obtain the probability of <span class="math inline">\(p \geq 0.75\)</span>. First a sample of <span class="math inline">\(S = 1000\)</span> from the Beta posterior is taken, storing the results in the vector <code>BetaSamples</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>BetaSamples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(S, <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The proportion of the 1000 simulated values of <span class="math inline">\(p\)</span> that are at least 0.75 gives an approximation of the probability that <span class="math inline">\(p \geq 0.75\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(BetaSamples <span class="sc">&gt;=</span> <span class="fl">0.75</span>) <span class="sc">/</span> S</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.036</code></pre>
</div>
</div>
<p>The simulation-based probability estimate is 0.037 which is an accurate approximation to the exact probability 0.04 obtained before.</p>
<p>It would be reasonable to question the choice of the number of simulations <span class="math inline">\(S = 1000\)</span>. One can change the simulation sample size to larger or smaller values as one sees fit. In general, the larger the value of <span class="math inline">\(S\)</span>, the more accurate the approximation. Figure 7.11 shows that the shape of a histogram of the simulated values of <span class="math inline">\(p\)</span> approaches the exact posterior density as the value of <span class="math inline">\(S\)</span> changes from 100 to 10,000. The corresponding simulation-based probabilities of <span class="math inline">\(p \geq 0.75\)</span> are <span class="math inline">\(\{0.02, 0.05, 0.033, 0.0422\}\)</span> indicating that the accuracy of the approximation improves for larger simulation sample sizes.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/4Svalues_new.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Histograms of simulated draws from Beta(15.06, 10.56) with the exact Beta density overlaid for four different number of samples drawn where <span class="math inline">\(S\)</span> = {10, 500, 1000, 10000}.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>One will observe variation from one simulation from another (see the two different but similar approximated probabilities 0.037 and 0.033 when <span class="math inline">\(S = 1000\)</span>). To replicate one’s results one specifies the seed of the random number simulator <code>set.seed()</code>. Choose any number that you like to put in – if this <code>set.seed()</code> line of code is executed first, then the same sequence of random values will be generated and one replicates the simulation-based computation.</p>
</section>
<section id="bayesian-credible-intervals" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="bayesian-credible-intervals"><span class="header-section-number">1.5.2</span> Bayesian credible intervals</h3>
<p>Another type of inference is a <em>Bayesian credible interval</em>, an interval that one is confident contains <span class="math inline">\(p\)</span>. Such an interval provides an uncertainty estimate for the parameter <span class="math inline">\(p\)</span>. A 90% Bayesian credible interval is an interval that contains 90% of the posterior probability.</p>
<p>One convenient 90% credible interval is the “equal tails” interval that contains the middle 90% of the probability content. The function <code>beta_interval()</code> in <code>ProbBayes</code> R package illustrates and computes the equal-tails interval. The shaded area in Figure 7.12 corresponds to <span class="math inline">\(90\%\)</span> of the posterior probability. The probability <span class="math inline">\(p\)</span> falls between 0.427 and 0.741 is exactly 90 percent.</p>
<pre><code>beta_interval(0.9, c(15.06, 10.56))</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/betapost2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Display of 90% probability interval for the proportion <span class="math inline">\(p\)</span>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>One obtains this middle 90% credible interval using the <code>qbeta()</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>), <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4266788 0.7410141</code></pre>
</div>
</div>
<p>This Bayesian credible interval differs from the interpretation of a traditional confidence interval. With a traditional confidence interval, one does not have confidence that one particular interval will contain <span class="math inline">\(p\)</span>. Instead 90% confidence refers to the average coverage of the interval in repeated sampling.</p>
<p>Other types of Bayesian credible intervals can be computed. For example, instead of a credible interval covering the middle 90% of the posterior probability, one could create a credible interval covers the lower 90%, or the upper 90%, or the middle 95%. The <code>qbeta()</code> function is helpful in achieving all of these different type of intervals, as long as we know the exact posterior distribution, that is, the two shape parameters of the posterior Beta distribution. For example, the following code computes a credible interval that covers the lower 90% of the posterior distribution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qbeta</span>(<span class="fu">c</span>(<span class="fl">0.00</span>, <span class="fl">0.90</span>), <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0000000 0.7099912</code></pre>
</div>
</div>
<p>An alternative way of creating credible intervals is by simulation. One first takes a random sample from the <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span> distribution, then summarizes the simulated values by finding the two cutoff points of the middle 90% of the sample. The <code>quantile()</code> function is useful for this purpose. As a demonstration, below we simulate <span class="math inline">\(S = 1000\)</span> proportion values and compute the credible interval.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>BetaSamples <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(S, <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="fu">quantile</span>(BetaSamples, <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       5%       95% 
0.4225222 0.7419642 </code></pre>
</div>
</div>
<p>The approximate middle 90% credible interval is [0.427, 0.733], which is close in value to the exact 90% credible interval [0.427, 0.741] computed using the <code>qbeta()</code> and <code>beta_interval()</code> functions. In an end-of-chapter exercise the reader is encouraged to practice and experiment with different values of the size of the simulated sample <span class="math inline">\(S\)</span>.</p>
</section>
<section id="bayesian-prediction" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="bayesian-prediction"><span class="header-section-number">1.5.3</span> Bayesian prediction</h3>
<p>Prediction is a typical task of Bayesian inference and statistical inference in general. Once we are able to make inference about the parameter in our statistical model, one may be interested in predicting future observations.</p>
<p>Denote a new observation by the random variable <span class="math inline">\(\tilde{Y}\)</span>. In particular, if the new survey is given to <span class="math inline">\(m\)</span> students, the random variable <span class="math inline">\(\tilde{Y}\)</span> is the number of students preferring Friday to dine out out of the <span class="math inline">\(m\)</span> respondents. If again the survey is given to a random sample, the random variable <span class="math inline">\(\tilde{Y}\)</span>, conditional on <span class="math inline">\(p\)</span>, follows a Binomial distribution with the fixed total number of trails <span class="math inline">\(m\)</span> and success probability <span class="math inline">\(p\)</span>. One’s knowledge about the location of <span class="math inline">\(p\)</span> is expressed by the posterior distribution of <span class="math inline">\(p\)</span>.</p>
<p>Mathematically, to make a prediction of a new observation, one is asking for the distribution of <span class="math inline">\(\tilde{Y}\)</span> given the observed data <span class="math inline">\(Y = y\)</span>. That is, one is interested in the probability function <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid Y = y)\)</span> where <span class="math inline">\(\tilde y\)</span> is a value of <span class="math inline">\(\tilde{Y}\)</span>. But the conditional distribution of <span class="math inline">\(\tilde{Y}\)</span> given a value of the proportion <span class="math inline">\(p\)</span> is Binomial(<span class="math inline">\(m, p\)</span>) and the current beliefs about <span class="math inline">\(p\)</span> are described by the posterior density. So one writes the joint density of <span class="math inline">\(\tilde{Y}\)</span> and <span class="math inline">\(p\)</span> as the product <span class="math display">\[\begin{eqnarray}
f(\tilde{Y}= \tilde{y},  p \mid Y = y) = f(\tilde{Y} = \tilde{y} \mid p) \pi(p \mid Y = y).
\end{eqnarray}\]</span> By integrating out <span class="math inline">\(p\)</span>, one obtains the predictive distribution <span class="math display">\[\begin{eqnarray}
f(\tilde{Y} = \tilde{y} \mid Y = y) = \int  f(\tilde{Y} =\tilde{y} \mid p) \pi(p \mid Y = y) dp.
\label{eq:Binomial:pred}
\end{eqnarray}\]</span></p>
<p>The density of <span class="math inline">\(\tilde{Y}\)</span> given <span class="math inline">\(p\)</span> is Binomial with <span class="math inline">\(m\)</span> trials and success probability <span class="math inline">\(p\)</span>, and the posterior density of <span class="math inline">\(p\)</span> is <span class="math inline">\({\rm Beta}(a + y, b + n - y)\)</span>. After the substitution of densities and an integration step (see Appendix B for the detail), one finds that the predictive density is given by <span class="math display">\[\begin{eqnarray}
f(\tilde{Y} =\tilde{y}  \mid Y = y) &amp;=&amp; {m \choose \tilde{y}}
\frac{B(a + y + \tilde{y}, b  + n - y + m - \tilde{y})}{B(a + y, b + n - y)}. \nonumber \\
\end{eqnarray}\]</span> This is the Beta-Binomial distribution with parameters <span class="math inline">\(m\)</span>, <span class="math inline">\(a + y\)</span> and <span class="math inline">\(b + n - y\)</span>. <span class="math display">\[\begin{eqnarray}
\tilde{Y} \mid Y = y \sim \textrm{Beta-Binomial}(m, a + y, b + n - y).
\end{eqnarray}\]</span> To summarize, Bayesian prediction of a new observation is a Beta-Binomial distribution where <span class="math inline">\(m\)</span> is the number of trials in the new sample, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are shape parameters from the Beta prior, and <span class="math inline">\(y\)</span> and <span class="math inline">\(n\)</span> are quantities from the data/likelihood.</p>
<p>Using this Beta-Binomial distribution in our example, one computes the predictive probability that <span class="math inline">\(\tilde y\)</span> students prefer Friday in a new survey of 20 students. We illustrate the use of the <code>pbetap()</code> function from the <code>ProbBayes</code> package. The inputs to <code>pbetap()</code> are the vector of Beta shape parameters <span class="math inline">\((a, b)\)</span>, the sample size 20, and the values of <span class="math inline">\(\tilde{y}\)</span> of interest.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">15.06</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fl">10.56</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">&lt;-</span> <span class="fu">pbetap</span>(<span class="fu">c</span>(a, b), <span class="dv">20</span>, <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>pred_distribution <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Y =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, </span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>                                <span class="at">Probability =</span> prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>prob_plot(pred_distribution,
          Color = crcblue, Size = 4) +
  theme(text=element_text(size=18))</code></pre>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/Predictive_dining.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Display of the exact predictive distribution of the number of students <span class="math inline">\(\tilde y\)</span> favoring Friday in a future sample of 20.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>These predictive probabilities are displayed in Table 7.2 <span class="math inline">\(\ref{tab:predictive_dining}\)</span> and graphed in Figure 7.13.</p>
<p>Table 7.2. Predictive distribution of the number of students preferring Friday in a future sample of 20.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: right;">Y</th>
<th style="text-align: right;">Probability</th>
<th style="text-align: right;">Y</th>
<th style="text-align: right;">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">0.127</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">12</td>
<td style="text-align: right;">0.134</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">13</td>
<td style="text-align: right;">0.127</td>
</tr>
<tr class="even">
<td style="text-align: right;">3</td>
<td style="text-align: right;">0.001</td>
<td style="text-align: right;">14</td>
<td style="text-align: right;">0.108</td>
</tr>
<tr class="odd">
<td style="text-align: right;">4</td>
<td style="text-align: right;">0.004</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">0.080</td>
</tr>
<tr class="even">
<td style="text-align: right;">5</td>
<td style="text-align: right;">0.010</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">0.052</td>
</tr>
<tr class="odd">
<td style="text-align: right;">6</td>
<td style="text-align: right;">0.021</td>
<td style="text-align: right;">17</td>
<td style="text-align: right;">0.028</td>
</tr>
<tr class="even">
<td style="text-align: right;">7</td>
<td style="text-align: right;">0.037</td>
<td style="text-align: right;">18</td>
<td style="text-align: right;">0.012</td>
</tr>
<tr class="odd">
<td style="text-align: right;">8</td>
<td style="text-align: right;">0.059</td>
<td style="text-align: right;">19</td>
<td style="text-align: right;">0.004</td>
</tr>
<tr class="even">
<td style="text-align: right;">9</td>
<td style="text-align: right;">0.085</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">0.001</td>
</tr>
<tr class="odd">
<td style="text-align: right;">10</td>
<td style="text-align: right;">0.109</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Looking at the table, the most likely number of students preferring Friday is 12. Just as in the inference situation, it is desirable to construct an interval that will contain <span class="math inline">\(\tilde Y\)</span> with a high probability. Suppose the desired probability content is 0.90. One constructs this prediction interval by putting in the most likely values of <span class="math inline">\(\tilde Y\)</span> until the probability content of the set exceeds 0.90.</p>
<p>This method is implemented using the following command:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="fu">discint</span>(pred_distribution, .<span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$prob
[1] 0.9185699

$set
 [1]  7  8  9 10 11 12 13 14 15 16</code></pre>
</div>
</div>
<p>One therefore finds that <span class="math display">\[
Prob(7 \le \tilde Y \le 16) = 0.919.
\]</span></p>
<p>This exact predictive distribution is based on the posterior distribution of <span class="math inline">\(p\)</span>, as one uses <span class="math inline">\(\pi(p \mid Y=y)\)</span> in the integration process in Equation (7.14). For that reason this predictive distribution is called the <em>posterior predictive distribution</em>. There also exists a <em>prior predictive distribution</em>, a topic we will briefly introduce in Section 7.6.</p>
<p>In situations where it is difficult to derive the exact predictive distribution, one simulates values from this distribution. One implements this predictive simulation by first simulating draws of the parameter (in this case the proportion <span class="math inline">\(p\)</span>) from its posterior distribution, and then simulating values of the future observation (e.g.&nbsp;the new observation <span class="math inline">\(\tilde{Y}\)</span>) from the sampling density (here the Binomial distribution).</p>
<p>We illustrate this simulation procedure with the generic Beta posterior <span class="math inline">\(\textrm{Beta}(a + y, b + n - y)\)</span>. To simulate a single draw from the predictive distribution, one first simulates a single proportion value <span class="math inline">\(p\)</span> from the Beta posterior and then simulates a new data point <span class="math inline">\(\tilde{y}\)</span> (the number of successes out of <span class="math inline">\(m\)</span> trials) from a Binomial distribution with sample size <span class="math inline">\(m\)</span> and probability of success given by the simulated draw of <span class="math inline">\(p\)</span>. <span class="math display">\[\begin{eqnarray*}
\text{sample}\,\, p \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{Y} \sim {\rm{Binomial}}(m, p)
\end{eqnarray*}\]</span></p>
<p> This process of simulating a single draw is implemented by the <code>rbeta()</code> and <code>rbinom()</code> functions. Let <span class="math inline">\(m = n\)</span> (the size of the future sample is the same as the size of the observed sample).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">3.06</span>; b <span class="ot">&lt;-</span> <span class="fl">2.56</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span>; y <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>pred_p_sim <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(<span class="dv">1</span>, a <span class="sc">+</span> y, b <span class="sc">+</span> n <span class="sc">-</span> y)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>(pred_y_sim <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, n, pred_p_sim))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 14</code></pre>
</div>
</div>
<p>Due to the ability of R to work easily with vectors, ]the same code is essentially used for simulating <span class="math inline">\(S = 1000\)</span> draws from the predictive distribution.<br>
In the following R script, <code>pred_p_sim</code> contains 1000 simulated draws from the posterior, and for each element of this posterior sample, the <code>rbinom()</code> function is used to simulate a corresponding value of <span class="math inline">\(\tilde Y\)</span> from the Binomial sampling density.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">3.06</span>; b <span class="ot">&lt;-</span> <span class="fl">2.56</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span>; y <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>pred_p_sim <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(S, a <span class="sc">+</span> y, b <span class="sc">+</span> n <span class="sc">-</span> y)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>pred_y_sim <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(S, n, pred_p_sim)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Figure 7.14 displays predictive probabilities for the number of students who prefer Fridays using the exact Beta-Binomial and simulation methods. One observes good agreement using these two computation methods.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/two_predictive_dist.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Display of the exact and simulated predictive probabilities for dining example.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For example, using the simulated values of <span class="math inline">\(\tilde Y\)</span> one finds that <span class="math display">\[
Prob(6 \le \tilde Y \le 15) = 0.927
\]</span> which is close in value to the range <span class="math inline">\(Prob(7 \le \tilde Y \le 16) = 0.919\)</span> found using the exact predictive distribution.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">15.06</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fl">10.56</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>prob <span class="ot">&lt;-</span> <span class="fu">pbetap</span>(<span class="fu">c</span>(a, b), <span class="dv">20</span>, <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>pred_distribution <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Y =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">20</span>, </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>                                <span class="at">Probability =</span> prob)</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a><span class="fu">discint</span>(pred_distribution, .<span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$prob
[1] 0.9185699

$set
 [1]  7  8  9 10 11 12 13 14 15 16</code></pre>
</div>
</div>
</section>
</section>
<section id="predictive-checking" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="predictive-checking"><span class="header-section-number">1.6</span> Predictive Checking</h2>
<p>In the previous section, the use of the predictive distribution has been illustrated in learning about future data. This is more precisely described as the posterior predictive density as one is obtaining this density by integrating the sampling density <span class="math inline">\(f(\tilde Y = \tilde y \mid p)\)</span> over the posterior density <span class="math inline">\(\pi(p \mid y)\)</span>.</p>
<p>The prior predictive density is also useful in model checking. In a Bayesian model where <span class="math inline">\(p\)</span> has a prior <span class="math inline">\(\pi(p)\)</span> and <span class="math inline">\(Y\)</span> has a sampling density <span class="math inline">\(f(Y = y \mid p)\)</span>, one writes the joint density of <span class="math inline">\((p, Y)\)</span> as the product of the sampling density and the prior: <span class="math display">\[\begin{eqnarray}
f(p, Y = y) = f(Y = y \mid p) \pi(p).
\end{eqnarray}\]</span> Suppose one conditions on <span class="math inline">\(y\)</span> instead of <span class="math inline">\(p\)</span> and then one obtains an alternative representation of the joint density: <span class="math display">\[\begin{eqnarray}
f(p, Y = y) = \pi(p \mid Y = y) f(Y = y).
\end{eqnarray}\]</span> The first term in this product, the density <span class="math inline">\(\pi(p \mid Y = y)\)</span>, is the posterior density of <span class="math inline">\(p\)</span> given the observation <span class="math inline">\(y\)</span>; this density is useful for performing inference about the proportion. The second term in this product is the density <span class="math inline">\(f(Y = y)\)</span> is the prior predictive density – this represents the density of future data before the observation <span class="math inline">\(y\)</span> is taken. If the actual observation denoted by <span class="math inline">\(y_{obs}\)</span> is not consistent with the prior predictive density <span class="math inline">\(f(Y = y)\)</span>, this indicates some problem with the Bayesian model. Basically, this says that the observed data is unlikely to happen if one simulates predictions of data from our model.</p>
<p>To illustrate the use of prior predictive checking, recall that the restaurant owner assigned a Beta(3.06, 2.56) prior to the proportion <span class="math inline">\(p\)</span> of students dining on Friday. A sample of 20 students will be taken. Based on this information, one computes the predictive probability <span class="math inline">\(f(Y = y)\)</span> of <span class="math inline">\(y\)</span> students preferring Friday dining of the sample of 20. This predictive distribution for all possible values of <span class="math inline">\(y\)</span> is displayed in Figure 7.15. Recall that we actually observed <span class="math inline">\(y_{obs} = 12\)</span> Friday diners — this value is shown in Figure 7.15 as a large black dot. This value is in the middle of the distribution – the takeaway is that the observed data is consistent with predictions from the owner’s Bayesian model.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/predcheck1.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Prior predictive distribution of <span class="math inline">\(y\)</span> using the owner’s Beta prior. The observed number of <span class="math inline">\(y\)</span> id indicated with a large black dot. In this case the observed data is consistent with the Bayesian model.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In contrast, suppose another restaurant worker is more pessimistic about the likelihood of students dining on Friday. This worker’s prior median of the proportion <span class="math inline">\(p\)</span> is 0.2 and her 90th percentile is 0.4 — this information is matched with a Beta prior with shape parameters 2.07 and 7.32. Figure 7.16 displays the predictive density of the number of Friday diners of a sample of 20 using this worker’s prior. Here one reaches a different conclusion. The observed number 12 of Friday diners is in the tail of this predictive distribution — this observation is not consistent with predictions from the Bayesian model. In closer examination, one sees conflict between the information in the worker’s prior and the data — her prior said that the proportion <span class="math inline">\(p\)</span> was close to 0.20 and the data result (12 out of 20 successes) indicates that the proportion is close to 0.60. Predictive checking is helpful in this case in detecting this prior/data conflict.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/chapter7/predcheck2.png" class="img-fluid figure-img" width="500"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Prior predictive distribution of <span class="math inline">\(y\)</span> using a worker’s Beta prior. The observed number of <span class="math inline">\(y\)</span> is indicated by a large black dot. In this case the observed data is not consistent with the Bayesian model.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<section id="comparing-bayesian-models" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="comparing-bayesian-models"><span class="header-section-number">1.6.1</span> Comparing Bayesian models</h3>
<p>The prior predictive distribution is also useful in comparing two Bayesian models. To illustrate model comparison, suppose a second worker at the restaurant is also asked about the fraction of students who dine on Friday. He knows that the owner’s belief about the proportion <span class="math inline">\(p\)</span> is described by a Beta(3.06, 2.56) density, and the fellow worker’s belief about <span class="math inline">\(p\)</span> is represented by a Beta(2.07, 7.32) density. Who should the second worker believe?</p>
<p>Suppose this second worker believes that both the owner’s and fellow worker’s beliefs about the proportion <span class="math inline">\(p\)</span> are equally plausible. So he places a probability of 0.5 on the Beta(3.06, 2.56) prior and a probability of 0.5 on the Beta(2.07, 7.32) prior. This second worker’s prior <span class="math inline">\(\pi(p)\)</span> is written as the mixture <span class="math display">\[\begin{eqnarray}
\pi(p) = q \pi_1(p) + (1 - q) \pi_2(p),
\end{eqnarray}\]</span> where <span class="math inline">\(q = 0.5\)</span> and <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> denote the owner’s and worker’s Beta priors.</p>
<p>Now one observes the survey data – <span class="math inline">\(y\)</span> Fridays in a sample of size <span class="math inline">\(n\)</span>. Using the usual prior times likelihood procedure, the posterior density of <span class="math inline">\(p\)</span> is proportional to the product <span class="math display">\[\begin{eqnarray}
\pi(p \mid Y = y) \propto \Big[q \pi_1(p) + (1 - q) \pi_2(p)\Big] \times
{n \choose y} p ^ {y }(1 - p) ^ {n - y}.
\end{eqnarray}\]</span> After some manipulation, one can show that the posterior density for the proportion <span class="math inline">\(p\)</span> has the mixture form <span class="math display">\[\begin{eqnarray}
\pi(p \mid Y = y) = q(y) \pi_1(p \mid Y = y) + (1 - q(y)) \pi_2(p \mid Y = y).
\end{eqnarray}\]</span></p>
<p>The posterior densities <span class="math inline">\(\pi_1(p \mid y)\)</span> and <span class="math inline">\(\pi_2(p \mid y)\)</span> are the familiar Beta forms. For example, <span class="math inline">\(\pi_1(p \mid Y = y)\)</span> will be the Beta(3.06 + <span class="math inline">\(y\)</span>, 2.56 + <span class="math inline">\(n - y\)</span>) posterior density combining the Beta(3.06, 2.56) prior and the sample data of <span class="math inline">\(y\)</span> successes in a sample of size <span class="math inline">\(n\)</span>. Likewise, <span class="math inline">\(\pi_2(p \mid Y = y)\)</span> will be the Beta density combining the worker’s Beta(2.07, 7.32) prior and the data.</p>
<p>The quantity <span class="math inline">\(q(y)\)</span> represents the posterior probability of the owner’s prior. One expresses this probability as <span class="math display">\[\begin{eqnarray}
q(y) = \frac{q f_1(Y = y)}{q f_1(Y = y) + (1 - q) f_2(Y =y)}
\end{eqnarray}\]</span> where <span class="math inline">\(f_1(Y = y)\)</span> and <span class="math inline">\(f_2(Y = y)\)</span> denote the predictive densities corresponding to the owner’s and worker’s priors. With a little algebra, one represents the posterior odds of the model probabilities as follows.</p>
<p><span class="math display">\[\begin{eqnarray}
\frac{P(Prior \, 1 \mid Y = y)}{P(Prior \, 2 \mid Y = y)} = \frac{q(y)}{1 - q(y)} = \left[\frac{q}{1 - q}\right] \left[\frac{f_1(Y = y)}{f_2(Y = y)}\right]
\end{eqnarray}\]</span></p>
<p>The posterior odds of the owner’s prior <span class="math inline">\(P(Prior \, 1 \mid Y = y) / P(Prior \, 2 \mid y = y)\)</span> is written as the product of two terms.</p>
<ul>
<li><p>The ratio <span class="math inline">\(q / (1 - q)\)</span> represents the prior odds of the owner’s prior.</p></li>
<li><p>The term <span class="math inline">\(f_1(Y = y) / f_2(Y = y)\)</span>, the ratio of the predictive densities, is called the Bayes factor. It reflects the relative abilities of the two priors to predict the observation <span class="math inline">\(y\)</span>.</p></li>
</ul>
<p>The function <code>binomial.beta.mix()</code> is used to find the Bayes factor for our example. One inputs the prior probabilities of the two models (priors), and the vectors of Beta shape parameters that define the owner’s prior and the worker’s prior. The displayed output is the posterior odds value of 6.77.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>beta_par1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">3.06</span>, <span class="fl">2.56</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>beta_par2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.07</span>, <span class="fl">7.32</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>beta_par <span class="ot">&lt;-</span> <span class="fu">rbind</span>(beta_par1, beta_par2)</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>output <span class="ot">&lt;-</span> <span class="fu">binomial.beta.mix</span>(probs, beta_par, <span class="fu">c</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>(posterior_odds <span class="ot">&lt;-</span> output<span class="sc">$</span>probs[<span class="dv">1</span>] <span class="sc">/</span> output<span class="sc">$</span>probs[<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>beta_par1 
 6.777823 </code></pre>
</div>
</div>
<p>Since the two priors are given equal probabilities, the prior odds <span class="math inline">\(q / (1 - q)\)</span> is equal to one. In this case the posterior odds is equal to the Bayes factor. The interpretation is that for the given observation (12 successes in 20 trials), there is 6.77 times more support for the owner’s prior than for the worker’s prior. This conclusion is consistent with the earlier work that showed that the observed value of <span class="math inline">\(y\)</span> was inconsistent with the Bayesian model for the worker’s prior.</p>
</section>
<section id="posterior-predictive-checking" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="posterior-predictive-checking"><span class="header-section-number">1.6.2</span> Posterior predictive checking</h3>
<p>Although the prior predictive distribution is useful in model checking, it has some disadvantages. One problem is that the distribution <span class="math inline">\(f(Y = y)\)</span> may not exist in the situation where the prior <span class="math inline">\(\pi()\)</span> is not a proper probability distribution. We will see particular situations in future chapters where a vague or imprecise probability distribution is assigned as our prior and then the prior predictive distribution will not be well-defined. A related issue is that a prior may be assigned that may not accurately reflect one’s prior beliefs about a parameter. Small errors in the specification of the prior will result in errors in the prior predictive distribution. So there needs to be some caution in the use of the prior predictive distribution in assessing the goodness of the Bayesian model.</p>
<p>An alternative method of checking the suitability of a Bayesian model is based on the posterior predictive distribution. In this setting, one computes the posterior predictive distribution of a replicated dataset, that is a dataset of the same sample size as our observed sample. One sees if the observed value of <span class="math inline">\(y\)</span> is in the middle of this predictive distribution. If this is true, then this means that the observed sample is consistent with predictions of replicated data. On the other hand, if the observed <span class="math inline">\(y\)</span> is in the tails of the posterior distribution, this indicates some model misspecification which means that there is possibility some issue with the specified prior or sampling density.</p>
<p>One attractive aspect of the posterior prediction distribution is that replicated datasets is conveniently simulated. To simulate one replicated dataset, we first simulate a parameter from its posterior distribution, then simulate new data from the data model given the simulated parameter value. In the Beta-Binomial situation, the posterior of the proportion <span class="math inline">\(p\)</span> is <span class="math inline">\({{\rm{Beta}}}(a + y, b + n - y)\)</span>.<br>
To simulate a new data point <span class="math inline">\(\tilde{Y} = \tilde{y}\)</span>, one first simulates a proportion value <span class="math inline">\(p^{(1)}\)</span> from the Beta posterior and then simulate a new data point <span class="math inline">\(\tilde{y}^{(1)}\)</span> from a Binomial distribution with sample size <span class="math inline">\(n\)</span> and probability of success <span class="math inline">\(p^{(1)}\)</span>. If we wish to obtain a sample of size <span class="math inline">\(S\)</span> from the posterior predictive distribution, this process is repeated <span class="math inline">\(S\)</span> times as showed in the following diagram. <span class="math display">\[\begin{eqnarray*}
\text{sample}\,\, p^{(1)} \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(1)} \sim {\rm{Binomial}}(n, p^{(1)})\\
\text{sample}\,\, p^{(2)} \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(2)} \sim {\rm{Binomial}}(n, p^{(2)})\\
&amp;\vdots&amp; \\
\text{sample}\,\, p^{(S)} \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(S)} \sim {\rm{Binomial}}(n, p^{(S)})
\end{eqnarray*}\]</span> The sample <span class="math inline">\(\tilde{y}^{(1)}, ..., \tilde{y}^{(S)}\)</span> is an approximation to the posterior predictive distribution that is used for model checking. In practice, one constructs a histogram of this sample and decides if the observed value of <span class="math inline">\(y\)</span> is in the central portion of this predictive distribution. The reader will be given an opportunity to use this algorithm to see if the observed data is consistent with simulations of replicated data from this predictive distribution.</p>


</section>
</section>

</main> <!-- /main -->
<script type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Preface</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./mean.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>