<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Learning About a Binomial Probability | Probability and Bayesian Modeling</title>
  <meta name="description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Learning About a Binomial Probability | Probability and Bayesian Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Learning About a Binomial Probability | Probability and Bayesian Modeling" />
  
  <meta name="twitter:description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  

<meta name="author" content="Jim Albert and Jingchen Hu" />


<meta name="date" content="2020-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="joint-probability-distributions.html"/>
<link rel="next" href="mean.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



   <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Probability and Bayesian Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html"><i class="fa fa-check"></i><b>1</b> Probability: A Measurement of Uncertainty</a><ul>
<li class="chapter" data-level="1.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-classical-view-of-a-probability"><i class="fa fa-check"></i><b>1.2</b> The Classical View of a Probability</a></li>
<li class="chapter" data-level="1.3" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-frequency-view-of-a-probability"><i class="fa fa-check"></i><b>1.3</b> The Frequency View of a Probability</a></li>
<li class="chapter" data-level="1.4" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-subjective-view-of-a-probability"><i class="fa fa-check"></i><b>1.4</b> The Subjective View of a Probability</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#measuring-probabilities-subjectively"><i class="fa fa-check"></i><b>1.4.1</b> Measuring probabilities subjectively</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-sample-space"><i class="fa fa-check"></i><b>1.5</b> The Sample Space</a><ul>
<li class="chapter" data-level="1.5.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#roll-two-fair-indistinguishable-dice"><i class="fa fa-check"></i><b>1.5.1</b> Roll two fair, indistinguishable dice</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#assigning-probabilities"><i class="fa fa-check"></i><b>1.6</b> Assigning Probabilities</a><ul>
<li class="chapter" data-level="1.6.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#events-and-event-operations"><i class="fa fa-check"></i><b>1.6.1</b> Events and Event Operations</a></li>
<li class="chapter" data-level="1.6.2" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-three-probability-axioms"><i class="fa fa-check"></i><b>1.6.2</b> The Three Probability Axioms</a></li>
<li class="chapter" data-level="1.6.3" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-complement-and-addition-properties"><i class="fa fa-check"></i><b>1.6.3</b> The Complement and Addition Properties</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#exercises"><i class="fa fa-check"></i><b>1.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="counting-methods.html"><a href="counting-methods.html"><i class="fa fa-check"></i><b>2</b> Counting Methods</a><ul>
<li class="chapter" data-level="2.1" data-path="counting-methods.html"><a href="counting-methods.html#introduction-rolling-dice-yahtzee-and-roulette"><i class="fa fa-check"></i><b>2.1</b> Introduction: Rolling Dice, Yahtzee, and Roulette</a></li>
<li class="chapter" data-level="2.2" data-path="counting-methods.html"><a href="counting-methods.html#equally-likely-outcomes"><i class="fa fa-check"></i><b>2.2</b> Equally Likely Outcomes</a></li>
<li class="chapter" data-level="2.3" data-path="counting-methods.html"><a href="counting-methods.html#the-multiplication-counting-rule"><i class="fa fa-check"></i><b>2.3</b> The Multiplication Counting Rule</a></li>
<li class="chapter" data-level="2.4" data-path="counting-methods.html"><a href="counting-methods.html#permutations"><i class="fa fa-check"></i><b>2.4</b> Permutations</a></li>
<li class="chapter" data-level="2.5" data-path="counting-methods.html"><a href="counting-methods.html#combinations"><i class="fa fa-check"></i><b>2.5</b> Combinations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="counting-methods.html"><a href="counting-methods.html#number-of-subsets"><i class="fa fa-check"></i><b>2.5.1</b> Number of subsets</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="counting-methods.html"><a href="counting-methods.html#arrangements-of-non-distinct-objects"><i class="fa fa-check"></i><b>2.6</b> Arrangements of Non-Distinct Objects</a></li>
<li class="chapter" data-level="2.7" data-path="counting-methods.html"><a href="counting-methods.html#playing-yahtzee"><i class="fa fa-check"></i><b>2.7</b> Playing Yahtzee</a></li>
<li class="chapter" data-level="2.8" data-path="counting-methods.html"><a href="counting-methods.html#exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>3</b> Conditional Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="conditional-probability.html"><a href="conditional-probability.html#introduction-the-three-card-problem"><i class="fa fa-check"></i><b>3.1</b> Introduction: The Three Card Problem</a></li>
<li class="chapter" data-level="3.2" data-path="conditional-probability.html"><a href="conditional-probability.html#independent-events"><i class="fa fa-check"></i><b>3.2</b> Independent Events</a></li>
<li class="chapter" data-level="3.3" data-path="conditional-probability.html"><a href="conditional-probability.html#in-everyday-life"><i class="fa fa-check"></i><b>3.3</b> In Everyday Life</a></li>
<li class="chapter" data-level="3.4" data-path="conditional-probability.html"><a href="conditional-probability.html#in-a-two-way-table"><i class="fa fa-check"></i><b>3.4</b> In a Two-Way Table</a></li>
<li class="chapter" data-level="3.5" data-path="conditional-probability.html"><a href="conditional-probability.html#definition-and-the-multiplication-rule"><i class="fa fa-check"></i><b>3.5</b> Definition and the Multiplication Rule</a></li>
<li class="chapter" data-level="3.6" data-path="conditional-probability.html"><a href="conditional-probability.html#the-multiplication-rule"><i class="fa fa-check"></i><b>3.6</b> The Multiplication Rule</a></li>
<li class="chapter" data-level="3.7" data-path="conditional-probability.html"><a href="conditional-probability.html#the-multiplication-rule-under-independence"><i class="fa fa-check"></i><b>3.7</b> The Multiplication Rule Under Independence</a></li>
<li class="chapter" data-level="3.8" data-path="conditional-probability.html"><a href="conditional-probability.html#learning-using-bayes-rule"><i class="fa fa-check"></i><b>3.8</b> Learning Using Bayes’ Rule</a></li>
<li class="chapter" data-level="3.9" data-path="conditional-probability.html"><a href="conditional-probability.html#r-example-learning-about-a-spinner"><i class="fa fa-check"></i><b>3.9</b> R Example: Learning About a Spinner</a></li>
<li class="chapter" data-level="3.10" data-path="conditional-probability.html"><a href="conditional-probability.html#exercises"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discrete-distributions.html"><a href="discrete-distributions.html"><i class="fa fa-check"></i><b>4</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="4.1" data-path="discrete-distributions.html"><a href="discrete-distributions.html#introduction-the-hat-check-problem"><i class="fa fa-check"></i><b>4.1</b> Introduction: The Hat Check Problem</a></li>
<li class="chapter" data-level="4.2" data-path="discrete-distributions.html"><a href="discrete-distributions.html#random-variable-and-probability-distribution"><i class="fa fa-check"></i><b>4.2</b> Random Variable and Probability Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="discrete-distributions.html"><a href="discrete-distributions.html#probability-distribution"><i class="fa fa-check"></i><b>4.3</b> Probability distribution</a></li>
<li class="chapter" data-level="4.4" data-path="discrete-distributions.html"><a href="discrete-distributions.html#summarizing-a-probability-distribution"><i class="fa fa-check"></i><b>4.4</b> Summarizing a Probability Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="discrete-distributions.html"><a href="discrete-distributions.html#standard-deviation-of-a-probability-distribution"><i class="fa fa-check"></i><b>4.5</b> Standard Deviation of a Probability Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="discrete-distributions.html"><a href="discrete-distributions.html#coin-tossing-distributions"><i class="fa fa-check"></i><b>4.6</b> Coin-Tossing Distributions</a></li>
<li class="chapter" data-level="4.7" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Binomial probabilities</a></li>
<li class="chapter" data-level="4.8" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-experiments"><i class="fa fa-check"></i><b>4.8</b> Binomial experiments</a></li>
<li class="chapter" data-level="4.9" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-computations"><i class="fa fa-check"></i><b>4.9</b> Binomial computations</a></li>
<li class="chapter" data-level="4.10" data-path="discrete-distributions.html"><a href="discrete-distributions.html#mean-and-standard-deviation-of-a-binomial"><i class="fa fa-check"></i><b>4.10</b> Mean and standard deviation of a Binomial</a></li>
<li class="chapter" data-level="4.11" data-path="discrete-distributions.html"><a href="discrete-distributions.html#negative-binomial-experiments"><i class="fa fa-check"></i><b>4.11</b> Negative Binomial Experiments</a></li>
<li class="chapter" data-level="4.12" data-path="discrete-distributions.html"><a href="discrete-distributions.html#exercises"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="continuous-distributions.html"><a href="continuous-distributions.html"><i class="fa fa-check"></i><b>5</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="continuous-distributions.html"><a href="continuous-distributions.html#introduction-a-baseball-spinner-game"><i class="fa fa-check"></i><b>5.1</b> Introduction: A Baseball Spinner Game</a></li>
<li class="chapter" data-level="5.2" data-path="continuous-distributions.html"><a href="continuous-distributions.html#the-uniform-distribution"><i class="fa fa-check"></i><b>5.2</b> The Uniform Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="continuous-distributions.html"><a href="continuous-distributions.html#binomial-probabilities-and-the-normal-curve"><i class="fa fa-check"></i><b>5.3</b> Binomial Probabilities and the Normal Curve</a></li>
<li class="chapter" data-level="5.4" data-path="continuous-distributions.html"><a href="continuous-distributions.html#sampling-distribution-of-the-mean"><i class="fa fa-check"></i><b>5.4</b> Sampling Distribution of the Mean</a></li>
<li class="chapter" data-level="5.5" data-path="continuous-distributions.html"><a href="continuous-distributions.html#exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html"><i class="fa fa-check"></i><b>6</b> Joint Probability Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#joint-probability-mass-function-sampling-from-a-box"><i class="fa fa-check"></i><b>6.1</b> Joint Probability Mass Function: Sampling From a Box</a></li>
<li class="chapter" data-level="6.2" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#multinomial-experiments"><i class="fa fa-check"></i><b>6.2</b> Multinomial Experiments</a></li>
<li class="chapter" data-level="6.3" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#joint-density-functions"><i class="fa fa-check"></i><b>6.3</b> Joint Density Functions</a></li>
<li class="chapter" data-level="6.4" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#independence-and-measuring-association"><i class="fa fa-check"></i><b>6.4</b> Independence and Measuring Association</a></li>
<li class="chapter" data-level="6.5" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#flipping-a-random-coin-the-beta-binomial-distribution"><i class="fa fa-check"></i><b>6.5</b> Flipping a Random Coin: The Beta-Binomial Distribution</a></li>
<li class="chapter" data-level="6.6" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.7" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#exercises"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="proportion.html"><a href="proportion.html"><i class="fa fa-check"></i><b>7</b> Learning About a Binomial Probability</a><ul>
<li class="chapter" data-level="7.1" data-path="proportion.html"><a href="proportion.html#introduction-thinking-about-a-proportion-subjectively"><i class="fa fa-check"></i><b>7.1</b> Introduction: Thinking About a Proportion Subjectively</a></li>
<li class="chapter" data-level="7.2" data-path="proportion.html"><a href="proportion.html#bayesian-inference-with-discrete-priors"><i class="fa fa-check"></i><b>7.2</b> Bayesian Inference with Discrete Priors</a><ul>
<li class="chapter" data-level="7.2.1" data-path="proportion.html"><a href="proportion.html#example-students-dining-preference"><i class="fa fa-check"></i><b>7.2.1</b> Example: students’ dining preference</a></li>
<li class="chapter" data-level="7.2.2" data-path="proportion.html"><a href="proportion.html#discrete-prior-distributions-for-proportion-p"><i class="fa fa-check"></i><b>7.2.2</b> Discrete prior distributions for proportion <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="7.2.3" data-path="proportion.html"><a href="proportion.html#likelihood"><i class="fa fa-check"></i><b>7.2.3</b> Likelihood</a></li>
<li class="chapter" data-level="7.2.4" data-path="proportion.html"><a href="proportion.html#posterior-distribution-for-proportion-p"><i class="fa fa-check"></i><b>7.2.4</b> Posterior distribution for proportion <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="7.2.5" data-path="proportion.html"><a href="proportion.html#inference-students-dining-preference"><i class="fa fa-check"></i><b>7.2.5</b> Inference: students’ dining preference</a></li>
<li class="chapter" data-level="7.2.6" data-path="proportion.html"><a href="proportion.html#discussion-using-a-discrete-prior"><i class="fa fa-check"></i><b>7.2.6</b> Discussion: using a discrete prior</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="proportion.html"><a href="proportion.html#continuous-priors"><i class="fa fa-check"></i><b>7.3</b> Continuous Priors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="proportion.html"><a href="proportion.html#the-beta-distribution-and-probabilities"><i class="fa fa-check"></i><b>7.3.1</b> The Beta distribution and probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="proportion.html"><a href="proportion.html#updating-the-beta-prior"><i class="fa fa-check"></i><b>7.4</b> Updating the Beta Prior</a><ul>
<li class="chapter" data-level="7.4.1" data-path="proportion.html"><a href="proportion.html#bayes-rule-calculation"><i class="fa fa-check"></i><b>7.4.1</b> Bayes’ rule calculation</a></li>
<li class="chapter" data-level="7.4.2" data-path="proportion.html"><a href="proportion.html#from-beta-prior-to-beta-posterior"><i class="fa fa-check"></i><b>7.4.2</b> From Beta prior to Beta posterior</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="proportion.html"><a href="proportion.html#bayesian-inferences-with-continuous-priors"><i class="fa fa-check"></i><b>7.5</b> Bayesian Inferences with Continuous Priors</a><ul>
<li class="chapter" data-level="7.5.1" data-path="proportion.html"><a href="proportion.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>7.5.1</b> Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="7.5.2" data-path="proportion.html"><a href="proportion.html#bayesian-credible-intervals"><i class="fa fa-check"></i><b>7.5.2</b> Bayesian credible intervals</a></li>
<li class="chapter" data-level="7.5.3" data-path="proportion.html"><a href="proportion.html#bayesian-prediction"><i class="fa fa-check"></i><b>7.5.3</b> Bayesian prediction</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="proportion.html"><a href="proportion.html#predictive-checking"><i class="fa fa-check"></i><b>7.6</b> Predictive Checking</a><ul>
<li class="chapter" data-level="7.6.1" data-path="proportion.html"><a href="proportion.html#comparing-bayesian-models"><i class="fa fa-check"></i><b>7.6.1</b> Comparing Bayesian models</a></li>
<li class="chapter" data-level="7.6.2" data-path="proportion.html"><a href="proportion.html#posterior-predictive-checking"><i class="fa fa-check"></i><b>7.6.2</b> Posterior predictive checking</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="proportion.html"><a href="proportion.html#exercises"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mean.html"><a href="mean.html"><i class="fa fa-check"></i><b>8</b> Modeling Measurement and Count Data</a><ul>
<li class="chapter" data-level="8.1" data-path="mean.html"><a href="mean.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="mean.html"><a href="mean.html#modeling-measurements"><i class="fa fa-check"></i><b>8.2</b> Modeling Measurements</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mean.html"><a href="mean.html#examples"><i class="fa fa-check"></i><b>8.2.1</b> Examples</a></li>
<li class="chapter" data-level="8.2.2" data-path="mean.html"><a href="mean.html#the-general-approach"><i class="fa fa-check"></i><b>8.2.2</b> The general approach</a></li>
<li class="chapter" data-level="8.2.3" data-path="mean.html"><a href="mean.html#outline-of-chapter"><i class="fa fa-check"></i><b>8.2.3</b> Outline of chapter</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mean.html"><a href="mean.html#Normal:Discrete"><i class="fa fa-check"></i><b>8.3</b> Bayesian Inference with Discrete Priors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mean.html"><a href="mean.html#Normal:Discrete:Roger"><i class="fa fa-check"></i><b>8.3.1</b> Example: Roger Federer’s time-to-serve</a></li>
<li class="chapter" data-level="8.3.2" data-path="mean.html"><a href="mean.html#Normal:SamplingModel:derivation"><i class="fa fa-check"></i><b>8.3.2</b> Simplification of the likelihood</a></li>
<li class="chapter" data-level="8.3.3" data-path="mean.html"><a href="mean.html#Normal:SamplingModel:inference"><i class="fa fa-check"></i><b>8.3.3</b> Inference: Federer’s time-to-serve</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mean.html"><a href="mean.html#Normal:Continuous"><i class="fa fa-check"></i><b>8.4</b> Continuous Priors</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mean.html"><a href="mean.html#Normal:Continuous:prior"><i class="fa fa-check"></i><b>8.4.1</b> The Normal prior for mean <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="8.4.2" data-path="mean.html"><a href="mean.html#Normal:Continuous:choosing"><i class="fa fa-check"></i><b>8.4.2</b> Choosing a Normal prior</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate"><i class="fa fa-check"></i><b>8.5</b> Updating the Normal Prior</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mean.html"><a href="mean.html#introduction-1"><i class="fa fa-check"></i><b>8.5.1</b> Introduction</a></li>
<li class="chapter" data-level="8.5.2" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:Overview"><i class="fa fa-check"></i><b>8.5.2</b> A quick peak at the update procedure</a></li>
<li class="chapter" data-level="8.5.3" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:BayesRule"><i class="fa fa-check"></i><b>8.5.3</b> Bayes’ rule calculation</a></li>
<li class="chapter" data-level="8.5.4" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:Conjugate"><i class="fa fa-check"></i><b>8.5.4</b> Conjugate Normal prior</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference"><i class="fa fa-check"></i><b>8.6</b> Bayesian Inferences for Continuous Normal Mean</a><ul>
<li class="chapter" data-level="8.6.1" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference:HTandCI"><i class="fa fa-check"></i><b>8.6.1</b> Bayesian hypothesis testing and credible interval</a></li>
<li class="chapter" data-level="8.6.2" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference:Prediction"><i class="fa fa-check"></i><b>8.6.2</b> Bayesian prediction</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="mean.html"><a href="mean.html#Normal:PPC"><i class="fa fa-check"></i><b>8.7</b> Posterior Predictive Checking</a></li>
<li class="chapter" data-level="8.8" data-path="mean.html"><a href="mean.html#modeling-count-data"><i class="fa fa-check"></i><b>8.8</b> Modeling Count Data</a><ul>
<li class="chapter" data-level="8.8.1" data-path="mean.html"><a href="mean.html#examples-1"><i class="fa fa-check"></i><b>8.8.1</b> Examples</a></li>
<li class="chapter" data-level="8.8.2" data-path="mean.html"><a href="mean.html#the-poisson-distribution"><i class="fa fa-check"></i><b>8.8.2</b> The Poisson distribution</a></li>
<li class="chapter" data-level="8.8.3" data-path="mean.html"><a href="mean.html#bayesian-inferences"><i class="fa fa-check"></i><b>8.8.3</b> Bayesian inferences</a></li>
<li class="chapter" data-level="8.8.4" data-path="mean.html"><a href="mean.html#case-study-learning-about-website-counts"><i class="fa fa-check"></i><b>8.8.4</b> Case study: Learning about website counts</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="mean.html"><a href="mean.html#exercises"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>9</b> Simulation by Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="9.1" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#markov-chains"><i class="fa fa-check"></i><b>9.2</b> Markov Chains</a></li>
<li class="chapter" data-level="9.3" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>9.3</b> The Metropolis Algorithm</a></li>
<li class="chapter" data-level="9.4" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#example-cauchy-normal-problem"><i class="fa fa-check"></i><b>9.4</b> Example: Cauchy-Normal problem</a></li>
<li class="chapter" data-level="9.5" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#gibbs-sampling"><i class="fa fa-check"></i><b>9.5</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="9.6" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#mcmc-inputs-and-diagnostics"><i class="fa fa-check"></i><b>9.6</b> MCMC Inputs and Diagnostics</a></li>
<li class="chapter" data-level="9.7" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#using-jags"><i class="fa fa-check"></i><b>9.7</b> Using JAGS</a></li>
<li class="chapter" data-level="9.8" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#exercises"><i class="fa fa-check"></i><b>9.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html"><i class="fa fa-check"></i><b>10</b> Bayesian Hierarchical Modeling</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#hierarchical-normal-modeling"><i class="fa fa-check"></i><b>10.2</b> Hierarchical Normal Modeling</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#hierarchical-beta-binomial-modeling"><i class="fa fa-check"></i><b>10.3</b> Hierarchical Beta-Binomial Modeling</a></li>
<li class="chapter" data-level="10.4" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#exercises"><i class="fa fa-check"></i><b>10.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#example-prices-and-areas-of-house-sales"><i class="fa fa-check"></i><b>11.2</b> Example: Prices and Areas of House Sales</a></li>
<li class="chapter" data-level="11.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-simple-linear-regression-model"><i class="fa fa-check"></i><b>11.3</b> A Simple Linear Regression Model</a></li>
<li class="chapter" data-level="11.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-weakly-informative-prior"><i class="fa fa-check"></i><b>11.4</b> A Weakly Informative Prior</a></li>
<li class="chapter" data-level="11.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#posterior-analysis"><i class="fa fa-check"></i><b>11.5</b> Posterior Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-through-mcmc"><i class="fa fa-check"></i><b>11.6</b> Inference through MCMC</a></li>
<li class="chapter" data-level="11.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#bayesian-inferences-with-simple-linear-regression"><i class="fa fa-check"></i><b>11.7</b> Bayesian Inferences with Simple Linear Regression</a></li>
<li class="chapter" data-level="11.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#informative-prior"><i class="fa fa-check"></i><b>11.8</b> Informative Prior</a></li>
<li class="chapter" data-level="11.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-conditional-means-prior"><i class="fa fa-check"></i><b>11.9</b> A Conditional Means Prior</a></li>
<li class="chapter" data-level="11.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises"><i class="fa fa-check"></i><b>11.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html"><i class="fa fa-check"></i><b>12</b> Bayesian Multiple Regression and Logistic Models</a><ul>
<li class="chapter" data-level="12.1" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#introduction"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#bayesian-multiple-linear-regression"><i class="fa fa-check"></i><b>12.2</b> Bayesian Multiple Linear Regression</a></li>
<li class="chapter" data-level="12.3" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#bayesian-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Bayesian Logistic Regression </a></li>
<li class="chapter" data-level="12.4" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#exercises"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>13</b> Case Studies</a><ul>
<li class="chapter" data-level="13.1" data-path="case-studies.html"><a href="case-studies.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="case-studies.html"><a href="case-studies.html#federalist-papers-study"><i class="fa fa-check"></i><b>13.2</b> Federalist Papers Study</a></li>
<li class="chapter" data-level="13.3" data-path="case-studies.html"><a href="case-studies.html#negative-binomial-sampling"><i class="fa fa-check"></i><b>13.3</b> Negative Binomial sampling</a></li>
<li class="chapter" data-level="13.4" data-path="case-studies.html"><a href="case-studies.html#comparison-of-rates-for-two-authors"><i class="fa fa-check"></i><b>13.4</b> Comparison of rates for two authors</a></li>
<li class="chapter" data-level="13.5" data-path="case-studies.html"><a href="case-studies.html#which-words-distinguish-the-two-authors"><i class="fa fa-check"></i><b>13.5</b> Which words distinguish the two authors?</a></li>
<li class="chapter" data-level="13.6" data-path="case-studies.html"><a href="case-studies.html#career-trajectories"><i class="fa fa-check"></i><b>13.6</b> Career Trajectories</a></li>
<li class="chapter" data-level="13.7" data-path="case-studies.html"><a href="case-studies.html#latent-class-modeling"><i class="fa fa-check"></i><b>13.7</b> Latent Class Modeling</a></li>
<li class="chapter" data-level="13.8" data-path="case-studies.html"><a href="case-studies.html#exercises"><i class="fa fa-check"></i><b>13.8</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probability and Bayesian Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="proportion" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Learning About a Binomial Probability</h1>
<div id="introduction-thinking-about-a-proportion-subjectively" class="section level2">
<h2><span class="header-section-number">7.1</span> Introduction: Thinking About a Proportion Subjectively</h2>
<p>In previous chapters, we have seen many examples involving drawing color balls from a box. In those examples, we are given the numbers of balls of various colors in the box, and we consider questions related to calculating probabilities. For example, there are 40 white and 20 red balls in a box. If you draw two balls at random, what is the probability that both balls are white?</p>
<p>Here we consider a new scenario where we do not know the proportions of color balls in the box. That is, in the previous example, we only know that there are two kinds of color balls in the box, but we don’t know 40 out of 60 of the balls are white (proportion of white = <span class="math inline">\(2/3\)</span>) and 20 out of the 60 of the balls are red (proportion of red = <span class="math inline">\(1/3\)</span>). How can we learn about the proportions of white and red balls? Since counting 60 balls can be tedious, how can we infer those proportions by drawing a sample of balls out of the box and observe the colors of balls in the sample? This becomes an inference question, because we are trying to infer the proportion <span class="math inline">\(p\)</span> of the population, based on a sample from the population.</p>
<p>Let’s continue discussing the scenario where we are told that there are 60 balls in total in a box, and the balls are either white or red. We do not know the count of balls of each of the two colors.
We are given the opportunity to take a random sample of 10 balls out of these 60 balls. We are interested in the quantity <span class="math inline">\(p\)</span>, the proportion of red balls in the 60 balls. How can we infer <span class="math inline">\(p\)</span>, the proportion of red balls in the population (i.e. the 60 balls), based on the numbers of red and white balls we observe in the sample (i.e. the 10 balls)?</p>
<p>Proportions are like probabilities. Recall in Chapter 1 three views of a probability were discussed. We briefly review them here, and state the specific requirements to obtain each view.</p>
<ol style="list-style-type: decimal">
<li><p>The classical view: one needs to write down the sample space where each outcome is equally likely.</p></li>
<li><p>The frequency view: one needs to repeat the random experiments many times under identical conditions.</p></li>
<li><p>The subjective view: one needs to express one’s opinion about the likelihood of a one-time event.</p></li>
</ol>
<p>The classical view does not seem to work here, because we only know there are two kinds of color balls and the total number of balls is 60. Even if we take a sample of 10 balls, we are only going to observe the proportion of red balls in the sample. There does not seem to be a way for us to write down the sample space where each outcome is equally likely.</p>
<p>The frequency view would work here. One could treat the process of obtaining a sample (i.e. taking a random sample of 10 balls from the box) as an experiment, and obtain a sample proportion <span class="math inline">\(\hat{p}\)</span> from the experiment. One then could repeat the experiment many times under the same condition, get many sample proportions <span class="math inline">\(\hat{p}\)</span>, and summarize all the <span class="math inline">\(\hat{p}\)</span>. When one repeats the experiment enough times (a large number), one gets a good sense about the proportion <span class="math inline">\(p\)</span> of red balls in the population of 60 balls in the box. This process is doable, but tedious, time-consuming, and prone to errors.</p>
<p>The subjective view perceives the unknown proportion <span class="math inline">\(p\)</span> subjectively. It does require one to express his or her opinion about the value of <span class="math inline">\(p\)</span>, and he or she could be skeptical and unconfident about the opinion. In Chapter 1, a calibration experiment was introduced to help one sharpen an opinion about the likelihood of an event by comparisons with opinion about the likelihood of other events. In this chapter and the chapters to follow, we introduce the key ideas and practice about thinking subjectively about unknowns and quantify one’s opinions about the values of these unknowns using probability distributions.</p>
<p>As an example, let’s think about plausible values for the proportion <span class="math inline">\(p\)</span> of red balls. As <span class="math inline">\(p\)</span> is a proportion, it can take any possible value between 0 and 1. In the calibration experiment introduced in Chapter 1, we focus on the scenario where only one value of <span class="math inline">\(p\)</span> is of interest. For example, when one thinks that <span class="math inline">\(p\)</span> is 0.5, it is saying that one’s opinion about the probability of the value <span class="math inline">\(p=0.5\)</span> is one. When we phrase it this way ("one’s opinion about the probability of <span class="math inline">\(p=0.5\)</span> is one"), it sounds like a very strong opinion, because one only allows <span class="math inline">\(p\)</span> to take one possible value, and gives probability one of that happening. Since one typically has no thought about the exact value of the proportion <span class="math inline">\(p\)</span>, setting one possible value for the proportion with probability one seems too strong.</p>
<p>Instead suppose that the proportion <span class="math inline">\(p\)</span> can take multiple values between 0 and 1. In particular, let’s consider two scenarios, in both <span class="math inline">\(p\)</span> can take 10 different values, denoted by set <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray}
A = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}
\end{eqnarray}\]</span></p>
<p>Though <span class="math inline">\(p\)</span> can take the same 10 multiple values in both scenarios, we assign different probabilities to each possible value.</p>
<ul>
<li>Scenario 1:
<span class="math display">\[\begin{eqnarray}
f_1(A) = (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)
\end{eqnarray}\]</span></li>
<li>Scenario 2:
<span class="math display">\[\begin{eqnarray}
f_2(A) = (0.05, 0.05, 0.05, 0.175, 0.175, 0.175, 0.175, 0.05, 0.05, 0.05) \nonumber \\
\end{eqnarray}\]</span></li>
</ul>
<p>To visually compare the values of two probability distributions <span class="math inline">\(f_1(A)\)</span> and <span class="math inline">\(f_2(A)\)</span>, we plot <span class="math inline">\(f_1(A)\)</span> and <span class="math inline">\(f_2(A)\)</span> on the same graph.</p>
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="../LATEX/figures/chapter7/RedBallProb_prob.png" alt="The same ten possible values of $p$, but two sets of probabilities." width="500" />
<p class="caption">
Figure 7.1: The same ten possible values of <span class="math inline">\(p\)</span>, but two sets of probabilities.
</p>
</div>
<p>Figure 7.1  labels the <span class="math inline">\(x\)</span>-axis as the values of <span class="math inline">\(p\)</span> (range from 0 to 1), <span class="math inline">\(y\)</span>-axis as the probabilities (range from 0 to 1). For both panels, there are ten bars, each representing the possible values of <span class="math inline">\(p\)</span> in the set <span class="math inline">\(A = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}\)</span>.</p>
<p>The probability assignment in <span class="math inline">\(f_1(A)\)</span> is called a discrete Uniform distribution, where each possible value of the proportion <span class="math inline">\(p\)</span> is equally likely. Since there are ten possible values of <span class="math inline">\(p\)</span>, each value gets assigned a probability of <span class="math inline">\(1/10 = 0.1\)</span>. This assignment expresses the opinion that <span class="math inline">\(p\)</span> can be any value from the set <span class="math inline">\(A = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}\)</span>, and each value has a probability of <span class="math inline">\(0.1\)</span>.</p>
<p>The probability assignment in <span class="math inline">\(f_2(A)\)</span> is also discrete, however, we do not see a Uniform distribution pattern of the probabilities across the board. What we see is that the probabilities of the first three values (0.1, 0.2, and 0.3) and last three (0.8, 0.9, and 1.0) values of <span class="math inline">\(p\)</span> are each <span class="math inline">\(1/3.5\)</span> of that of the middle four (0.4, 0.5, 0.6, and 0.7) values. The shape of the bins reflects the opinion that the middle values of <span class="math inline">\(p\)</span> are 3.5 times as likely as the extreme values of <span class="math inline">\(p\)</span>.</p>
<p>Both sets of probabilities follow the three probability axioms in Chapter 1. One sees that within each set,</p>
<ol style="list-style-type: decimal">
<li><p>Each probability is nonnegative;</p></li>
<li><p>The sum of the probabilities is 1;</p></li>
<li><p>The probability of mutually exclusive values is the sum of probability of each value, e.g. probability of <span class="math inline">\(p = 0.1\)</span> or <span class="math inline">\(p = 0.2\)</span> is <span class="math inline">\(0.1 + 0.1\)</span> in <span class="math inline">\(f_1(A)\)</span>, and <span class="math inline">\(0.05 + 0.05\)</span> in <span class="math inline">\(f_2(A)\)</span>.</p></li>
</ol>
<p>In this introduction, we have presented a way to think about proportions subjectively. We have introduced a way to allow multiple values of <span class="math inline">\(p\)</span>, and perform probability assignments that follow the three probability axioms. One probability distribution expresses a unique opinion about the proportion <span class="math inline">\(p\)</span>.</p>
<p>To answer our inference question “what is the proportion of red balls in the box”, we will take a random sample of 10 balls, and use the observed proportion of red balls in that sample to sharpen and update our belief about <span class="math inline">\(p\)</span>. Bayesian inference is a formal method for implementing this way of thinking and problem solving, including three general steps.</p>
<ul>
<li><p>Step 1: <strong>Prior</strong>: express an opinion about the location of the proportion <span class="math inline">\(p\)</span> before sampling.</p></li>
<li><p>Step 2: <strong>Data/Likelihood</strong>: take the sample and record the observed proportion of red balls.</p></li>
<li><p>Step 3: <strong>Posterior</strong>:use Bayes’ rule to sharpen and update the previous opinion about <span class="math inline">\(p\)</span> given the information from the sample.</p></li>
</ul>
<p>As indicated in the parentheses, the first step “Prior” constructs <strong>prior</strong> opinion about the quantity of interest, and a probability distribution is used (like <span class="math inline">\(f_1(A)\)</span> and <span class="math inline">\(f_2(A)\)</span> earlier) to quantify the prior opinion. The name “prior” indicates that the opinion should be formed before collecting any data.</p>
<p>The second step “Data” is the process of data collection, where the quantity of interest is observed in the collected data. For example, if our 10-ball sample contains 4 red balls and 6 white balls, the observed proportion of red balls is <span class="math inline">\(4/10 = 0.4\)</span>. Informally, how does this information help us sharpen one’s opinion about <span class="math inline">\(p\)</span>? Intuitively one would give more probability to <span class="math inline">\(p = 0.4\)</span>, but it is unclear how the probabilities would be redistributed among the 10 values in <span class="math inline">\(A\)</span>. Since the sum of all probabilities is 1, is it possible that some of the larger proportion values, such as <span class="math inline">\(p = 0.9\)</span> and <span class="math inline">\(p = 1.0\)</span>, will receive probabilities of zero? To address these questions, the third step is needed.</p>
<p>The third step “Posterior” combines one’s prior opinion and the collected data to update one’s opinion about the quantity of interest. Just like the example of observing 4 red balls in the 10-ball sample, one needs a structured way of updating the opinion from prior to posterior.</p>
<p>Throughout this chapter, the entire inference process will be described for learning about a proportion <span class="math inline">\(p\)</span>. This chapter will discuss how to express prior opinion that matches with one’s belief, how to extract information from the data/likelihood, and how to update our opinion to its posterior.</p>
</div>
<div id="bayesian-inference-with-discrete-priors" class="section level2">
<h2><span class="header-section-number">7.2</span> Bayesian Inference with Discrete Priors</h2>
<div id="example-students-dining-preference" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Example: students’ dining preference</h3>
<p>Let’s start our Bayesian inference for proportion <span class="math inline">\(p\)</span> with discrete prior distributions with a students’ dining preference example.
A popular restaurant in a college town has been in business for about 5 years. Though the business is doing well, the restaurant owner wishes to learn more about his customers. Specifically, he is interested in learning about the dining preferences of the students. The owner plans to conduct a survey by asking students "what is your favorite day for eating out?" In particular, he wants to find out what percentage of students prefer to dine on Friday, so he can plan ahead for ordering supplies and giving promotions.</p>
<p>Let <span class="math inline">\(p\)</span> denote the proportion of all students whose answer is Friday.</p>
</div>
<div id="discrete-prior-distributions-for-proportion-p" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Discrete prior distributions for proportion <span class="math inline">\(p\)</span></h3>
<p>Before giving out the survey, let’s pause and think about the possible values for the proportion <span class="math inline">\(p\)</span>. Not only does one want to know about possible values, but also the probabilities associated with the values. A probability distribution provides a measure of belief for the proportion and it ultimately will help the restaurant owner improve his business.</p>
<p>One might not know much about students’ dining preference, but it is possible to come up with a list of plausible values for the proportion. There are seven days a week. If each day was equally popular, then one would expect 1/7 or approximately 15% of all students to choose Friday. The owner recognizes that Friday is the start of the weekend, therefore there should be a higher chance of being students’ preferred day of dining out. So perhaps <span class="math inline">\(p\)</span> starts with 0.3. Then what about the largest plausible value? Letting this largest value be 1 seems unrealistic, as there are six other days in the week. Suppose that one chooses 0.8 to be the largest plausible value, and then comes up with the list of values of <span class="math inline">\(p\)</span> to be the six values going from 0.3 to 0.8 with an increment of 0.1.</p>
<p><span class="math display" id="eq:plist1">\[\begin{eqnarray}
p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}
\label{eq:dining:p}
\tag{7.1}
\end{eqnarray}\]</span></p>
<p>Next one needs to assign probabilities to the list of plausible values of <span class="math inline">\(p\)</span>. Since one may not know much about the location of the probabilities <span class="math inline">\(p\)</span>, a good place to start is a discrete Uniform prior (recall the discrete Uniform prior distribution for <span class="math inline">\(p\)</span>, the proportion of red balls, in Section 7.1). A discrete Uniform prior distribution expresses the opinion that all plausible values of <span class="math inline">\(p\)</span> are equally likely. In the current students’ dining preference example, if one decides on six plausible values of <span class="math inline">\(p\)</span> as in Equation (7.1), each of the six values gets a prior probability of 1/6. One labels this prior as <span class="math inline">\(\pi_l\)</span>, where <span class="math inline">\(l\)</span> stands for laymen (for all of us who are not in the college town restaurant business). Note that in the notation <span class="math inline">\(f_l(p)\)</span>, the first <span class="math inline">\(p\)</span> stands for probability, and the <span class="math inline">\(p\)</span> in the parenthesis is our quantity of interest, the proportion <span class="math inline">\(p\)</span> of students preferring to dine out on Friday.</p>
<p><span class="math display" id="eq:prior1">\[\begin{eqnarray}
\pi_l(p)= (1/6, 1/6, 1/6, 1/6, 1/6, 1/6)
\label{eq:dining:laymenprior}
\tag{7.2}
\end{eqnarray}\]</span></p>
<p>With five years of experience of running his restaurant in this college town, the restaurant owner might have different opinions about likely values of <span class="math inline">\(p\)</span>. Suppose he agrees with us that <span class="math inline">\(p\)</span> could take the 6 plausible values from 0.3 to 0.8, but he assigns a different prior distribution for <span class="math inline">\(p\)</span>. In particular, the restaurant owner thinks that values of 0.5 and 0.6 are most likely – each of these values is twice as likely as the other values. His prior is labelled as <span class="math inline">\(\pi_e\)</span>, where <span class="math inline">\(e\)</span> stands for expert.</p>
<p><span class="math display" id="eq:prior2">\[\begin{eqnarray}
\pi_e(p)= (0.125, 0.125, 0.250, 0.250, 0.125, 0.125)
\label{eq:dining:expertprior}
\tag{7.3}
\end{eqnarray}\]</span></p>
<p>To obtain <span class="math inline">\(\pi_e(p)\)</span> efficiently, one can use the <code>ProbBayes</code> R package. First a data frame is created by providing the list of plausible values of <span class="math inline">\(p\)</span> and corresponding weights assigned to each value using the function <code>data.frame()</code>. As one can see here, one does not have to calculate the probability – one only needs to give the weights (e.g. giving <span class="math inline">\(p = 0.3, 0.4, 0.7, 0.8\)</span> weight 1 and giving <span class="math inline">\(p = 0.5, 0.6\)</span> weight 2, to reflect the owner’s opinion "0.5 and 0.6 are twice as likely as the other values").</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="proportion.html#cb1-1"></a>bayes_table &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">p =</span> <span class="kw">seq</span>(.<span class="dv">3</span>, <span class="fl">.8</span>, <span class="dt">by=</span>.<span class="dv">1</span>),</span>
<span id="cb1-2"><a href="proportion.html#cb1-2"></a>                          <span class="dt">Prior =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb1-3"><a href="proportion.html#cb1-3"></a>bayes_table</span></code></pre></div>
<pre><code>##     p Prior
## 1 0.3     1
## 2 0.4     1
## 3 0.5     2
## 4 0.6     2
## 5 0.7     1
## 6 0.8     1</code></pre>
<p>One uses the function <code>mutate()</code> to normalize these weights to obtain the prior probabilities in the Prior column.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="proportion.html#cb3-1"></a>bayes_table <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Prior =</span> Prior <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(Prior)) -&gt;<span class="st"> </span>bayes_table</span>
<span id="cb3-2"><a href="proportion.html#cb3-2"></a>bayes_table</span></code></pre></div>
<pre><code>##     p Prior
## 1 0.3 0.125
## 2 0.4 0.125
## 3 0.5 0.250
## 4 0.6 0.250
## 5 0.7 0.125
## 6 0.8 0.125</code></pre>
<p>One conveniently plots the restaurant owner’s prior distribution by use of <code>ggplot2</code> functions.
This distribution is displayed in Figure 7.2.</p>
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="../LATEX/figures/chapter7/prior.png" alt="The restaurant owner's prior distribution for the proportion $p$." width="500" />
<p class="caption">
Figure 7.2: The restaurant owner’s prior distribution for the proportion <span class="math inline">\(p\)</span>.
</p>
</div>
<p>It is left as an exercise for the reader to compute and plot the laymen’s prior <span class="math inline">\(\pi_l(p)\)</span> in Equation (7.2). For the rest of this section, we will work with the expert’s prior <span class="math inline">\(\pi_e(p)\)</span>.</p>
</div>
<div id="likelihood" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Likelihood</h3>
<p>The next step in the inference process is the data collection. The restaurant owner gives a survey to 20 student diners at the restaurant. Out of the 20 student respondents, 12 say that their favorite day for eating out is Friday. Recall the quantity of interest is proportion <span class="math inline">\(p\)</span> of the population of students choosing Friday.</p>
<p>The likelihood is a function of the quantity of interest, which is the proportion <span class="math inline">\(p\)</span>. The owner has conducted an experiment 20 times, where each experiment involves a “yes” or “no” answer from the respondent to the rephrased question “whether Friday is your preferred day to dine out”. Then the proportion <span class="math inline">\(p\)</span> is the probability a student answers "yes".</p>
<p>Does this ring a bell of what we have seen before? Indeed, in Chapter 4, one has seen this type of experiment, a Binomial experiment, similar to the dining survey. Recall that a Binomial experiment needs to satisfy four conditions:</p>
<ul>
<li>One is repeating the same basic task or trial many times – let the number of trials be denoted by <span class="math inline">\(n\)</span>.</li>
<li>On each trial, there are two possible outcomes called <code>success" or</code>failure".</li>
<li>The probability of a success, denoted by <span class="math inline">\(p\)</span>, is the same for each trial.</li>
<li>The results of outcomes from different trials are independent.</li>
</ul>
<p>If one recognizes an experiment as being Binomial, then all one needs to know is <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> to determine probabilities for the number of successes <span class="math inline">\(Y\)</span>. The probability of <span class="math inline">\(y\)</span> successes in a Binomial experiment is given by</p>
<p><span class="math display" id="eq:binprob1">\[\begin{eqnarray}
Prob(Y=y) = {n \choose y} p^y (1 - p)^{n - y}, y = 0, \cdots, n.
\tag{7.4}
\end{eqnarray}\]</span></p>
<p>Assuming the dining survey is a random sample (thus independent outcomes), this is the result of a Binomial experiment. The likelihood is the chance of 12 successes in 20 trials viewed as a function of the probability of success <span class="math inline">\(p\)</span>:
<span class="math display" id="eq:blikelihood1">\[\begin{eqnarray}
Likelihood = L(p) = {20 \choose 12} p ^ {12} (1 - p) ^ 8.
\label{eq:dining:likelihood}
\tag{7.5}
\end{eqnarray}\]</span></p>
<p>Generally one uses <span class="math inline">\(L\)</span> to denote a likelihood function — one sees in Equation (7.5), <span class="math inline">\(L\)</span> is a function of <span class="math inline">\(p\)</span>. Note that the value of <span class="math inline">\(n\)</span>, the total number of trials, is known and the number of successes <span class="math inline">\(Y\)</span> is observed to be 12. The proportion <span class="math inline">\(p\)</span>, is the parameter of the Binomial experiment and the likelihood is a function of the proportion <span class="math inline">\(p\)</span>.</p>
<p>The likelihood function <span class="math inline">\(L(p)\)</span> is efficiently computed using the <code>dbinom()</code> function in R. In order to use this function, we need to know the sample size <span class="math inline">\(n\)</span> (20 in the dining survey), the number of successes <span class="math inline">\(y\)</span> (12 in the dining survey), and <span class="math inline">\(p\)</span> (the list of 6 plausible values created in Section 7.2.2; <span class="math inline">\(p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}\)</span>). Note that we only need the plausible values of <span class="math inline">\(p\)</span>, not yet the assigned probabilities in the prior distribution. The prior will be used in the third step to update the opinion of <span class="math inline">\(p\)</span> to its posterior.</p>
<p>Below is the example R code of finding the probability of 12 successes in a sample of 20 for each value of the proportion <span class="math inline">\(p\)</span>. The values are placed in the <code>Likelihood</code> column of the <code>bayes_table</code> data frame.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="proportion.html#cb5-1"></a>bayes_table<span class="op">$</span>Likelihood &lt;-<span class="st"> </span><span class="kw">dbinom</span>(<span class="dv">12</span>, <span class="dt">size=</span><span class="dv">20</span>, <span class="dt">prob=</span>bayes_table<span class="op">$</span>p)</span>
<span id="cb5-2"><a href="proportion.html#cb5-2"></a>bayes_table</span></code></pre></div>
<pre><code>##     p Prior  Likelihood
## 1 0.3 0.125 0.003859282
## 2 0.4 0.125 0.035497440
## 3 0.5 0.250 0.120134354
## 4 0.6 0.250 0.179705788
## 5 0.7 0.125 0.114396740
## 6 0.8 0.125 0.022160877</code></pre>
</div>
<div id="posterior-distribution-for-proportion-p" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Posterior distribution for proportion <span class="math inline">\(p\)</span></h3>
<p>The posterior probabilities are found as an application of Bayes’ rule. This recipe will be illustrated first through a step-by-step calculation process. Next the process is demonstrated with the <code>bayesian_crank()</code> function in the <code>ProbBayes</code> R package, which implements the Bayes’ rule calculation and outputs the posterior probabilities.</p>
<p>Let <span class="math inline">\(\pi(p)\)</span> to be the prior distribution of <span class="math inline">\(p\)</span>, let <span class="math inline">\(L(p)\)</span> denote the likelihood function, and <span class="math inline">\(\pi(p \mid y)\)</span> to be the posterior distribution of <span class="math inline">\(p\)</span> after observing the number of successes <span class="math inline">\(y\)</span>.
For discrete parameters, such as the proportion <span class="math inline">\(p\)</span> in our case, one is able to enumerate the list of plausible values and assign prior probabilities to the values. If <span class="math inline">\(p_i\)</span> represents a particular value of <span class="math inline">\(p\)</span>, Bayes’ rule for a discrete parameter has the form
<span class="math display" id="eq:pbayes">\[\begin{eqnarray}
\pi(p_i \mid y)  = \frac{\pi(p_i) \times L(p_i)} {\sum_j \pi(p_j) \times L(p_j)},
\label{eq:Discrete:bayesrule}
\tag{7.6}
\end{eqnarray}\]</span>
where <span class="math inline">\(\pi(p_i)\)</span> is the prior probability of <span class="math inline">\(p = p_i\)</span>, <span class="math inline">\(L(p_i)\)</span> is the likelihood function evaluated at <span class="math inline">\(p = p_i\)</span>, and <span class="math inline">\(\pi(p_i \mid y)\)</span> is the posterior probability of <span class="math inline">\(p = p_i\)</span> given the number of successes <span class="math inline">\(y\)</span>. By the <strong>Law of Total Probability</strong>, the denominator gives the marginal distribution of the observation <span class="math inline">\(y\)</span>.</p>
<p>Bayes’ rule can also be expressed as "prior times likelihood":
<span class="math display" id="eq:priortimeslike">\[\begin{eqnarray}
\pi(p_i \mid y)  \propto \pi(p_i) \times L(p_i)
\label{eq:Discrete:bayesruleProp}
\tag{7.7}
\end{eqnarray}\]</span>
Equation (7.7) ignores the denominator and states that the posterior is proportional to the product of the prior and the likelihood. As one will see soon, the value of the denominator is a constant, meaning that its purpose is to normalize the numerator. It is convenient to work with Bayes’ rule as in Equation (7.7) in later chapters. However, it is instructive to show the exact calculation of Equation (7.6), because one has a finite sum in the denominator and it is possible to obtain the analytical solution. In the case where the prior is continuous, it will be more difficult to analytically compute the normalizing constant.</p>
<p>Returning to the students’ dining preference example, the list of plausible values of the proportion is <span class="math inline">\(p = \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\}\)</span> and according to the restaurant owner’s expert prior, the assigned probabilities are <span class="math inline">\(\pi_e(p)= (0.125, 0.125, 0.250, 0.250, 0.125, 0.125)\)</span> (recall Figure 7.2). After observing the number of successes, the likelihood values are calculated for the models using <code>dbinom()</code> function, as presented in Section 7.2.3.</p>
<p>The denominator is the sum of the products of the prior and the likelihood at each possible <span class="math inline">\(p_i\)</span>, which, given the Law of Total Probability, is equal to the marginal probability of the data <span class="math inline">\(f(y)\)</span>. One can think of the above formula as reweighing or normalizing the probability of <span class="math inline">\(\pi(p_i \mid y)\)</span> by all possible values of <span class="math inline">\(p\)</span>. In the case of discrete models like this, the marginal probability of the likelihood is computed through <span class="math inline">\(\sum_j f(p_j) \times L(p_j)\)</span>.</p>
<p>In this setup, the computation of the posterior probabilities of different <span class="math inline">\(p_i\)</span> values is straightforward. First, one calculates the denominator and denote the value as <span class="math inline">\(D\)</span>.
<span class="math display">\[\begin{eqnarray*}
D &amp;=&amp; \pi(0.3) \times L(0.3) + \pi(0.4) \times L(0.4) + \cdots + \pi(0.8) \times L(0.8) \\
&amp;=&amp; 0.125 \times {20 \choose 12}(0.3)^{12}(1-0.3)^{8} + \cdots + 0.125 \times {20 \choose 12}(0.8)^{12}(1-0.8)^{8} \nonumber \\ &amp;\approx&amp; 0.0969.
\end{eqnarray*}\]</span>
Then the posterior probability of <span class="math inline">\(p = 0.3\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
\pi(p = 0.3 \mid 12) &amp;=&amp; \frac{\pi(0.3) \times L(0.3)}{D} \\
&amp;=&amp; \frac{0.125 \times {20 \choose 12}(0.3)^{12}(1-0.3)^{8}}{D}  \\
&amp;\approx&amp; 0.005.
\end{eqnarray*}\]</span>
In a similar fashion, the posterior probability of <span class="math inline">\(p=0.5\)</span> is calculated as
<span class="math display">\[\begin{eqnarray*}
\pi(p = 0.5 \mid 12) &amp;=&amp; \frac{\pi(0.5) \times L(0.5)}{D} \\
&amp;=&amp; \frac{0.125 \times {20 \choose 12}(0.5)^{12}(1-0.5)^{8}}{D} \\
&amp;\approx&amp; 0.310.
\end{eqnarray*}\]</span>
One sees that the denominator is the same for the posterior probability calculation of every value of <span class="math inline">\(p\)</span>. This calculation gets tedious for a large number of possible values of <span class="math inline">\(p\)</span>. Relying on statistical software such as <code>R</code> helps us simplify the tasks.</p>
<p>To use the <code>bayesian_crank()</code> function, recall that we have already created a data frame with variables <code>p</code>, <code>Prior</code>, and <code>Likelihood</code>. Then the <code>bayesian_crank()</code> function is used to compute the posterior probabilities.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="proportion.html#cb7-1"></a><span class="kw">bayesian_crank</span>(bayes_table) -&gt;<span class="st"> </span>bayes_table</span>
<span id="cb7-2"><a href="proportion.html#cb7-2"></a>bayes_table</span></code></pre></div>
<pre><code>##     p Prior  Likelihood      Product   Posterior
## 1 0.3 0.125 0.003859282 0.0004824102 0.004975901
## 2 0.4 0.125 0.035497440 0.0044371799 0.045768032
## 3 0.5 0.250 0.120134354 0.0300335884 0.309786454
## 4 0.6 0.250 0.179705788 0.0449264469 0.463401326
## 5 0.7 0.125 0.114396740 0.0142995925 0.147495530
## 6 0.8 0.125 0.022160877 0.0027701096 0.028572757</code></pre>
<p>As one sees in the <code>bayes_table</code> output, the <code>bayesian_crank()</code> function computes the product of <code>Prior</code> and <code>Likelihood</code> and stores the values in the column <code>Product</code>, then normalizes each product with the sum of all products to produce the posterior probabilities, stored in the column <code>Posterior</code>.</p>
<p>Figure 7.3 compares the prior probabilities in the bottom panel with the posterior probabilities in the top panel. Notice the difference in the two distributions. After observing the survey results (i.e. the data/likelihood), the owner is more confident that <span class="math inline">\(p\)</span> is equal to 0.5 or 0.6, and it is unlikely for <span class="math inline">\(p\)</span> to be 0.3, 0.4, 0.7, and 0.8. Recall that the data gives an observed proportion 12/20 = 0.6. Since the posterior is a combination of prior and data/likelihood, it is not surprising that the data/likelihood helps the owner to sharpen his belief about proportion <span class="math inline">\(p\)</span> and place a larger posterior probability around 0.6.</p>
<div class="figure"><span id="fig:unnamed-chunk-9"></span>
<img src="../LATEX/figures/chapter7/priorpost2.png" alt="Prior and posterior distributions on the proportion $p$." width="500" />
<p class="caption">
Figure 7.3: Prior and posterior distributions on the proportion <span class="math inline">\(p\)</span>.
</p>
</div>
</div>
<div id="inference-students-dining-preference" class="section level3">
<h3><span class="header-section-number">7.2.5</span> Inference: students’ dining preference</h3>
<p>Let’s revisit the posterior distribution table to perform some inference. What is the posterior probability that over half of the students prefer eating out on Friday? One is interested in the probability that <span class="math inline">\(p &gt;\)</span> 0.5, in the posterior. Looking at the table, this posterior probability is equal to
<span class="math display">\[\begin{eqnarray*}
 Prob(p &gt; 0.5) \approx 0.463 + 0.147 + 0.029 = 0.639.
\end{eqnarray*}\]</span>
This means the owner is reasonably confident (with probability 0.639) that over half of the college students prefer to eat out on Friday.</p>
<p>One easily obtains the probability from the R output, for example.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="proportion.html#cb9-1"></a><span class="kw">sum</span>(bayes_table<span class="op">$</span>Posterior[bayes_table<span class="op">$</span>p <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span>])</span></code></pre></div>
<pre><code>## [1] 0.6394696</code></pre>
</div>
<div id="discussion-using-a-discrete-prior" class="section level3">
<h3><span class="header-section-number">7.2.6</span> Discussion: using a discrete prior</h3>
<p>Specifying a discrete prior has two steps: (1) specifying a list of plausible values of the parameter of interest, and (2) assigning probabilities to the plausible values. It is important to remember the three probability axioms when specifying a discrete prior.</p>
<p>After the prior specification, the next component is the data/likelihood, which can also be broken up into two steps. First, one constructs a suitable experiment that works for the particular scenario. Here one has a Binomial experiment for a survey to a fixed number of respondents, the answers are classified into <code>yes" and</code>no" or <code>success" and</code>failure", the outcome of interest is the number of successes and trials are independent.
From the Binomial distribution, one obtains the likelihood function which is evaluated at each possible value of the parameter of interest. In our example, the <code>dbinom()</code> R function was used to calculate the likelihood function.</p>
<p>Last, the posterior probabilities are calculated using Bayes’ rule. In particular for the discrete case, follow Equation (7.6). The calculation of the denominator is tedious, however practice with the Bayes’ rule calculation enhances one’s understanding of Bayesian inference. R functions such as <code>bayesian_crank()</code> are helpful for implementing the Bayes’ rule calculations. Bayesian inference follows from a suitable summarization of the posterior probabilities. In our example, inference was illustrated by calculating the probability that over half of the students prefer eating out on Friday.</p>
<p>Let’s revisit the list of plausible values of proportion <span class="math inline">\(p\)</span> of students preferring Friday in dining out in the example. Although <span class="math inline">\(p = 1.0\)</span>, that is, everyone prefers Friday, is very unlikely, one might not want to eliminate this proportion value from consideration. As one observes in the Bayes’ rule calculation process shown in Sections 7.2.3 and 7.2.4, if one does not include <span class="math inline">\(p = 1.0\)</span> as one of the plausible values in the prior distribution in Section 7.2.2, this value will also be given a probability of zero in the posterior.</p>
<p>Alternatively, one could choose the alternative set of values
<span class="math display">\[\begin{eqnarray*}
p = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\},
\end{eqnarray*}\]</span>
and assign a very small prior probability (e.g. 0.05 or even smaller) for <span class="math inline">\(p = 1.0\)</span> to express the opinion that <span class="math inline">\(p = 1.0\)</span> is very unlikely. One may assign small prior probabilities for other large values of <span class="math inline">\(p\)</span> such as <span class="math inline">\(p = 0.9\)</span>.</p>
<p>This comment illustrates a limitation of specifying a discrete prior for a proportion <span class="math inline">\(p\)</span>. If a plausible value is not specified in the prior distribution (e.g. <span class="math inline">\(p = 1.0\)</span> is not in the restaurant owner’s prior distribution), it will be assigned a probability of zero in the posterior (e.g. <span class="math inline">\(p = 1.0\)</span> is not in the restaurant owner’s posterior distribution).</p>
<p>It generally is more desirable to have <span class="math inline">\(p\)</span> to be any value in [0, 1] including less plausible values such as <span class="math inline">\(p = 1.0\)</span>.
To make this happen, the proportion <span class="math inline">\(p\)</span> should be allowed to take any value between 0 and 1, which means <span class="math inline">\(p\)</span> will be a continuous variable. In this situation, it is necessary to construct a continuous prior distribution for <span class="math inline">\(p\)</span>. A popular class of continuous prior distributions for proportion is the Beta distribution which is the subject of the next section.</p>
</div>
</div>
<div id="continuous-priors" class="section level2">
<h2><span class="header-section-number">7.3</span> Continuous Priors</h2>
<p>Let’s continue our students’ dining preference example. A restaurant owner is interested in learning about the proportion <span class="math inline">\(p\)</span> of students whose favorite day for eating out is Friday.</p>
<p>The proportion <span class="math inline">\(p\)</span> should be a value between 0 and 1. Previously, we used a discrete prior for <span class="math inline">\(p\)</span>, representing the belief that <span class="math inline">\(p\)</span> only takes the six different values 0.3, 0.4, 0.5, 0.6, 0.7, and 0.8. An obvious limitation of this assumption is, what if the true <span class="math inline">\(p\)</span> is 0.55? If the value 0.55 is not specified in the prior distribution of <span class="math inline">\(p\)</span> (that is, a zero probability is assigned to the value <span class="math inline">\(p\)</span> = 0.55), then by the Bayes’ rule calculation (either by hand or by the useful <code>bayesian_crank()</code> function) there will be zero posterior probability assigned to 0.55. It is therefore preferable to specify a prior that allows <span class="math inline">\(p\)</span> to be any value in the interval [0, 1].</p>
<p>To represent such a prior belief, it is assumed that <span class="math inline">\(p\)</span> is continuous on [0, 1]. Suppose again that one is a layman unfamiliar with the pattern of dining during a week. Then one possible choice of a continuous prior for <span class="math inline">\(p\)</span> is the continuous Uniform distribution, which expresses the opinion that <span class="math inline">\(p\)</span> is equally likely to take any value between 0 and 1.</p>
<p>Formally, the probability density function of the continuous Uniform on the interval <span class="math inline">\((a, b)\)</span> is
<span class="math display" id="eq:uniformprior">\[\begin{eqnarray}
\pi(p) = 
\begin{cases}
  \frac{1}{b - a} &amp; \text{for }a \le p \le b,\\    
  0     		&amp; \text{for }p &lt; a \,\, \text{or } p &gt; b.
\end{cases}
\label{eq:Binomial:Continuous:Uniform}
\tag{7.8}
\end{eqnarray}\]</span>
In our situation <span class="math inline">\(p\)</span> is a continuous Uniform random variable on [0, 1], we have <span class="math inline">\(\pi(p) = 1\)</span> for <span class="math inline">\(p \in [0, 1]\)</span>, and <span class="math inline">\(\pi(p) = 0\)</span> everywhere else.</p>
<p>What about other possible continuous prior distributions for <span class="math inline">\(p\)</span> on [0, 1]? Consider a prior distribution for the restaurant owner who has some information about the location (i.e. value) of <span class="math inline">\(p\)</span>. This owner would be interested in a continuous version of the discrete prior distribution where values of <span class="math inline">\(p\)</span> between 0.3 and 0.8 are more likely than the values at the two ends.</p>
<p>The Beta family of continuous distributions is useful for representing prior knowledge in this situation. A Beta distribution, denoted by Beta(<span class="math inline">\(a, b)\)</span>, represents probabilities for a random variable falling between 0 and 1. This distribution has two shape parameters, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, with probability density function given by
<span class="math display" id="eq:betaprior">\[\begin{eqnarray}
\pi(p) = \frac{1}{B(a, b)} p^{a - 1} (1 - p)^{b - 1}, \, \, 0 \le p \le 1,
\tag{7.9}
\end{eqnarray}\]</span>
where <span class="math inline">\(B(a, b)\)</span> is the Beta function defined by <span class="math inline">\(B(a, b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\)</span>, where <span class="math inline">\(\Gamma\)</span> is the Gamma function.
For future reference, it is useful to know that if <span class="math inline">\(p \sim {\rm Beta}(a, b)\)</span>, its mean <span class="math inline">\(E[p] = \frac{a}{a+b}\)</span> and its variance <span class="math inline">\(V(p) = \frac{ab}{(a+b)^2(a+b+1)}\)</span>.
The continuous Uniform in Equation (7.8) is a special case of the Beta distribution: <span class="math inline">\(\textrm{Uniform}(0, 1) = \textrm{Beta}(1, 1)\)</span>.</p>
<p>For the remainder of this section, Section 7.3.1 introduces the Beta distribution and Beta probabilities, and Section 7.3.2 focuses on several ways of choosing a Beta prior that reflects one’s opinion about the location of a proportion.</p>
<div id="the-beta-distribution-and-probabilities" class="section level3">
<h3><span class="header-section-number">7.3.1</span> The Beta distribution and probabilities</h3>
<p>The two shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> control the shape of the Beta density curve. Figure 7.4 shows density curves of Beta distributions for several choices of the shape parameters. One observes from this figure that the Beta density curve displays vastly different shapes for varying choices of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. For example, <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> represents the prior belief that extreme values of <span class="math inline">\(p\)</span> are likely and <span class="math inline">\(p=0.5\)</span> is the least probable value. In the students’ dining preference example, specifying a <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> would reflect the owner’s belief that the proportion of students dining out on Friday is either very high (near one) or very low (near one) and not likely to be moderate values.</p>
<div class="figure"><span id="fig:unnamed-chunk-11"></span>
<img src="../LATEX/figures/chapter7/Beta-priors.png" alt="Illustration of nine Beta density curves." width="500" />
<p class="caption">
Figure 7.4: Illustration of nine Beta density curves.
</p>
</div>
<p>As the Beta is a common continuous distribution, R functions are available for Beta distribution calculations. We provide a small example of “Beta” functions for <code>Beta(1, 1)</code>, where the two shape parameters 1 and 1 are the second and third arguments of the functions.</p>

<p>Recall the following useful results from previous material: (1) a <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> distribution is a Uniform density on (0, 1), (2) the density of <span class="math inline">\(\textrm{Uniform}(0, 1)\)</span> is <span class="math inline">\(\pi(p) = 1\)</span> on [0, 1], and (3) if <span class="math inline">\(p \sim \textrm{Uniform}(0, 1)\)</span>, then the cdf <span class="math inline">\(F(x) = Prob(p \leq x) = x\)</span> for <span class="math inline">\(x \in [0, 1]\)</span>.</p>
<ul>
<li><code>dbeta()</code>: the probability density function for a <span class="math inline">\(\textrm{Beta}(a, b)\)</span> which takes a value of the random variable as its input and outputs the probability density function at that value.</li>
</ul>
<p>For example, we
evaluate the density function of <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> at the values <span class="math inline">\(p = 0.5\)</span> and <span class="math inline">\(p = 0.8\)</span>, which should be both 1, and 0 at <span class="math inline">\(p = 1.2\)</span> which should be 0 since this value is outside of [0, 1].</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="proportion.html#cb11-1"></a><span class="kw">dbeta</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>, <span class="fl">1.2</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 1 1 0</code></pre>
<ul>
<li><code>pbeta()</code>: the distribution function of a Beta(a; b) random variable,
which takes a value x and gives the value of the random variable at
that value, F(x).</li>
</ul>
<p>For example, suppose one wishes to evaluate the distribution function of <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> at <span class="math inline">\(p = 0.5\)</span> and <span class="math inline">\(p = 0.8\)</span>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="proportion.html#cb13-1"></a><span class="kw">pbeta</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.5 0.8</code></pre>
<p>One calculates the probability of <span class="math inline">\(p\)</span> between 0.5 and 0.8, i.e. <span class="math inline">\(Prob(0.5 \le p \le 0.8)\)</span> by taking the difference of the cdf at the two values.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="proportion.html#cb15-1"></a><span class="kw">pbeta</span>(<span class="fl">0.8</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pbeta</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.3</code></pre>
<p><code>qbeta()</code>: the quantile function of a <span class="math inline">\(\textrm{Beta}(a, b)\)</span>, which inputs a probability value <span class="math inline">\(p\)</span> and outputs the value of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F(x) = p\)</span>.</p>
<p>For example, suppose one wishes to calculate the quantile of <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> at <span class="math inline">\(p = 0.5\)</span> and <span class="math inline">\(p = 0.8\)</span>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="proportion.html#cb17-1"></a><span class="kw">qbeta</span>(<span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.8</span>), <span class="dv">1</span>, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.5 0.8</code></pre>
<ul>
<li><code>rbeta()</code>: the random number generator for <span class="math inline">\(\textrm{Beta}(a, b)\)</span>, which inputs the size of a random sample and gives a vector of the simulated random variates.</li>
</ul>
<p>For example, suppose one is interested in simulating a sample of size five from <span class="math inline">\(\textrm{Beta}(1, 1)\)</span>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="proportion.html#cb19-1"></a><span class="kw">rbeta</span>(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.5908538 0.6020907 0.3867753 0.2924141 0.6692194</code></pre>
<p>There are additional functions in the <code>ProbBayes</code> R package that aid in visualizing Beta distribution calculations.
For example, suppose one has a <span class="math inline">\(\textrm{Beta}(7, 10)\)</span> curve and we want to find the chance that <span class="math inline">\(p\)</span> is between 0.4 and 0.8. Looking at Figure 7.5, this probability corresponds to the area of the shaded region. The special function <code>beta_area()</code> will compute and illustrate this probability. Note the use of the vector <code>c(7, 10)</code> to input the two shape parameters.</p>
<pre><code>beta_area(0.4, 0.8, c(7, 10))</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-17"></span>
<img src="../LATEX/figures/chapter7/betaprob1.png" alt="Area represents the probability that a Beta(7, 10) variable lies between 0.4 and 0.8" width="500" />
<p class="caption">
Figure 7.5: Area represents the probability that a Beta(7, 10) variable lies between 0.4 and 0.8
</p>
</div>
<p>One could also find the chance that <span class="math inline">\(p\)</span> is between 0.4 and 0.8 by subtracting two <code>pbeta()</code> functions.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="proportion.html#cb22-1"></a><span class="kw">pbeta</span>(<span class="fl">0.8</span>, <span class="dv">7</span>, <span class="dv">10</span>) <span class="op">-</span><span class="st"> </span><span class="kw">pbeta</span>(<span class="fl">0.4</span>, <span class="dv">7</span>, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [1] 0.5269265</code></pre>
<p>The function <code>beta_quantile()</code> works in the same way as <code>qbeta()</code>, the quantile function. However, <code>beta_quantile()</code> automatically produces a plot with the shaded probability area. Figure 7.6 plots and computes the quantile to be 0.408. The chance that <span class="math inline">\(p\)</span> is smaller than 0.408 is 0.5.</p>
<pre><code>beta_quantile(0.5, c(7, 10))</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-19"></span>
<img src="../LATEX/figures/chapter7/betaprob2.png" alt="Illustration of a 0.5 quantile for a Beta(7, 10) variable." width="500" />
<p class="caption">
Figure 7.6: Illustration of a 0.5 quantile for a Beta(7, 10) variable.
</p>
</div>
<p>Alternatively, use the <code>qbeta()</code> function without returning a plot.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="proportion.html#cb25-1"></a><span class="kw">qbeta</span>(<span class="fl">0.5</span>, <span class="dv">7</span>, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## [1] 0.4082265</code></pre>
</div>
<div id="choosing-a-beta-density-to-represent-prior-opinion" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Choosing a Beta density to represent prior opinion</h3>
<p>One wants to use a <span class="math inline">\(\textrm{Beta}(a, b)\)</span> density curve to represent one’s prior opinion about the values of the proportion <span class="math inline">\(p\)</span> and their associated probabilities. It is difficult to guess at values of the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> directly. However, there are indirect ways of guessing their values. We present two general methods here.</p>
<p>The first method is to consider the shape parameter <span class="math inline">\(a\)</span> as the prior count of “successes” and the other shape parameter <span class="math inline">\(b\)</span> as the prior count of “failures”. Subsequently, the value <span class="math inline">\(a + b\)</span> represents the <strong>prior sample size</strong> comparable to <span class="math inline">\(n\)</span>, the <strong>data sample size</strong>.
Following this setup, one could specify a Beta prior with shape parameter <span class="math inline">\(a\)</span> expressing the number of successes in one’s prior opinion, and the other shape parameter <span class="math inline">\(b\)</span> expressing the number of failures in one’s prior opinion. For example, if one believes that <strong>a priori</strong> there should be about 4 successes and 4 failures, then one could use <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> as the prior distribution for the proportion <span class="math inline">\(p\)</span>.</p>

<p>How can we check if <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> looks like what we believe <strong>a priori</strong>? Recall that <code>rbeta()</code> generates a random sample from a Beta distribution. The R script below generates a random sample of size 1000 from <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> and we plot a histogram and an overlapping density curve. (See top panel of Figure 7.7.) By an inspection of this graph, one decides if this prior is a reasonable approximation to one’s beliefs about the proportion.</p>
<pre><code>Beta44samples &lt;- rbeta(1000, 4, 4)
Beta29samples &lt;- rbeta(1000, 2, 9)
df1 &lt;- data.frame(P = Beta44samples, Type = &quot;Beta(4, 4)&quot;)
df2 &lt;- data.frame(P = Beta29samples, Type = &quot;Beta(2, 9)&quot;)
df &lt;- rbind(df1, df2)
ggplot(df, aes(P)) +
  geom_histogram(fill = crcblue, color = &quot;white&quot;,
                 bins = 15) +
  facet_wrap(~ Type, ncol = 1) + increasefont()</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-21"></span>
<img src="../LATEX/figures/chapter7/ChooseBeta1_new.png" alt="Histograms of 1000 samples of two Beta density curves Beta(4, 4) and Beta(2, 9)." width="500" />
<p class="caption">
Figure 7.7: Histograms of 1000 samples of two Beta density curves Beta(4, 4) and Beta(2, 9).
</p>
</div>
<p>As a second example, consider a belief that <strong>a priori</strong> there are 2 successes and 9 failures, corresponding to the <span class="math inline">\(\textrm{Beta}(2, 9)\)</span> prior? One can use the <code>rbeta()</code> function take a random sample of 1000 from this prior.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="proportion.html#cb28-1"></a>Beta29samples &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">9</span>)</span></code></pre></div>
<p>Comparing the two distributions, note from Figure 7.7 that <span class="math inline">\(\textrm{Beta}(2, 9)\)</span> favors smaller proportion values than <span class="math inline">\(\textrm{Beta}(4, 4)\)</span>.</p>
<p>To further check the quantiles of the prior, one can use the <code>quantile()</code> function on the simulated draws from the prior. For example, if one wishes to check the middle 50% range of values of <span class="math inline">\(p\)</span> from the random sample of values from <span class="math inline">\(\textrm{Beta}(4, 4)\)</span>, one types</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="proportion.html#cb29-1"></a>Beta44samples &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1000</span>, <span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb29-2"><a href="proportion.html#cb29-2"></a><span class="kw">quantile</span>(Beta44samples, <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.75</span>))</span></code></pre></div>
<pre><code>##       25%       75% 
## 0.3759192 0.6207058</code></pre>
<p>This tells us that the probability that <span class="math inline">\(p \leq 0.366\)</span> is 0.25 and the probability that <span class="math inline">\(p \geq 0.616\)</span> is also 0.25. These probability statements should be checked against one’s prior belief about <span class="math inline">\(p\)</span>. If these quantiles do not seem reasonable, one should make adjustments to the values of the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> .</p>
<p>On the surface the two priors <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> and <span class="math inline">\(\textrm{Beta}(40, 40)\)</span> seem similar in that they both have a mean of <span class="math inline">\(0.5\)</span> and represent similar breakdowns of the success and failure counts. However, the aforementioned concept of prior sample size tells us that <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> has a prior sample size of 8 while that of <span class="math inline">\(\textrm{Beta}(40, 40)\)</span> is 80. As we will see in Section 7.4, the prior sample size determines the strength of the prior (i.e. the confidence level in the prior) and so the <span class="math inline">\(\textrm{Beta}(40, 40)\)</span> prior represents a much stronger belief that <span class="math inline">\(p\)</span> is close to the value 0.5.</p>
<p>A second indirect method of determining a Beta prior is by specification of quantiles of the distribution. Specifically, one determines the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> by first specifying two quantiles of the Beta density curve, and then finding the Beta density curve that matches these quantiles. Suppose the restaurant owner uses his knowledge to specify the 0.5 and 0.9 quantiles of the proportion <span class="math inline">\(p\)</span> as follows.</p>
<ol style="list-style-type: decimal">
<li><p>First, the restaurant owner thinks of a value <span class="math inline">\(p_{50}\)</span> such that the proportion <span class="math inline">\(p\)</span> is equally likely to be smaller or larger than <span class="math inline">\(p_{50}\)</span>. After some thought, he thinks that <span class="math inline">\(p_{50}\)</span> = 0.55.</p></li>
<li><p>Next, the owner thinks of a value <span class="math inline">\(p_{90}\)</span> that he is pretty sure (with probability 0.90) that the proportion <span class="math inline">\(p\)</span> is smaller than <span class="math inline">\(p_{90}\)</span>. After more thought, he decides <span class="math inline">\(p_{90}\)</span> = 0.80.</p></li>
</ol>

<p> One then uses the <code>beta.select()</code> function in the <code>ProbBayes</code> package to find shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> of the Beta density curve that match this information. Each quantile is specified by a list with values <span class="math inline">\(x\)</span> and <span class="math inline">\(p\)</span>. From the output, we see <span class="math inline">\(\textrm{Beta}(3.06, 2.56)\)</span> curve represents the owner’s prior beliefs.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="proportion.html#cb31-1"></a><span class="kw">beta.select</span>(<span class="kw">list</span>(<span class="dt">x =</span> <span class="fl">0.55</span>, <span class="dt">p =</span> <span class="fl">0.5</span>),</span>
<span id="cb31-2"><a href="proportion.html#cb31-2"></a>            <span class="kw">list</span>(<span class="dt">x =</span> <span class="fl">0.80</span>, <span class="dt">p =</span> <span class="fl">0.9</span>))</span></code></pre></div>
<pre><code>## [1] 3.06 2.56</code></pre>
<p>The owner’s Beta density curve is shown here. To make sure this prior is reasonable, the owner should compute several probabilities and quantiles for his prior distribution and see if these values correspond to his opinion.<br />
To illustrate this checking process, Figure 7.8 shows the middle 50% area of the prior distribution. This graph shows that the probability that <span class="math inline">\(p \leq 0.402\)</span> is 0.25 and the probability that <span class="math inline">\(p \geq 0.692\)</span> is also 0.25. If these calculations do not correspond to the owner’s opinion, then maybe some change in the prior distribution would be appropriate.</p>
<div class="figure"><span id="fig:unnamed-chunk-25"></span>
<img src="../LATEX/figures/chapter7/betaprob3.png" alt="Illustration of the middle 50% of a Beta(3.06, 2.56) curve." width="500" />
<p class="caption">
Figure 7.8: Illustration of the middle 50% of a Beta(3.06, 2.56) curve.
</p>
</div>
</div>
</div>
<div id="updating-the-beta-prior" class="section level2">
<h2><span class="header-section-number">7.4</span> Updating the Beta Prior</h2>
<p>In the previous section, we have seen that the restaurant owner thinks that a Beta curve with shape parameters 3.06 and 2.56 is a reasonable reflection of his prior opinion about the proportion of students <span class="math inline">\(p\)</span> whose favorite day for eating out is Friday. Therefore, we work with <span class="math inline">\(\textrm{Beta}(3.06, 2.56)\)</span> as the prior distribution for <span class="math inline">\(p\)</span>.</p>
<p>Now we have the survey results – the survey was administered to 20 students and 12 say that their favorite day for eating out is Friday.
As before in Section 7.2, the likelihood, that is the chance of getting this data if the probability of success is <span class="math inline">\(p\)</span> is given by the Binomial formula,
<span class="math display">\[\begin{eqnarray*}
Likelihood = L(p) = {20 \choose 12} p ^ {12 }(1 - p) ^ 8.
\end{eqnarray*}\]</span></p>
<p>In this section, the Bayes’ rule calculation of the posterior is presented for the continuous prior case and one discovers an interesting result: if one starts with a Beta prior for a proportion <span class="math inline">\(p\)</span>, and the data is Binomial, then the posterior will also be a Beta distribution. The Beta posterior is a natural combination of the information contained in the Beta prior and the Binomial sampling, as one would expect in typical Bayesian inference. This is an illustration of the use of a conjugate prior where the prior and posterior densities are in the same family of distributions.</p>
<div id="bayes-rule-calculation" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Bayes’ rule calculation</h3>
<p>First we demonstrate the Bayes’ rule calculation of the posterior of <span class="math inline">\(p\)</span> through the proportional statement:
<span class="math display" id="eq:priortimeslike2">\[\begin{eqnarray}
\pi(p \mid y) \propto  \pi(p) \times L(p).
\tag{7.10}
\end{eqnarray}\]</span></p>
<p>The prior distribution of <span class="math inline">\(p\)</span>, with density <span class="math inline">\(\pi(p)\)</span>, is Beta with shape parameters <span class="math inline">\(3.06\)</span> and <span class="math inline">\(2.56\)</span>
<span class="math display">\[\begin{eqnarray*}
p \sim \textrm{Beta}(3.06, 2.56).
\end{eqnarray*}\]</span>
The symbol “<span class="math inline">\(\sim\)</span>” is read “follows”, meaning that the random variable before the symbol follows the distribution after the symbol.</p>
<p>For the data/likelihood, we introduce proper notation. Let <span class="math inline">\(Y\)</span> be the random variable of the number of students say that their favorite day for eating out is Friday. We know that the sampling distribution for <span class="math inline">\(Y\)</span> is a Binomial distribution with number of trials <span class="math inline">\(20\)</span> and success probability <span class="math inline">\(p\)</span>. Using the notation of "<span class="math inline">\(\sim\)</span>", we have
<span class="math display">\[\begin{eqnarray*}
Y \sim \textrm{Binomial}(20, p).
\end{eqnarray*}\]</span>
After the value <span class="math inline">\(Y = y\)</span> is observed, <span class="math inline">\(L(p) = f(y \mid p)\)</span> denotes the likelihood, which is the probability of observing this sample value <span class="math inline">\(y\)</span> viewed as a function of the proportion <span class="math inline">\(p\)</span>. (Note that a small letter <span class="math inline">\(y\)</span> is used to denote the actual data observed, as opposed to the random variable <span class="math inline">\(Y\)</span>.) From the dining survey, we know that <span class="math inline">\(y = 12\)</span>.</p>
<p>Now we have the following prior density and the likelihood function.</p>
<ul>
<li><p>The prior distribution:
<span class="math display">\[\begin{eqnarray*}
\pi(p) = \frac{1}{B(3.06, 2.56)}p^{3.06-1}(1-p)^{2.56-1}.
\end{eqnarray*}\]</span></p></li>
<li><p>The likelihood:
<span class="math display">\[\begin{eqnarray*}
f(Y =12 \mid p) = L(p) = {20 \choose 12}p^{12}(1-p)^{8}.
\end{eqnarray*}\]</span></p></li>
</ul>
<p>By Bayes’ rule, the posterior density <span class="math inline">\(\pi(p \mid y)\)</span> is proportional to the product of the prior and the likelihood.<br />
<span class="math display">\[\begin{eqnarray*}
\pi(p \mid y) \propto \pi(p) \times L(p).
\end{eqnarray*}\]</span></p>
<p>Substituting the current prior and likelihood, one can perform the algebra for the posterior density.
<span class="math display" id="eq:betaposterior">\[\begin{eqnarray}
\pi(p \mid Y = 12) &amp;\propto&amp; \pi(p) \times f(Y = 12 \mid p) \nonumber \\
&amp;=&amp;  \frac{1}{B(3.06, 2.56)}p^{3.06-1}(1-p)^{2.56-1} \times \nonumber \\
&amp;&amp; {20 \choose 12}p^{12}(1-p)^{8} \nonumber \\
\texttt{[drop the constants]} &amp;\propto&amp; p^{12}(1-p)^{8}p^{3.06-1}(1-p)^{2.56-1} \nonumber \\
\texttt{[combine the powers]} &amp;=&amp; p^{15.06-1}(1-p)^{10.56-1}. \nonumber \\
\tag{7.11}
\end{eqnarray}\]</span>
One observes that the posterior density of <span class="math inline">\(p\)</span> given <span class="math inline">\(Y = 12\)</span> is, up to a proportionality constant,
<span class="math display">\[\begin{eqnarray*}
\pi(p \mid Y = 12) \propto p^{15.06-1}(1-p)^{10.56-1}.
\end{eqnarray*}\]</span></p>
<p>
Note that in the posterior derivation, the constants <span class="math inline">\({20 \choose 12}\)</span> and <span class="math inline">\(\frac{1}{B(3.06, 2.56)}\)</span> are dropped due to the proportional sign “<span class="math inline">\(\propto\)</span>”. That is, the expression of <span class="math inline">\(\pi(p \mid Y = 12)\)</span> is computed up to some constant. In this case, Appendix A demonstrates the calculation of the constant.</p>
<p>Next, one recognizes if the posterior distribution of <span class="math inline">\(p\)</span> is recognizable as a member of a familiar family of distributions. In the computation of the posterior, we have intentionally kept the expression of "<span class="math inline">\(-1\)</span>" in the powers of <span class="math inline">\(p\)</span> and <span class="math inline">\(1-p\)</span> terms, instead of using <span class="math inline">\(14.06\)</span> and <span class="math inline">\(9.56\)</span> directly. By doing this, one recognizes that the posterior density has the familiar form
<span class="math display">\[\begin{eqnarray*}
p^{a-1}(1-p)^{b-1}.
\end{eqnarray*}\]</span>
As the reader might have guessed, the posterior distribution turns out to be a Beta distribution with updated shape parameters. That is, the posterior distribution of <span class="math inline">\(p\)</span> given <span class="math inline">\(Y = 12\)</span> is Beta with parameters 15.06 and 10.56.</p>
</div>
<div id="from-beta-prior-to-beta-posterior" class="section level3">
<h3><span class="header-section-number">7.4.2</span> From Beta prior to Beta posterior</h3>
<p>The results about a proportion <span class="math inline">\(p\)</span> from the Bayes’ rule calculation performed in Section 7.4.1 can be generalized.
Suppose one works with the following prior distribution and sampling density:</p>
<ul>
<li><p>The prior distribution:
<span class="math display">\[\begin{eqnarray*}
p \sim \textrm{Beta}(a, b)
\end{eqnarray*}\]</span></p></li>
<li><p>The sampling density:
<span class="math display">\[\begin{eqnarray*}
Y \sim \textrm{Binomial}(n, p)
\end{eqnarray*}\]</span></p></li>
</ul>
<p>One observes the count <span class="math inline">\(Y = y\)</span>, the number of successes in the collected data.
Then the posterior distribution of <span class="math inline">\(p\)</span> is another Beta distribution with shape parameters <span class="math inline">\(a + y\)</span> and <span class="math inline">\(b + n - y\)</span>.</p>
<ul>
<li>The posterior distribution:
<span class="math display" id="eq:betaposterior2">\[\begin{eqnarray}
p \mid Y = y \sim \textrm{Beta}(a + y, b + n - y)
\tag{7.12}
\end{eqnarray}\]</span></li>
</ul>
<p>The two shape parameters of the Beta posterior distribution, <span class="math inline">\(a + y\)</span> and <span class="math inline">\(b + n - y\)</span>, are the sums of the prior and data/likelihood counts of successes and failures, respectively. We algebraically combine the shape parameters of the Beta prior and the Binomial likelihood to obtain the shape parameters of the posterior Beta distribution.</p>
<p>Table 7.1 demonstrates this process with three rows labelled Prior, Data/Likelihood and Posterior. The Prior row contains the shape parameters of the Beta prior <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in the Successes and Failures columns, respectively. The Data/Likelihood row contains the number of successes <span class="math inline">\(y\)</span> and the number of failures <span class="math inline">\(n - y\)</span>. The shape parameters of the Beta posterior are found by adding the prior parameter values and the data values.</p>
<p>Table 7.1. Updating the Beta prior.</p>
<table>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">Successes</th>
<th align="center">Failures</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Prior</td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="even">
<td align="center">Data/Likelihood</td>
<td align="center"><span class="math inline">\(y\)</span></td>
<td align="center"><span class="math inline">\(n-y\)</span></td>
</tr>
<tr class="odd">
<td align="center">Posterior</td>
<td align="center"><span class="math inline">\(a + y\)</span></td>
<td align="center"><span class="math inline">\(b + n - y\)</span></td>
</tr>
</tbody>
</table>

<p>In the following R script we update the Beta shape parameters. We see that the owner’s posterior distribution for <span class="math inline">\(p\)</span> is Beta with shape parameters 15.06 and 10.56.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="proportion.html#cb33-1"></a>ab &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">3.06</span>, <span class="fl">2.56</span>)</span>
<span id="cb33-2"><a href="proportion.html#cb33-2"></a>yny &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">12</span>, <span class="dv">8</span>)</span>
<span id="cb33-3"><a href="proportion.html#cb33-3"></a>(ab_new &lt;-<span class="st"> </span>ab <span class="op">+</span><span class="st"> </span>yny)</span></code></pre></div>
<pre><code>## [1] 15.06 10.56</code></pre>
<p>The function <code>beta_prior_post()</code> in the <code>ProbBayes</code> R package plots the prior and posterior Beta curves together on one graph, see Figure 7.9.</p>
<pre><code>beta_prior_post(ab, ab_new)</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-27"></span>
<img src="../LATEX/figures/chapter7/betapriorpost.png" alt="Prior and posterior curves for the proportion of students who prefer to dine out on Friday." width="500" />
<p class="caption">
Figure 7.9: Prior and posterior curves for the proportion of students who prefer to dine out on Friday.
</p>
</div>
<p>Comparing the two Beta curves, several observations can be made.</p>
<ul>
<li><p>One can compare the prior and posterior Beta curves using the respective means. The mean of a <span class="math inline">\(\textrm{Beta}(a, b)\)</span> distribution is <span class="math inline">\(\frac{a}{a+b}\)</span>. Using this formula, the posterior mean of <span class="math inline">\(p\)</span> is 15.06 / (15.06 + 10.56) = 0.588 which is slightly larger than the prior mean 3.06 / (30.6 + 2.56) = 0.544. Recall that the sample proportion from the survey results is <span class="math inline">\(12/20 = 0.6\)</span>. The posterior mean lies between the prior mean and sample mean and it is closer to the sample mean.</p></li>
<li><p>Next one compares the spreads of the two curves. One sees a much wider spread of the prior Beta curve (dashed line) than that of the posterior Beta curve (solid line). Initially the owner was unsure about the proportion of students favoring Friday to dine out. After observing the results of the survey, the solid posterior curve indicates that he is more certain that <span class="math inline">\(p\)</span> is between 0.5 and 0.7. This sheds light on a general feature of Bayesian inference: the data helps sharpen the belief about the parameter of interest, producing a posterior distribution with a smaller spread than the prior distribution.</p></li>
</ul>
<p>The attractive combination of a Beta prior and a Binomial sampling density to obtain a posterior motivates a definition of conjugate priors. If the prior distribution and the posterior distribution come from the same family of distributions, the prior is then called a conjugate prior. Here a Beta is a conjugate prior for a success probability <span class="math inline">\(p\)</span>, since the posterior distribution for <span class="math inline">\(p\)</span> is also in the Beta family.
Conjugate priors are specific to the choice of sampling density. For example, a Beta prior is conjugate with Binomial sampling, but not to Normal sampling which is popular for continuous outcome. In Chapter 8 we will discover the conjugate prior distribution for a Normal sampling distribution.</p>
<p>Conjugate priors are desirable because they simplify the Bayesian inference procedure. In the dining preference example, when a <span class="math inline">\({\rm Beta}(3.06, 2.56)\)</span> prior is assigned to <span class="math inline">\(p\)</span>, the posterior is <span class="math inline">\({\rm Beta}(15.06, 10.56)\)</span> and inference about <span class="math inline">\(p\)</span> is made in a straightforward way. One can easily plot the he prior and posterior Beta distributions as in Figure 7.9. One can also make precise comparative statements about the locations of the prior and posterior distribution using quantiles of a Beta curve.</p>
<p>Although conjugate priors are convenient and straightforward to use, they may not be appropriate for use in a Bayesian analysis. One should choose a prior that fits one’s belief, not one that is convenient to use. In some situations it may be appropriate to choose a prior distribution that does not provide conjugacy. In Chapter 9, we will describe computational methods to facilitate posterior inferences when non-conjugate priors are used. Modern Bayesian posterior computations accommodate a wide variety of choices of prior and sampling distributions. Therefore it is more important to choose a prior that matches one’s prior belief than choosing a prior that is computationally convenient.</p>
</div>
</div>
<div id="bayesian-inferences-with-continuous-priors" class="section level2">
<h2><span class="header-section-number">7.5</span> Bayesian Inferences with Continuous Priors</h2>
<p>We will continue with the dining preference example to illustrate different types of Bayesian inference. The restaurant owner has taken his dining survey and the posterior distribution <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span> reflects his opinion about the proportion <span class="math inline">\(p\)</span> of students whose favorite day for eating out is Friday.</p>
<p>All Bayesian inferences about the proportion <span class="math inline">\(p\)</span> are based on various summaries of this posterior Beta distribution. The summary we compute from the posterior will depend on the type of inference. We will focus on three types of inference: (1) testing problems where one is interested in assessing the likelihood of some values of <span class="math inline">\(p\)</span>, (2) interval estimations where one wants to find an interval that is likely to contain <span class="math inline">\(p\)</span>, and (3) Bayesian prediction where one wants to learn about new observation(s) in the future.</p>
<p>Simulation will be incorporated for all three types of Bayesian inference problems. Since one has a conjugate prior distribution, one can derive the exact posterior distribution (a Beta) and inferences are performed with the exact posterior Beta distribution. In other situations when conjugacy is not available, meaning that no exact representation of the posterior is available, inferences through simulation are much more widely used. It is instructive to present the exact solutions and the approximated simulation-based solutions together, so one learns through practice and prepare for future use of simulation in other settings.</p>
<p>There is nothing magic about simulation. In fact, simulation has been used earlier, when the <code>rbeta()</code> function was used to generate simulated samples from <span class="math inline">\(\textrm{Beta}(4, 4)\)</span> and <span class="math inline">\(\textrm{Beta}(2, 9)\)</span> and check the appropriateness of the chosen Beta prior (review Section 7.3.2} as needed). Information on simulation and the relevant R code will be introduced in the description of each inferential problem.</p>
<div id="bayesian-hypothesis-testing" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Bayesian hypothesis testing</h3>
<p>Suppose one of the restaurant workers claims that at least 75% of the students prefer to eat out on Friday. Is this a reasonable claim?</p>
<p>In traditional classical statistics, one might be interested in testing the hypothesis <span class="math inline">\(H: p \ge 0.75\)</span>.<br />
From a Bayesian viewpoint, it is straightforward to implement this test. Since the hypothesis is an interval of values, one finds the posterior probability that <span class="math inline">\(p \ge 0.75\)</span> and makes a decision based on the value of this probability. If the probability is small, one rejects this claim.</p>

<p> First the exact solution will be presented. Since the posterior distribution is <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span>, the owner’s posterior density is graphed and the area under the curve for values of <span class="math inline">\(p\)</span> between 0.75 and 1 is found. The <code>beta_area()</code> function is used to display and show the area; see Figure 7.10. Since the probability is only about 4%, one rejects the worker’s claim that <span class="math inline">\(p\)</span> is at least 0.75.</p>
<pre><code>beta_area(lo = 0.75, hi = 1.0,
          shape_par = c(15.06, 10.56))</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-28"></span>
<img src="../LATEX/figures/chapter7/betapost1.png" alt="Probability of the hypothesis from the Beta posterior density." width="500" />
<p class="caption">
Figure 7.10: Probability of the hypothesis from the Beta posterior density.
</p>
</div>
<p>This computation can be implemented using simulation. Since the posterior distribution is <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span>, one generates a large number of random values from this Beta distribution, then summarizes the sample of simulated draws to obtain the probability of <span class="math inline">\(p \geq 0.75\)</span>. First a sample of <span class="math inline">\(S = 1000\)</span> from the Beta posterior is taken, storing the results in the vector <code>BetaSamples</code>.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="proportion.html#cb37-1"></a>S &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb37-2"><a href="proportion.html#cb37-2"></a>BetaSamples &lt;-<span class="st"> </span><span class="kw">rbeta</span>(S, <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span></code></pre></div>
<p>The proportion of the 1000 simulated values of <span class="math inline">\(p\)</span> that are at least 0.75 gives an approximation of the probability that <span class="math inline">\(p \geq 0.75\)</span>.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="proportion.html#cb38-1"></a><span class="kw">sum</span>(BetaSamples <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.75</span>) <span class="op">/</span><span class="st"> </span>S</span></code></pre></div>
<pre><code>## [1] 0.048</code></pre>
<p>The simulation-based probability estimate is 0.037 which is an accurate approximation to the exact probability 0.04 obtained before.</p>
<p>It would be reasonable to question the choice of the number of simulations <span class="math inline">\(S = 1000\)</span>. One can change the simulation sample size to larger or smaller values as one sees fit. In general, the larger the value of <span class="math inline">\(S\)</span>, the more accurate the approximation. Figure 7.11 shows that the shape of a histogram of the simulated values of <span class="math inline">\(p\)</span> approaches the exact posterior density as the value of <span class="math inline">\(S\)</span> changes from 100 to 10,000. The corresponding simulation-based probabilities of <span class="math inline">\(p \geq 0.75\)</span> are <span class="math inline">\(\{0.02, 0.05, 0.033, 0.0422\}\)</span> indicating that the accuracy of the approximation improves for larger simulation sample sizes.</p>
<div class="figure"><span id="fig:unnamed-chunk-31"></span>
<img src="../LATEX/figures/chapter7/4Svalues_new.png" alt="Histograms of simulated draws from Beta(15.06, 10.56) with the exact Beta density overlaid for four different number of samples drawn where $S$ = {10, 500, 1000, 10000}." width="500" />
<p class="caption">
Figure 7.11: Histograms of simulated draws from Beta(15.06, 10.56) with the exact Beta density overlaid for four different number of samples drawn where <span class="math inline">\(S\)</span> = {10, 500, 1000, 10000}.
</p>
</div>
<p>One will observe variation from one simulation from another (see the two different but similar approximated probabilities 0.037 and 0.033 when <span class="math inline">\(S = 1000\)</span>). To replicate one’s results one specifies the seed of the random number simulator <code>set.seed()</code>. Choose any number that you like to put in – if this <code>set.seed()</code> line of code is executed first, then the same sequence of random values will be generated and one replicates the simulation-based computation.</p>
</div>
<div id="bayesian-credible-intervals" class="section level3">
<h3><span class="header-section-number">7.5.2</span> Bayesian credible intervals</h3>
<p>Another type of inference is a <em>Bayesian credible interval</em>, an interval that one is confident contains <span class="math inline">\(p\)</span>. Such an interval provides an uncertainty estimate for the parameter <span class="math inline">\(p\)</span>. A 90% Bayesian credible interval is an interval that contains 90% of the posterior probability.</p>

<p>One convenient 90% credible interval is the “equal tails” interval that contains the middle 90% of the probability content. The function <code>beta_interval()</code> in <code>ProbBayes</code> R package illustrates and computes the equal-tails interval. The shaded area in Figure 7.12 corresponds to <span class="math inline">\(90\%\)</span> of the posterior probability. The probability <span class="math inline">\(p\)</span> falls between 0.427 and 0.741 is exactly 90 percent.</p>
<pre><code>beta_interval(0.9, c(15.06, 10.56))</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-32"></span>
<img src="../LATEX/figures/chapter7/betapost2.png" alt="Display of 90% probability interval for the proportion $p$." width="500" />
<p class="caption">
Figure 7.12: Display of 90% probability interval for the proportion <span class="math inline">\(p\)</span>.
</p>
</div>
<p>One obtains this middle 90% credible interval using the <code>qbeta()</code> function.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="proportion.html#cb41-1"></a><span class="kw">qbeta</span>(<span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>), <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span></code></pre></div>
<pre><code>## [1] 0.4266788 0.7410141</code></pre>
<p>This Bayesian credible interval differs from the interpretation of a traditional confidence interval. With a traditional confidence interval, one does not have confidence that one particular interval will contain <span class="math inline">\(p\)</span>. Instead 90% confidence refers to the average coverage of the interval in repeated sampling.</p>
<p>Other types of Bayesian credible intervals can be computed. For example, instead of a credible interval covering the middle 90% of the posterior probability, one could create a credible interval covers the lower 90%, or the upper 90%, or the middle 95%. The <code>qbeta()</code> function is helpful in achieving all of these different type of intervals, as long as we know the exact posterior distribution, that is, the two shape parameters of the posterior Beta distribution. For example, the following code computes a credible interval that covers the lower 90% of the posterior distribution.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="proportion.html#cb43-1"></a><span class="kw">qbeta</span>(<span class="kw">c</span>(<span class="fl">0.00</span>, <span class="fl">0.90</span>), <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span></code></pre></div>
<pre><code>## [1] 0.0000000 0.7099912</code></pre>
<p>An alternative way of creating credible intervals is by simulation. One first takes a random sample from the <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span> distribution, then summarizes the simulated values by finding the two cutoff points of the middle 90% of the sample. The <code>quantile()</code> function is useful for this purpose. As a demonstration, below we simulate <span class="math inline">\(S = 1000\)</span> proportion values and compute the credible interval.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="proportion.html#cb45-1"></a>S &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb45-2"><a href="proportion.html#cb45-2"></a>BetaSamples &lt;-<span class="st"> </span><span class="kw">rbeta</span>(S, <span class="fl">15.06</span>, <span class="fl">10.56</span>)</span>
<span id="cb45-3"><a href="proportion.html#cb45-3"></a><span class="kw">quantile</span>(BetaSamples, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span></code></pre></div>
<pre><code>##        5%       95% 
## 0.4181172 0.7311519</code></pre>
<p>The approximate middle 90% credible interval is [0.427, 0.733], which is close in value to the exact 90% credible interval [0.427, 0.741] computed using the <code>qbeta()</code> and <code>beta_interval()</code> functions. In an end-of-chapter exercise the reader is encouraged to practice and experiment with different values of the size of the simulated sample <span class="math inline">\(S\)</span>.</p>
</div>
<div id="bayesian-prediction" class="section level3">
<h3><span class="header-section-number">7.5.3</span> Bayesian prediction</h3>
<p>Prediction is a typical task of Bayesian inference and statistical inference in general. Once we are able to make inference about the parameter in our statistical model, one may be interested in predicting future observations.</p>
<p>Denote a new observation by the random variable <span class="math inline">\(\tilde{Y}\)</span>. In particular, if the new survey is given to <span class="math inline">\(m\)</span> students, the random variable <span class="math inline">\(\tilde{Y}\)</span> is the number of students preferring Friday to dine out out of the <span class="math inline">\(m\)</span> respondents. If again the survey is given to a random sample, the random variable <span class="math inline">\(\tilde{Y}\)</span>, conditional on <span class="math inline">\(p\)</span>, follows a Binomial distribution with the fixed total number of trails <span class="math inline">\(m\)</span> and success probability <span class="math inline">\(p\)</span>. One’s knowledge about the location of <span class="math inline">\(p\)</span> is expressed by the posterior distribution of <span class="math inline">\(p\)</span>.</p>
<p>Mathematically, to make a prediction of a new observation, one is asking for the distribution of <span class="math inline">\(\tilde{Y}\)</span> given the observed data <span class="math inline">\(Y = y\)</span>. That is, one is interested in the probability function <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid Y = y)\)</span> where <span class="math inline">\(\tilde y\)</span> is a value of <span class="math inline">\(\tilde{Y}\)</span>.
But the conditional distribution of <span class="math inline">\(\tilde{Y}\)</span> given a value of the proportion <span class="math inline">\(p\)</span> is Binomial(<span class="math inline">\(m, p\)</span>) and the current beliefs about <span class="math inline">\(p\)</span> are described by the posterior density. So one writes the joint density of <span class="math inline">\(\tilde{Y}\)</span> and <span class="math inline">\(p\)</span> as the product
<span class="math display" id="eq:binpred1">\[\begin{eqnarray}
f(\tilde{Y}= \tilde{y},  p \mid Y = y) = f(\tilde{Y} = \tilde{y} \mid p) \pi(p \mid Y = y).
\tag{7.13}
\end{eqnarray}\]</span>
By integrating out <span class="math inline">\(p\)</span>, one obtains the predictive distribution
<span class="math display" id="eq:binpred2">\[\begin{eqnarray}
f(\tilde{Y} = \tilde{y} \mid Y = y) = \int  f(\tilde{Y} =\tilde{y} \mid p) \pi(p \mid Y = y) dp.
\label{eq:Binomial:pred}
\tag{7.14}
\end{eqnarray}\]</span></p>
<p>The density of <span class="math inline">\(\tilde{Y}\)</span> given <span class="math inline">\(p\)</span> is Binomial with <span class="math inline">\(m\)</span> trials and success probability <span class="math inline">\(p\)</span>, and the posterior density of <span class="math inline">\(p\)</span> is <span class="math inline">\({\rm Beta}(a + y, b + n - y)\)</span>. After the substitution of densities and an integration step (see Appendix B for the detail), one finds that the predictive density is given by
<span class="math display" id="eq:binpred3">\[\begin{eqnarray}
f(\tilde{Y} =\tilde{y}  \mid Y = y) &amp;=&amp; {m \choose \tilde{y}}
\frac{B(a + y + \tilde{y}, b  + n - y + m - \tilde{y})}{B(a + y, b + n - y)}. \nonumber \\
\tag{7.15}
\end{eqnarray}\]</span>
This is the Beta-Binomial distribution with parameters <span class="math inline">\(m\)</span>, <span class="math inline">\(a + y\)</span> and <span class="math inline">\(b + n - y\)</span>.
<span class="math display" id="eq:binpred4">\[\begin{eqnarray}
\tilde{Y} \mid Y = y \sim \textrm{Beta-Binomial}(m, a + y, b + n - y).
\tag{7.16}
\end{eqnarray}\]</span>
To summarize, Bayesian prediction of a new observation is a Beta-Binomial distribution where <span class="math inline">\(m\)</span> is the number of trials in the new sample, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are shape parameters from the Beta prior, and <span class="math inline">\(y\)</span> and <span class="math inline">\(n\)</span> are quantities from the data/likelihood.</p>

<p>Using this Beta-Binomial distribution in our example, one computes the predictive probability that <span class="math inline">\(\tilde y\)</span> students prefer Friday in a new survey of 20 students. We illustrate the use of the <code>pbetap()</code> function from the <code>ProbBayes</code> package. The inputs to <code>pbetap()</code> are the vector of Beta shape parameters <span class="math inline">\((a, b)\)</span>, the sample size 20, and the values of <span class="math inline">\(\tilde{y}\)</span> of interest.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="proportion.html#cb47-1"></a>a &lt;-<span class="st"> </span><span class="fl">15.06</span></span>
<span id="cb47-2"><a href="proportion.html#cb47-2"></a>b &lt;-<span class="st"> </span><span class="fl">10.56</span></span>
<span id="cb47-3"><a href="proportion.html#cb47-3"></a>prob &lt;-<span class="st"> </span><span class="kw">pbetap</span>(<span class="kw">c</span>(a, b), <span class="dv">20</span>, <span class="dv">0</span><span class="op">:</span><span class="dv">20</span>)</span>
<span id="cb47-4"><a href="proportion.html#cb47-4"></a>pred_distribution &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">20</span>, </span>
<span id="cb47-5"><a href="proportion.html#cb47-5"></a>                                <span class="dt">Probability =</span> prob)</span></code></pre></div>
<pre><code>prob_plot(pred_distribution,
          Color = crcblue, Size = 4) +
  theme(text=element_text(size=18))</code></pre>
<div class="figure"><span id="fig:unnamed-chunk-37"></span>
<img src="../LATEX/figures/chapter7/Predictive_dining.png" alt="Display of the exact predictive distribution of the number of students $\tilde y$ favoring Friday in a future sample of 20." width="500" />
<p class="caption">
Figure 7.13: Display of the exact predictive distribution of the number of students <span class="math inline">\(\tilde y\)</span> favoring Friday in a future sample of 20.
</p>
</div>
<p>These predictive probabilities are displayed in Table 7.2  and graphed in Figure 7.13.</p>
<p>Table 7.2. Predictive
distribution of the number of students preferring Friday in a future
sample of 20.</p>
<table>
<thead>
<tr class="header">
<th align="right">Y</th>
<th align="right">Probability</th>
<th align="right">Y</th>
<th align="right">Probability</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0</td>
<td align="right">0</td>
<td align="right">11</td>
<td align="right">0.127</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">12</td>
<td align="right">0.134</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">0</td>
<td align="right">13</td>
<td align="right">0.127</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">0.001</td>
<td align="right">14</td>
<td align="right">0.108</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">0.004</td>
<td align="right">15</td>
<td align="right">0.080</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="right">0.010</td>
<td align="right">16</td>
<td align="right">0.052</td>
</tr>
<tr class="odd">
<td align="right">6</td>
<td align="right">0.021</td>
<td align="right">17</td>
<td align="right">0.028</td>
</tr>
<tr class="even">
<td align="right">7</td>
<td align="right">0.037</td>
<td align="right">18</td>
<td align="right">0.012</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="right">0.059</td>
<td align="right">19</td>
<td align="right">0.004</td>
</tr>
<tr class="even">
<td align="right">9</td>
<td align="right">0.085</td>
<td align="right">20</td>
<td align="right">0.001</td>
</tr>
<tr class="odd">
<td align="right">10</td>
<td align="right">0.109</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Looking at the table, the most likely number of students preferring Friday is 12. Just as in the inference situation, it is desirable to construct an interval that will contain <span class="math inline">\(\tilde Y\)</span> with a high probability. Suppose the desired probability content is 0.90. One constructs this prediction interval by putting in the most likely values of <span class="math inline">\(\tilde Y\)</span> until the probability content of the set exceeds 0.90.</p>

<p>This method is implemented using the following command:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="proportion.html#cb49-1"></a><span class="kw">discint</span>(pred_distribution, <span class="fl">.9</span>)</span></code></pre></div>
<pre><code>## $prob
## [1] 0.9185699
## 
## $set
##  [1]  7  8  9 10 11 12 13 14 15 16</code></pre>
<p>One therefore finds that
<span class="math display">\[
Prob(7 \le \tilde Y \le 16) = 0.919.
\]</span></p>
<p>This exact predictive distribution is based on the posterior distribution of <span class="math inline">\(p\)</span>, as one uses <span class="math inline">\(\pi(p \mid Y=y)\)</span> in the integration process in Equation (7.14). For that reason this predictive distribution is called the <em>posterior predictive distribution</em>. There also exists a <em>prior predictive distribution</em>, a topic we will briefly introduce in Section 7.6.</p>
<p>In situations where it is difficult to derive the exact predictive distribution, one simulates values from this distribution. One implements this predictive simulation by first simulating draws of the parameter (in this case the proportion <span class="math inline">\(p\)</span>) from its posterior distribution, and then simulating values of the future observation (e.g. the new observation <span class="math inline">\(\tilde{Y}\)</span>) from the sampling density (here the Binomial distribution).</p>
<p>We illustrate this simulation procedure with the generic Beta posterior <span class="math inline">\(\textrm{Beta}(a + y, b + n - y)\)</span>. To simulate a single draw from the predictive distribution, one first simulates a single proportion value <span class="math inline">\(p\)</span> from the Beta posterior and then simulates a new data point <span class="math inline">\(\tilde{y}\)</span> (the number of successes out of <span class="math inline">\(m\)</span> trials) from a Binomial distribution with sample size <span class="math inline">\(m\)</span> and probability of success given by the simulated draw of <span class="math inline">\(p\)</span>.
<span class="math display">\[\begin{eqnarray*}
\text{sample}\,\, p \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{Y} \sim {\rm{Binomial}}(m, p)
\end{eqnarray*}\]</span></p>

<p> This process of simulating a single draw is implemented by the <code>rbeta()</code> and <code>rbinom()</code> functions. Let <span class="math inline">\(m = n\)</span> (the size of the future sample is the same as the size of the observed sample).</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="proportion.html#cb51-1"></a>a &lt;-<span class="st"> </span><span class="fl">3.06</span>; b &lt;-<span class="st"> </span><span class="fl">2.56</span></span>
<span id="cb51-2"><a href="proportion.html#cb51-2"></a>n &lt;-<span class="st"> </span><span class="dv">20</span>; y &lt;-<span class="st"> </span><span class="dv">12</span></span>
<span id="cb51-3"><a href="proportion.html#cb51-3"></a>pred_p_sim &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dv">1</span>, a <span class="op">+</span><span class="st"> </span>y, b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span>y)</span>
<span id="cb51-4"><a href="proportion.html#cb51-4"></a>(pred_y_sim &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1</span>, n, pred_p_sim))</span></code></pre></div>
<pre><code>## [1] 7</code></pre>
<p>Due to the ability of R to work easily with vectors, ]the same code is essentially used for simulating <span class="math inline">\(S = 1000\)</span> draws from the predictive distribution.<br />
In the following R script, <code>pred_p_sim</code> contains 1000 simulated draws from the posterior, and for each element of this posterior sample, the <code>rbinom()</code> function is used to simulate a corresponding value of <span class="math inline">\(\tilde Y\)</span> from the Binomial sampling density.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="proportion.html#cb53-1"></a>a &lt;-<span class="st"> </span><span class="fl">3.06</span>; b &lt;-<span class="st"> </span><span class="fl">2.56</span></span>
<span id="cb53-2"><a href="proportion.html#cb53-2"></a>n &lt;-<span class="st"> </span><span class="dv">20</span>; y &lt;-<span class="st"> </span><span class="dv">12</span></span>
<span id="cb53-3"><a href="proportion.html#cb53-3"></a>S &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb53-4"><a href="proportion.html#cb53-4"></a>pred_p_sim &lt;-<span class="st"> </span><span class="kw">rbeta</span>(S, a <span class="op">+</span><span class="st"> </span>y, b <span class="op">+</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span>y)</span>
<span id="cb53-5"><a href="proportion.html#cb53-5"></a>pred_y_sim &lt;-<span class="st"> </span><span class="kw">rbinom</span>(S, n, pred_p_sim)</span></code></pre></div>
<p>Figure 7.14 displays predictive probabilities for the number of students who prefer Fridays using the exact Beta-Binomial and simulation methods. One observes good agreement using these two computation methods.</p>
<div class="figure"><span id="fig:unnamed-chunk-41"></span>
<img src="../LATEX/figures/chapter7/two_predictive_dist.png" alt="Display of the exact and simulated predictive probabilities for dining example." width="500" />
<p class="caption">
Figure 7.14: Display of the exact and simulated predictive probabilities for dining example.
</p>
</div>
<p>For example, using the simulated values of <span class="math inline">\(\tilde Y\)</span> one finds that
<span class="math display">\[
Prob(6 \le \tilde Y \le 15) = 0.927
\]</span>
which is close in value to the range <span class="math inline">\(Prob(7 \le \tilde Y \le 16) = 0.919\)</span> found using the exact predictive distribution.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="proportion.html#cb54-1"></a>a &lt;-<span class="st"> </span><span class="fl">15.06</span></span>
<span id="cb54-2"><a href="proportion.html#cb54-2"></a>b &lt;-<span class="st"> </span><span class="fl">10.56</span></span>
<span id="cb54-3"><a href="proportion.html#cb54-3"></a>prob &lt;-<span class="st"> </span><span class="kw">pbetap</span>(<span class="kw">c</span>(a, b), <span class="dv">20</span>, <span class="dv">0</span><span class="op">:</span><span class="dv">20</span>)</span>
<span id="cb54-4"><a href="proportion.html#cb54-4"></a>pred_distribution &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Y =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">20</span>, </span>
<span id="cb54-5"><a href="proportion.html#cb54-5"></a>                                <span class="dt">Probability =</span> prob)</span>
<span id="cb54-6"><a href="proportion.html#cb54-6"></a><span class="kw">discint</span>(pred_distribution, <span class="fl">.9</span>)</span></code></pre></div>
<pre><code>## $prob
## [1] 0.9185699
## 
## $set
##  [1]  7  8  9 10 11 12 13 14 15 16</code></pre>
</div>
</div>
<div id="predictive-checking" class="section level2">
<h2><span class="header-section-number">7.6</span> Predictive Checking</h2>
<p>In the previous section, the use of the predictive distribution has been illustrated in learning about future data. This is more precisely described as the posterior predictive density as one is obtaining this density by integrating the sampling density <span class="math inline">\(f(\tilde Y = \tilde y \mid p)\)</span> over the posterior density <span class="math inline">\(\pi(p \mid y)\)</span>.</p>
<p>The prior predictive density is also useful in model checking. In a Bayesian model where <span class="math inline">\(p\)</span> has a prior <span class="math inline">\(\pi(p)\)</span> and <span class="math inline">\(Y\)</span> has a sampling density <span class="math inline">\(f(Y = y \mid p)\)</span>, one writes the joint density of <span class="math inline">\((p, Y)\)</span> as the product of the sampling density and the prior:
<span class="math display" id="eq:priorpred1">\[\begin{eqnarray}
f(p, Y = y) = f(Y = y \mid p) \pi(p).
\tag{7.17}
\end{eqnarray}\]</span>
Suppose one conditions on <span class="math inline">\(y\)</span> instead of <span class="math inline">\(p\)</span> and then one obtains an alternative representation of the joint density:
<span class="math display" id="eq:priorpred2">\[\begin{eqnarray}
f(p, Y = y) = \pi(p \mid Y = y) f(Y = y).
\tag{7.18}
\end{eqnarray}\]</span>
The first term in this product, the density <span class="math inline">\(\pi(p \mid Y = y)\)</span>, is the posterior density of <span class="math inline">\(p\)</span> given the observation <span class="math inline">\(y\)</span>; this density is useful for performing inference about the proportion. The second term in this product is the density <span class="math inline">\(f(Y = y)\)</span> is the prior predictive density – this represents the density of future data before the observation <span class="math inline">\(y\)</span> is taken. If the actual observation denoted by <span class="math inline">\(y_{obs}\)</span> is not consistent with the prior predictive density <span class="math inline">\(f(Y = y)\)</span>, this indicates some problem with the Bayesian model. Basically, this says that the observed data is unlikely to happen if one simulates predictions of data from our model.</p>
<p>To illustrate the use of prior predictive checking, recall that the restaurant owner assigned a Beta(3.06, 2.56) prior to the proportion <span class="math inline">\(p\)</span> of students dining on Friday. A sample of 20 students will be taken. Based on this information, one computes the predictive probability <span class="math inline">\(f(Y = y)\)</span> of <span class="math inline">\(y\)</span> students preferring Friday dining of the sample of 20. This predictive distribution for all possible values of <span class="math inline">\(y\)</span> is displayed in Figure 7.15. Recall that we actually observed <span class="math inline">\(y_{obs} = 12\)</span> Friday diners — this value is shown in Figure 7.15 as a large black dot. This value is in the middle of the distribution – the takeaway is that the observed data is consistent with predictions from the owner’s Bayesian model.</p>
<div class="figure"><span id="fig:unnamed-chunk-43"></span>
<img src="../LATEX/figures/chapter7/predcheck1.png" alt="Prior predictive distribution of $y$ using the owner's Beta prior.  The observed number of $y$ id indicated with a large black dot.  In this case the observed data is consistent with the Bayesian model." width="500" />
<p class="caption">
Figure 7.15: Prior predictive distribution of <span class="math inline">\(y\)</span> using the owner’s Beta prior. The observed number of <span class="math inline">\(y\)</span> id indicated with a large black dot. In this case the observed data is consistent with the Bayesian model.
</p>
</div>
<p>In contrast, suppose another restaurant worker is more pessimistic about the likelihood of students dining on Friday. This worker’s prior median of the proportion <span class="math inline">\(p\)</span> is 0.2 and her 90th percentile is 0.4 — this information is matched with a Beta prior with shape parameters 2.07 and 7.32. Figure 7.16 displays the predictive density of the number of Friday diners of a sample of 20 using this worker’s prior. Here one reaches a different conclusion. The observed number 12 of Friday diners is in the tail of this predictive distribution — this observation is not consistent with predictions from the Bayesian model. In closer examination, one sees conflict between the information in the worker’s prior and the data — her prior said that the proportion <span class="math inline">\(p\)</span> was close to 0.20 and the data result (12 out of 20 successes) indicates that the proportion is close to 0.60. Predictive checking is helpful in this case in detecting this prior/data conflict.</p>
<div class="figure"><span id="fig:unnamed-chunk-44"></span>
<img src="../LATEX/figures/chapter7/predcheck2.png" alt="Prior predictive distribution of $y$ using a worker's Beta prior.  The observed number of $y$ is indicated by a large black dot.  In this case the observed data is not consistent with the Bayesian model." width="500" />
<p class="caption">
Figure 7.16: Prior predictive distribution of <span class="math inline">\(y\)</span> using a worker’s Beta prior. The observed number of <span class="math inline">\(y\)</span> is indicated by a large black dot. In this case the observed data is not consistent with the Bayesian model.
</p>
</div>
<div id="comparing-bayesian-models" class="section level3">
<h3><span class="header-section-number">7.6.1</span> Comparing Bayesian models</h3>
<p>The prior predictive distribution is also useful in comparing two Bayesian models. To illustrate model comparison, suppose a second worker at the restaurant is also asked about the fraction of students who dine on Friday. He knows that the owner’s belief about the proportion <span class="math inline">\(p\)</span> is described by a Beta(3.06, 2.56) density, and the fellow worker’s belief about <span class="math inline">\(p\)</span> is represented by a Beta(2.07, 7.32) density. Who should the second worker believe?</p>
<p>Suppose this second worker believes that both the owner’s and fellow worker’s beliefs about the proportion <span class="math inline">\(p\)</span> are equally plausible. So he places a probability of 0.5 on the Beta(3.06, 2.56) prior and a probability of 0.5 on the Beta(2.07, 7.32) prior. This second worker’s prior <span class="math inline">\(\pi(p)\)</span> is written as the mixture
<span class="math display" id="eq:bayescompare1">\[\begin{eqnarray}
\pi(p) = q \pi_1(p) + (1 - q) \pi_2(p),
\tag{7.19}
\end{eqnarray}\]</span>
where <span class="math inline">\(q = 0.5\)</span> and <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> denote the owner’s and worker’s Beta priors.</p>
<p>Now one observes the survey data – <span class="math inline">\(y\)</span> Fridays in a sample of size <span class="math inline">\(n\)</span>. Using the usual prior times likelihood procedure, the posterior density of <span class="math inline">\(p\)</span> is proportional to the product
<span class="math display" id="eq:bayescompare2">\[\begin{eqnarray}
\pi(p \mid Y = y) \propto \Big[q \pi_1(p) + (1 - q) \pi_2(p)\Big] \times 
{n \choose y} p ^ {y }(1 - p) ^ {n - y}.
\tag{7.20}
\end{eqnarray}\]</span>
After some manipulation, one can show that the posterior density for the proportion <span class="math inline">\(p\)</span> has the mixture form
<span class="math display" id="eq:bayescompare3">\[\begin{eqnarray}
\pi(p \mid Y = y) = q(y) \pi_1(p \mid Y = y) + (1 - q(y)) \pi_2(p \mid Y = y).
\tag{7.21}
\end{eqnarray}\]</span></p>
<p>The posterior densities <span class="math inline">\(\pi_1(p \mid y)\)</span> and <span class="math inline">\(\pi_2(p \mid y)\)</span> are the familiar Beta forms. For example, <span class="math inline">\(\pi_1(p \mid Y = y)\)</span> will be the Beta(3.06 + <span class="math inline">\(y\)</span>, 2.56 + <span class="math inline">\(n - y\)</span>) posterior density combining the Beta(3.06, 2.56) prior and the sample data of <span class="math inline">\(y\)</span> successes in a sample of size <span class="math inline">\(n\)</span>. Likewise, <span class="math inline">\(\pi_2(p \mid Y = y)\)</span> will be the Beta density combining the worker’s Beta(2.07, 7.32) prior and the data.</p>
<p>The quantity <span class="math inline">\(q(y)\)</span> represents the posterior probability of the owner’s prior. One expresses this probability as
<span class="math display" id="eq:bayescompare4">\[\begin{eqnarray}
q(y) = \frac{q f_1(Y = y)}{q f_1(Y = y) + (1 - q) f_2(Y =y)}
\tag{7.22}
\end{eqnarray}\]</span>
where <span class="math inline">\(f_1(Y = y)\)</span> and <span class="math inline">\(f_2(Y = y)\)</span> denote the predictive densities corresponding to the owner’s and worker’s priors. With a little algebra, one represents the posterior odds of the model probabilities as follows.</p>
<p><span class="math display" id="eq:bayescompare5">\[\begin{eqnarray}
\frac{P(Prior \, 1 \mid Y = y)}{P(Prior \, 2 \mid Y = y)} = \frac{q(y)}{1 - q(y)} = \left[\frac{q}{1 - q}\right] \left[\frac{f_1(Y = y)}{f_2(Y = y)}\right]
\tag{7.23}
\end{eqnarray}\]</span></p>
<p>The posterior odds of the owner’s prior <span class="math inline">\(P(Prior \, 1 \mid Y = y) / P(Prior \, 2 \mid y = y)\)</span> is written as the product of two terms.</p>
<ul>
<li><p>The ratio <span class="math inline">\(q / (1 - q)\)</span> represents the prior odds of the owner’s prior.</p></li>
<li><p>The term <span class="math inline">\(f_1(Y = y) / f_2(Y = y)\)</span>, the ratio of the predictive densities, is called the Bayes factor. It reflects the relative abilities of the two priors to predict the observation <span class="math inline">\(y\)</span>.</p></li>
</ul>

<p>The function <code>binomial.beta.mix()</code> is used to find the Bayes factor for our example. One inputs the prior probabilities of the two models (priors), and the vectors of Beta shape parameters that define the owner’s prior and the worker’s prior. The displayed output is the posterior odds value of 6.77.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="proportion.html#cb56-1"></a>probs &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)</span>
<span id="cb56-2"><a href="proportion.html#cb56-2"></a>beta_par1 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">3.06</span>, <span class="fl">2.56</span>)</span>
<span id="cb56-3"><a href="proportion.html#cb56-3"></a>beta_par2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">2.07</span>, <span class="fl">7.32</span>)</span>
<span id="cb56-4"><a href="proportion.html#cb56-4"></a>beta_par &lt;-<span class="st"> </span><span class="kw">rbind</span>(beta_par1, beta_par2)</span>
<span id="cb56-5"><a href="proportion.html#cb56-5"></a>output &lt;-<span class="st"> </span><span class="kw">binomial.beta.mix</span>(probs, beta_par, <span class="kw">c</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb56-6"><a href="proportion.html#cb56-6"></a>(posterior_odds &lt;-<span class="st"> </span>output<span class="op">$</span>probs[<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>output<span class="op">$</span>probs[<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## beta_par1 
##  6.777823</code></pre>
<p>Since the two priors are given equal probabilities, the prior odds <span class="math inline">\(q / (1 - q)\)</span> is equal to one. In this case the posterior odds is equal to the Bayes factor. The interpretation is that for the given observation (12 successes in 20 trials), there is 6.77 times more support for the owner’s prior than for the worker’s prior. This conclusion is consistent with the earlier work that showed that the observed value of <span class="math inline">\(y\)</span> was inconsistent with the Bayesian model for the worker’s prior.</p>
</div>
<div id="posterior-predictive-checking" class="section level3">
<h3><span class="header-section-number">7.6.2</span> Posterior predictive checking</h3>
<p>Although the prior predictive distribution is useful in model checking, it has some disadvantages. One problem is that the distribution <span class="math inline">\(f(Y = y)\)</span> may not exist in the situation where the prior <span class="math inline">\(\pi()\)</span> is not a proper probability distribution. We will see particular situations in future chapters where a vague or imprecise probability distribution is assigned as our prior and then the prior predictive distribution will not be well-defined. A related issue is that a prior may be assigned that may not accurately reflect one’s prior beliefs about a parameter. Small errors in the specification of the prior will result in errors in the prior predictive distribution. So there needs to be some caution in the use of the prior predictive distribution in assessing the goodness of the Bayesian model.</p>
<p>An alternative method of checking the suitability of a Bayesian model is based on the posterior predictive distribution. In this setting, one computes the posterior predictive distribution of a replicated dataset, that is a dataset of the same sample size as our observed sample. One sees if the observed value of <span class="math inline">\(y\)</span> is in the middle of this predictive distribution. If this is true, then this means that the observed sample is consistent with predictions of replicated data. On the other hand, if the observed <span class="math inline">\(y\)</span> is in the tails of the posterior distribution, this indicates some model misspecification which means that there is possibility some issue with the specified prior or sampling density.</p>
<p>One attractive aspect of the posterior prediction distribution is that replicated datasets is conveniently simulated. To simulate one replicated dataset, we first simulate a parameter from its posterior distribution, then simulate new data from the data model given the simulated parameter value. In the Beta-Binomial situation, the posterior of the proportion <span class="math inline">\(p\)</span> is <span class="math inline">\({{\rm{Beta}}}(a + y, b + n - y)\)</span>.<br />
To simulate a new data point <span class="math inline">\(\tilde{Y} = \tilde{y}\)</span>, one first simulates a proportion value <span class="math inline">\(p^{(1)}\)</span> from the Beta posterior and then simulate
a new data point <span class="math inline">\(\tilde{y}^{(1)}\)</span> from a Binomial distribution with sample size <span class="math inline">\(n\)</span> and probability of success <span class="math inline">\(p^{(1)}\)</span>. If we wish to obtain a sample of size <span class="math inline">\(S\)</span> from the posterior predictive distribution, this process is repeated <span class="math inline">\(S\)</span> times as showed in the following diagram.
<span class="math display">\[\begin{eqnarray*}
\text{sample}\,\, p^{(1)} \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(1)} \sim {\rm{Binomial}}(n, p^{(1)})\\
\text{sample}\,\, p^{(2)} \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(2)} \sim {\rm{Binomial}}(n, p^{(2)})\\
&amp;\vdots&amp; \\
\text{sample}\,\, p^{(S)} \sim {\rm{Beta}}(a + y, b + n - y) &amp;\rightarrow&amp; \text{sample}\,\, \tilde{y}^{(S)} \sim {\rm{Binomial}}(n, p^{(S)})
\end{eqnarray*}\]</span>
The sample <span class="math inline">\(\tilde{y}^{(1)}, ..., \tilde{y}^{(S)}\)</span> is an approximation to the posterior predictive distribution that is used for model checking. In practice, one constructs a histogram of this sample and decides if the observed value of <span class="math inline">\(y\)</span> is in the central portion of this predictive distribution. The reader will be given an opportunity to use this algorithm to see if the observed data is consistent with simulations of replicated data from this predictive distribution.</p>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">7.7</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><strong>Laymen’s Prior in the Dining Preference Example</strong></li>
</ol>
<p>Revisit Section 7.2.1 for the laymen’s prior in Equation (7.2) and the expert’s prior in Equation (7.3). Follow the example R code (functions <code>data.frame()</code>, <code>mutate()</code> and <code>ggplot()</code>) to obtain the Bayes table and graph of the laymen’s prior distribution. Compare the similarities and differences between the laymen’s prior and the expert’s prior.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Inference for the Dining Preference (Discrete Priors)</strong></li>
</ol>
<p>Revisit Section 7.2.5 where we show how to find the posterior probability that over half of the students prefer eating out on Friday. Find the following posterior probabilities. (Be careful about the end points.)</p>
<ol style="list-style-type: lower-alpha">
<li><p>The probability that more than 60% of the students prefer eating out on Friday.</p></li>
<li><p>The probability that less than 40% of the students prefer eating out on Friday.</p></li>
<li><p>The probability that between 20% and 40% of the students prefer eating out on Friday.</p></li>
<li><p>No more than 50% of the students prefer eating out on Friday.</p></li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li><strong>Another Dining Survey (Discrete Priors)</strong></li>
</ol>
<p>Suppose the restaurant owner in the college town gives another survey to a different group of students. This time he gives the survey to 30 students – among these responses 10 of them say that Friday is their preferred day to eat out. Use the owner’s prior (restated below) to calculate the following posterior probabilities.
<span class="math display">\[\begin{eqnarray}
p &amp;=&amp; \{0.3, 0.4, 0.5, 0.6, 0.7, 0.8\} \nonumber \\
\pi_e(p) &amp;=&amp; (0.125, 0.125, 0.250, 0.250, 0.125, 0.125) \nonumber
\end{eqnarray}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>The probability that 30% of the students prefer eating out on Friday.</p></li>
<li><p>The probability that more than half of the students prefer eating out on Friday.</p></li>
<li><p>The probability that between 20% and 40% of the students prefer eating out on Friday.</p></li>
</ol>
<ol start="4" style="list-style-type: decimal">
<li><strong>Interpreting A Beta Curve</strong></li>
</ol>
<p>Revisit Figure 7.4 where nine different Beta curves are displayed. In the context of students’ dining preference example where <span class="math inline">\(p\)</span> is the proportion of students preferring Friday, interpret the following prior choices in terms of the opinion of <span class="math inline">\(p\)</span>. For example, <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> represents the prior belief the extreme values <span class="math inline">\(p = 0\)</span> and <span class="math inline">\(p = 1\)</span> are more probable and <span class="math inline">\(p = 0.5\)</span> is the least probable. In the students’ dining preference example, specifying a <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> prior indicates the owner thinks the students’ preference of dining out on Friday is either very strong or very weak.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Interpret the <span class="math inline">\(\textrm{Beta}(1, 1)\)</span> curve.</p></li>
<li><p>Interpret the <span class="math inline">\(\textrm{Beta}(0.5, 1)\)</span> curve.</p></li>
<li><p>Interpret the <span class="math inline">\(\textrm{Beta}(4, 2)\)</span> curve.</p></li>
<li><p>Compare the opinion about <span class="math inline">\(p\)</span> expressed by the two Beta curves: <span class="math inline">\(\textrm{Beta}(4, 1)\)</span> and <span class="math inline">\(\textrm{Beta}(4, 2)\)</span>.</p></li>
</ol>
<ol start="5" style="list-style-type: decimal">
<li><strong>Beta Probabilities</strong></li>
</ol>
<p>Use the functions <code>dbeta()</code>, <code>pbeta()</code>, <code>qbeta()</code>, <code>rbeta()</code>, <code>beta_area()</code>, and <code>beta_quantile()</code> to answer the following questions about Beta probabilities.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Compute the density of <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> at the values <span class="math inline">\(p = \{0.1, 0.5, 0.9, 1.5\}\)</span>. Check your answers with the <span class="math inline">\(\textrm{Beta}(0.5, 0.5)\)</span> curve in Figure 7.4.</p></li>
<li><p>If <span class="math inline">\(p \sim \textrm{Beta}(6, 3)\)</span>, compute the probability <span class="math inline">\(Prob(0.2 \le p \le 0.6)\)</span> if .</p></li>
<li><p>Compute the quantiles of the <span class="math inline">\(\textrm{Beta}(10, 10)\)</span> distribution at the probability values in the set $ {0.1, 0.5, 0.9, 1.5}$.</p></li>
<li><p>Simulate a sample of 100 random values from <span class="math inline">\(\textrm{Beta}(4, 2)\)</span>.</p></li>
</ol>
<ol start="6" style="list-style-type: decimal">
<li><strong>Comparing Beta Distributions</strong></li>
</ol>
<p>Consider four Beta curves: (1) (5, 5), (2) (10, 10), (3) (50, 50) and (4) (100, 100). Think of the shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> as counts of <code>successes" and</code>failures" in a prior sample. Use one of the R beta functions (e.g. <code>rbeta()</code>, <code>beta_area()</code>, among others) to discuss the similarities and differences between these four Beta curves.</p>
<ol start="7" style="list-style-type: decimal">
<li><strong>Specifying A Continuous Beta Prior</strong></li>
</ol>
<p>Consider another dining survey conducted by a restaurant owner in New York. The owner is also interested in knowing about the proportion <span class="math inline">\(p\)</span> of students prefer eating out on Friday. He believes that its <span class="math inline">\(0.4\)</span> quantile is <span class="math inline">\(0.7\)</span> and <span class="math inline">\(0.8\)</span> quantile is <span class="math inline">\(0.9\)</span>. Suppose the owner plans on using a Beta prior distribution.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Find the values of the Beta shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> to represent the restaurant owner’s belief.</p></li>
<li><p>Confirm the choice of Beta prior by taking a simulated sample from the prior predictive simulation. [Hint: use the <code>rbeta()</code> function to simulate a sample from the selected Beta distribution, and then simulate new <span class="math inline">\(\tilde{y}\)</span> values from the Binomial data model (function <code>rbinom()</code>) with a sample size of 20. Graph and/or calculate a few quantiles of the simulated <span class="math inline">\(\tilde{y}\)</span> sample from the predictive distribution to check the restaurant owner’s prior belief.]</p></li>
</ol>
<ol start="8" style="list-style-type: decimal">
<li><strong>Deriving the Beta Posterior</strong></li>
</ol>
<p>Following the derivation process of the dining preference example in Section 7.4.1, derive this more general result. If the proportion has a
(<span class="math inline">\(a, b\)</span>) prior and one samples <span class="math inline">\(Y\)</span> from a  distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, then if one observes <span class="math inline">\(Y = y\)</span>, then the posterior density of <span class="math inline">\(p\)</span> is
(<span class="math inline">\(a + y, b + n - y\)</span>).</p>
<ol start="9" style="list-style-type: decimal">
<li><strong>Prior Sample Size and Strength of Priors</strong></li>
</ol>
<p>Another way of specifying a <span class="math inline">\({\rm{Beta}}(a, b)\)</span> prior is to imagine a pre-survey with the same question and represent the Beta shape parameters in the form of <span class="math inline">\(a\)</span> successes and <span class="math inline">\(b\)</span> failures in the pre-survey. This exercise explores this prior specification method.</p>
<p>Table 7.3. Updating the Beta prior.</p>
<table>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">Successes</th>
<th align="center">Failures</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Prior</td>
<td align="center"><span class="math inline">\(a\)</span></td>
<td align="center"><span class="math inline">\(b\)</span></td>
</tr>
<tr class="even">
<td align="center">Data/Likelihood</td>
<td align="center"><span class="math inline">\(y\)</span></td>
<td align="center"><span class="math inline">\(n-y\)</span></td>
</tr>
<tr class="odd">
<td align="center">Posterior</td>
<td align="center"><span class="math inline">\(a + y\)</span></td>
<td align="center"><span class="math inline">\(b + n - y\)</span></td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li><p>Recall from Section 7.3 that the mean of the <span class="math inline">\({\rm{Beta}}(a, b)\)</span> distribution is <span class="math inline">\(\frac{a}{a+b}\)</span>. Define the prior sample size to be <span class="math inline">\(n_p = a + b\)</span>. Consider two Beta prior distributions: <span class="math inline">\({\rm{Beta}}(2, 2)\)</span> and <span class="math inline">\({\rm{Beta}}(20, 20)\)</span>. Find the prior means and prior sample sizes of these two prior distributions and compare the prior beliefs of these two Beta distributions.</p></li>
<li><p>Suppose a survey yields <span class="math inline">\(4\)</span> successes out of <span class="math inline">\(10\)</span> responses. Suppose one wishes to compare the posterior inference obtained by the two different Beta priors <span class="math inline">\({\rm{Beta}}(2, 2)\)</span> and <span class="math inline">\({\rm{Beta}}(20, 20)\)</span>. Find and compare the two posterior distributions corresponding to these two priors.</p></li>
<li><p>Consider the use of the <span class="math inline">\({\rm{Beta}}(2, 2)\)</span> and <span class="math inline">\({\rm{Beta}}(20, 20)\)</span> prior distributions. Show these two priors have the same prior mean, but different strengths of belief about the location of the proportion. Assuming the survey results in (b), use simulation and graphs to show how different prior sample sizes affect the posterior inference.</p></li>
<li><p>Suppose a survey yields <span class="math inline">\(40\)</span> successes out of <span class="math inline">\(100\)</span> responses. Find the two posterior distributions corresponding to the two prior distributions <span class="math inline">\({\rm{Beta}}(2, 2)\)</span> and <span class="math inline">\({\rm{Beta}}(20, 20)\)</span>. Contrast the two posterior distributions and compare with your answer to part (c).</p></li>
<li><p>Consider the two prior distributions <span class="math inline">\({\rm{Beta}}(9, 1)\)</span> and <span class="math inline">\({\rm{Beta}}(45, 5)\)</span>. Contrast these these two Beta prior distributions with respect to the mean and strength of belief. Compare the two posterior distributions with data <span class="math inline">\(n = 20, y = 5\)</span>, and with the data <span class="math inline">\(n = 200, y = 50\)</span>.</p></li>
</ol>
<ol start="10" style="list-style-type: decimal">
<li><strong>Beta Posterior Mean is a Weighted Mean</strong></li>
</ol>
<p>If the proportion has a
<span class="math inline">\(\textrm{Beta}\)</span>(<span class="math inline">\(a, b\)</span>) prior and one observes <span class="math inline">\(Y\)</span> from a <span class="math inline">\(\textrm{Binomial}\)</span> distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, then if one observes <span class="math inline">\(Y = y\)</span>, then the posterior density of <span class="math inline">\(p\)</span> is
<span class="math inline">\(\textrm{Beta}\)</span>(<span class="math inline">\(a + y, b + n - y\)</span>).</p>
<p>Recall that the mean of a <span class="math inline">\({\textrm Beta}(a, b)\)</span> random variable following is <span class="math inline">\(\frac{a}{a+b}\)</span>. Show that the posterior mean of <span class="math inline">\(p \mid Y = y \sim {\rm Beta}(a + y, b + n - y)\)</span> is a weighted average of the prior mean of <span class="math inline">\(p \sim {\rm Beta}(a, b)\)</span> and the sample mean <span class="math inline">\(\hat{p} = \frac{y}{n}\)</span>. Find the two weights and explain their implication for the posterior being a combination of prior and data.</p>
<ol start="11" style="list-style-type: decimal">
<li><strong>Sequential Updating</strong></li>
</ol>
<p>The restaurant owner’s belief about the proportion of students’ favorite dining day being Friday is represented by a <span class="math inline">\({\rm{Beta}}(15.06, 10.56)\)</span> distribution. Recall that he obtained this posterior distribution from a <span class="math inline">\({\rm{Beta}}(3.06, 2.56)\)</span> prior and a survey of <span class="math inline">\(12\)</span> yes’s out of <span class="math inline">\(20\)</span> responses. The owner is interested in conducting another dining survey a few months later with the same question and the owner is still interested in <span class="math inline">\(p\)</span>, the proportion of all students who say Friday.</p>
<ol style="list-style-type: lower-alpha">
<li><p>The second survey gives a result of <span class="math inline">\(8\)</span> yes’s out of <span class="math inline">\(20\)</span> responses. Use the owner’s current beliefs and this information to update the restaurant owner’s belief about the proportion <span class="math inline">\(p\)</span>.</p></li>
<li><p>Suppose the two surveys are conducted at the same time and the results are <span class="math inline">\(20\)</span> yes’s (<span class="math inline">\(12 + 8\)</span>) out of <span class="math inline">\(40\)</span> responses (<span class="math inline">\(20 + 20\)</span>). Starting with the <span class="math inline">\({\rm{Beta}}(3.06, 2.56)\)</span> prior, update the owner’s belief about the proportion of interest.</p></li>
<li><p>Are the two posterior distribution the same in parts (a) and (b)? Why or why not?</p></li>
<li><p>Suppose the two survey results are reversed. That is, the first survey gives <span class="math inline">\(8\)</span> yes’s and second survey gives <span class="math inline">\(12\)</span> yes’s. Do you still observe the same posterior as in part (b)? What does this tell you about the order of different pieces of information shaping the belief about a parameter?</p></li>
<li><p>What if the two survey results are slightly different? e.g., first survey gives <span class="math inline">\(15\)</span> yes’s and second survey gives <span class="math inline">\(5\)</span> yes’s. What is the posterior distribution in this case?</p></li>
<li><p>Should we combine the two survey results together? Describe a scenario where you it would be inappropriate to combine the survey results.</p></li>
</ol>
<ol start="12" style="list-style-type: decimal">
<li><strong>Bayesian Hypothesis Testing</strong></li>
</ol>
<p>In the dining preference example, the restaurant owner’s posterior distribution of proportion <span class="math inline">\(p\)</span> of students preferring Friday to eat out is <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span>. Suppose the owner’s wife claims that between 50% and 60% of the students prefer to eat out on Friday. Conduct a Bayesian hypothesis test of this claim.</p>
<ol start="13" style="list-style-type: decimal">
<li><strong>Simulation Sample Size</strong></li>
</ol>
<p>Revisit Section 7.5.2. Use R to simulate random samples of sizes <span class="math inline">\(S = \{10, 100, 500, 1000, 5000\}\)</span> of <span class="math inline">\(p\)</span> from the <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span> distribution. Use the <code>quantile()</code> function to find the approximate 90% credible interval of <span class="math inline">\(p\)</span> for each value of <span class="math inline">\(S\)</span>. Comment on the effect of the simulation size <span class="math inline">\(S\)</span> on the accuracy of the simulation results. Recall that the exact middle 90% posterior interval estimate is [0.427, 0.741].</p>
<ol start="14" style="list-style-type: decimal">
<li><strong>Bayesian Credible Intervals</strong></li>
</ol>
<p>In the dining preference example, the restaurant owner’s posterior distribution of proportion <span class="math inline">\(p\)</span> of students preferring Friday to eat out is <span class="math inline">\(\textrm{Beta}(15.06, 10.56)\)</span>. Find the exact Bayesian credible intervals for the following cases.</p>
<ol style="list-style-type: lower-alpha">
<li><p>The middle 95% credible interval.</p></li>
<li><p>The middle 98% credible interval.</p></li>
<li><p>The 90% credible interval of the form <span class="math inline">\((0, B)\)</span>.</p></li>
<li><p>The 99% credible interval of the form <span class="math inline">\((A, 1)\)</span></p></li>
</ol>
<ol start="15" style="list-style-type: decimal">
<li><strong>Simulating the Posterior of the Log Odds</strong>
</li>
</ol>
<p>Since one is able to compute exact posterior summaries using the <code>pbeta()</code> and <code>qbeta()</code> functions, what is the point of using simulation computations?
To illustrate the advantage of simulation, suppose one is interested in finding a 90% probability interval about the logit or log odds <span class="math inline">\(\textrm{log}\left(\frac{p}{1-p}\right)\)</span>. One can approximate the posterior of the logit by simulation. First simulate <span class="math inline">\(S = 1000\)</span> values from the Beta posterior for <span class="math inline">\(p\)</span>, and then for each simulated value of <span class="math inline">\(p\)</span>, compute a value of the logit. The resulting vector will be a random sample from the posterior distribution of the logit.</p>
<ol style="list-style-type: lower-alpha">
<li><p>If the posterior distribution for <span class="math inline">\(p\)</span> is Beta(12, 20), use R to simulate 1000 draws from the posterior of the logit <span class="math inline">\(\textrm{log}\left(\frac{p}{1-p}\right)\)</span>.</p></li>
<li><p>Construct a 90% interval estimate for the logit parameter.</p></li>
</ol>
<ol start="16" style="list-style-type: decimal">
<li><strong>Simulating the Odds</strong></li>
</ol>
<p>Revisit Exercise 15. Instead of the logit or log odds of the proportion <span class="math inline">\(p\)</span>, suppose we are interested in the odds <span class="math inline">\(\frac{p}{1-p}\)</span>. If the posterior distribution for <span class="math inline">\(p\)</span> is Beta(12, 20), use R to simulate 1000 values from the posterior distribution of the odds. Construct a histogram of the simulated odds and construct a <span class="math inline">\(90%\)</span> interval estimate. Experiment with different values of the simulation sample size <span class="math inline">\(S\)</span> and comment on the effect of the value of <span class="math inline">\(S\)</span> on the width of the <span class="math inline">\(90%\)</span> interval estimates.</p>
<ol start="17" style="list-style-type: decimal">
<li><strong>Teenagers and Televisions</strong>
</li>
</ol>
<p>In 1998, the New York Times and CBS News polled <span class="math inline">\(1048\)</span> randomly selected <span class="math inline">\(13 - 17\)</span> year olds to ask them if they had a television in their room. Among this group of teenagers, <span class="math inline">\(692\)</span> of them said they had a television in their room. Alex and Benedict both want to use the Binomial model for this dataset, but they have different prior believes about <span class="math inline">\(p\)</span>, the proportion of teenagers having a television in their room.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Alex asks <span class="math inline">\(10\)</span> friends the same question and <span class="math inline">\(8\)</span> of them have a television in their room. Alex decides to use this information to construct his prior. Design a continuous Beta prior reflecting Alex’s belief.</p></li>
<li><p>Benedict thinks the <span class="math inline">\(0.2\)</span> quantile is <span class="math inline">\(0.3\)</span> and the <span class="math inline">\(0.9\)</span> quantile is <span class="math inline">\(0.4\)</span>. Design a continuous Beta prior reflecting Benedict’s belief.</p></li>
<li><p>Calculate Alex’s posterior and Benedict’s posterior distributions. Plot the two priors on one graph, and plot the corresponding posteriors on another graph. In addition, obtain <span class="math inline">\(95\%\)</span> credible intervals for Alex and Benedict.</p></li>
<li><p>Conduct prior predictive checks for Alex and Benedict. For each person, is the prior consistent with the television data? Explain.</p></li>
</ol>
<ol start="18" style="list-style-type: decimal">
<li><strong>Teenagers and Televisions (continued)</strong></li>
</ol>
<p>Revisit Exercise 17. Consider the odds of having a television in the room. Recall that if <span class="math inline">\(p\)</span> is the probability of having a television in room, then the odds is <span class="math inline">\(\frac{p}{1-p}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Find the mean, median and 95% posterior interval of Alex’s analysis of the odds of teenagers having a television in their room.</p></li>
<li><p>Find the mean, median and 95% posterior interval of Benedict’s analysis of the odds of teenagers having a television in their room.</p></li>
<li><p>Compare the two posterior summaries from parts (a) and (b).</p></li>
</ol>
<ol start="19" style="list-style-type: decimal">
<li><strong>Comparing Two Proportions - Science Majors at Liberal Arts Colleges</strong></li>
</ol>
<p>Many liberal arts colleges and other organizations have been promoting science majors in recent years because of their value on the job market. One wishes to evaluate whether such promotion has any effect on student major preference. A college student Clara is interested in this question and collects data from three liberal arts colleges.</p>
<p>Table 7.4. Data from three liberal arts colleges on science majors in 2005 and 2015.</p>
<table>
<thead>
<tr class="header">
<th align="center">Year</th>
<th align="center">Science</th>
<th align="center">Non-Science</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">2005</td>
<td align="center">264</td>
<td align="center">1496</td>
</tr>
<tr class="even">
<td align="center">2015</td>
<td align="center">437</td>
<td align="center">1495</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li><p>Let <span class="math inline">\(p_{2005}\)</span> and <span class="math inline">\(p_{2015}\)</span> denote the proportions of science majors in 2005 and 2015, respectively. Assuming that <span class="math inline">\(p_{2005}\)</span> and <span class="math inline">\(p_{2015}\)</span> have independent uniform priors, obtain the joint posterior distribution of <span class="math inline">\(p_{2005}\)</span> and <span class="math inline">\(p_{2015}\)</span>. Recall that the <span class="math inline">\({\rm{Beta}}(1, 1)\)</span> distribution is equivalent to the <span class="math inline">\(\rm{Uniform}(0, 1)\)</span> distribution.</p></li>
<li><p>Suppose one uses the parameter <span class="math inline">\(\delta = p_{2015} - p_{2005}\)</span> to measure the difference in proportions. Use simulation from the posterior distribution to answer the question "have the proportions of science majors changed from 2005 to 2015?". [Hint: simulate a vector <span class="math inline">\(s_{2005}\)</span> of posterior samples of <span class="math inline">\(p_{2005}\)</span> and another vector <span class="math inline">\(s_{2015}\)</span> of posterior samples of <span class="math inline">\(p_{2015}\)</span> (make sure to use the same number of samples) and subtract <span class="math inline">\(s_{2005}\)</span> from <span class="math inline">\(s_{2015}\)</span> which yields a vector of simulated values from the posterior of <span class="math inline">\(\delta\)</span>.]</p></li>
<li><p>Did the proportion of science majors change from 2005 to 2015? Answer this question by a posterior computation.</p></li>
<li><p>Compile a similar dataset for your school type, and answer parts (a) through (c).</p></li>
<li><p>What assumption is made about the proportions <span class="math inline">\(p_{2005}\)</span> and <span class="math inline">\(p_{2015}\)</span> in our assignment of priors? Do you think such assumption is justified? If not, how do you think one can adjust the approach to be more realistic?</p></li>
</ol>
<ol start="20" style="list-style-type: decimal">
<li><strong>Comparing Two Proportions - Number of Depression Cases at a Hospital</strong></li>
</ol>
<p>Data are collected on depression cases at hospitals. For a particular hospital, in the year of 1992, there were 306 diagnosed with depression among 651 patients; in the year of 1993, there were 300 diagnosed with depression among 705 patients. One is interested in learning whether the probability of being diagnosed with depression changed between 1992 and 1993. Conduct a. Bayesian analysis of this question. State clearly the inference procedure, the choice of prior distributions, the choice of data model, the posterior distributions and the conclusions.</p>
<ol start="21" style="list-style-type: decimal">
<li><strong>Prior Predictive Checking - Pizza Popularity</strong>
</li>
</ol>
<p>Suppose a restaurant is serving pizza of two varieties, cheese and pepperoni, and a manager is interested in the proportion <span class="math inline">\(p\)</span> of customers who prefer pepperoni. After some thought, the manager’s prior beliefs about <span class="math inline">\(p\)</span> are represented by a Beta(6, 12) distribution.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose a random sample of 20 customers is surveyed on their pizza preference and let <span class="math inline">\(Y\)</span> denote the number that prefer pepperoni. Compute and graph the prior predictive density of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Suppose 20 customers are sampled and 14 prefer pepperoni. Is the value <span class="math inline">\(y = 14\)</span> consistent with the Bayesian model where <span class="math inline">\(p\)</span> has a Beta(6, 12) distribution? Explain why or why not.</p></li>
</ol>
<ol start="22" style="list-style-type: decimal">
<li><strong>Bayes Factor - Pizza Popularity</strong>
</li>
</ol>
<p>In the restaurant example of Exercise 21, suppose one of the waiters has a different opinion about the popularity of pepperoni pizza. His prior belief about the proportion <span class="math inline">\(p\)</span> preferring pizza is described by a Beta(12, 6) distribution.</p>
<ol style="list-style-type: lower-alpha">
<li>Find and graph the prior predictive density of the number <span class="math inline">\(y\)</span> who prefer pizza in a sample of 20 customers.</li>
<li>If 14 out of 20 customers prefer pepperoni, is this result consistent with the predictive distribution?</li>
<li>Compare the two Bayesian models where (1) <span class="math inline">\(p\)</span> is distributed Beta(12, 6) and (2) <span class="math inline">\(p\)</span> is distributed Beta(6, 12) by a Bayes factor. Interpret the value that you compute.</li>
</ol>
<ol start="23" style="list-style-type: decimal">
<li><strong>Posterior Predictive Checking - Pizza Popularity</strong></li>
</ol>
<p>Consider the same problem as in Exercise 22 where <span class="math inline">\(p\)</span> is the proportion of customers who prefer pepperoni and the manager’s prior beliefs are given by a Beta(6, 12) distribution.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose 14 out of 20 customers prefer pepperoni. Using the algorithm described in Section 7.6, simulate 1000 values of <span class="math inline">\(\tilde y\)</span> (out of 20 customers) from the posterior predictive distribution. Construct a histogram of these values.</p></li>
<li><p>Is the observation (14 preferring pepperoni) consistent with this predictive distribution? Explain.</p></li>
<li><p>Repeat parts (a) and (b) using the waiter’s Beta(12, 6) distribution.</p></li>
</ol>
<ol start="24" style="list-style-type: decimal">
<li><strong>Learning from a Multinomial Experiment</strong></li>
</ol>
<p>In Chapter 6 Section 6.3, we discussed the Multinomial distribution, an extension of the Binomial distribution where each trial has more than two outcomes.
As an application of a Multinomial experiment, in an analysis of an election poll, suppose that one wants inferences for three probabilities: <span class="math inline">\(\theta_A\)</span> = probability of a vote for a candidate from Party A, <span class="math inline">\(\theta_B\)</span> = probability of a vote for a candidate from Party B and <span class="math inline">\(\theta_C\)</span> = probability of a vote for a candidate from Party C. One assumes <span class="math inline">\(\theta_A + \theta_B + \theta_C = 1\)</span> as people can vote for only one party. If a random sample of <span class="math inline">\(n\)</span> potential voters is taken, the respective voter counts <span class="math inline">\(Y_A, Y_B, Y_C\)</span> has the probability mass function</p>
<p><span class="math display" id="eq:multinomial">\[\begin{equation}
p(Y_A = y_A, Y_B = y_B, Y_C = y_C) = \frac{n!}{y_A! y_B! y_C!}\theta_A^{y_A}\theta_B^{y_B}\theta_C^{y_C},
\tag{7.24}
\end{equation}\]</span>
where <span class="math inline">\(y_A + y_B + y_C = n\)</span> and <span class="math inline">\(y_A, y_B, y_C \geq 0\)</span>. This is written <span class="math inline">\(\textrm{Multinomial}(n; \theta_A, \theta_B, \theta_C)\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>A convenient prior distribution for <span class="math inline">\((\theta_A, \theta_B, \theta_C)\)</span>
is the Dirichlet distribution, which has the density function
<span class="math display">\[\begin{eqnarray*}
p(\theta_A, \theta_B, \theta_C) = K \theta_A^{\alpha_A - 1}
\theta_B^{\alpha_B - 1} \theta_C^{\alpha_C - 1},
\end{eqnarray*}\]</span>
where <span class="math inline">\((\alpha_A, \alpha_B, \alpha_C)\)</span> are positive constants, and $K =  $ is a
normalizing constant. This is written <span class="math inline">\(\textrm{Dirichlet}(\alpha_A,\alpha_B, \alpha_C)\)</span>. Install the <code>gtools</code> R package and explore <code>ddirichlet()</code> and <code>rdirichlet()</code> functions to evaluate the pdf and generate random samples from <span class="math inline">\(\textrm{Dirichlet}(\alpha_A = 2,\alpha_B = 1, \alpha_C = 1)\)</span>.</p></li>
<li><p>Suppose the prior distribution is <span class="math inline">\(\text{Dirichlet}(\alpha_A, \alpha_B, \alpha_C)\)</span> and
one collects from <span class="math inline">\(n\)</span> sampled voters, where <span class="math inline">\((Y_A, Y_B, Y_C) \sim \textrm{Multinomial}(n; \theta_A, \theta_B, \theta_C)\)</span>.
Find the posterior distribution of <span class="math inline">\((\theta_A, \theta_B,  \theta_C)\)</span> and show that this is a Dirichlet distribution with updated parameters.</p></li>
<li><p>Suppose in the sample of <span class="math inline">\(n = 100\)</span> voters, 53 voted for Party A, 18 voted for Party B, and 29 voted for Party C <span class="math inline">\((y_A = 53, y_B = 18, y_C = 29)\)</span>. Using the prior distribution <span class="math inline">\(\text{Dirichlet}(\alpha_A = 2, \alpha_B = 1, \alpha_C = 1)\)</span> and the generic results from part (b), obtain the posterior distribution for <span class="math inline">\((\theta_A, \theta_B, \theta_C)\)</span>. Plot the prior and the posterior distributions for <span class="math inline">\((\theta_A, \theta_B, \theta_C)\)</span> and discuss your findings.</p></li>
<li><p>Suppose one wants to compute the ratio of odds of voting for Party A to the odds of voting for Party B, <span class="math inline">\(\frac{\theta_A/(1 - \theta_A)}{\theta_B/(1 - \theta_B)}\)</span>. Compute a 95% posterior interval for this odds ratio.</p></li>
</ol>
<p>E1. <strong>How Good is the Shooter?</strong></p>
<p>Suppose you are interested in learning about <span class="math inline">\(p\)</span>, the probability that a high school basketball player will succeed in a free-throw attempt. You believe before observing any data that <span class="math inline">\(p\)</span> is equal to 0.5, 0.6, 0.7, or 0.8 and each value has the same probability. Suppose she is going to take three shots and let <span class="math inline">\(Y\)</span> denote the number of shots she makes.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Find the joint distribution of <span class="math inline">\(p\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Compute the predictive (marginal) distribution of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Find the probability that she makes at least two shots.</p></li>
<li><p>Find the expected number of shots she will make.</p></li>
</ol>
<p>E2. <strong>How Good is the Shooter? (continued)</strong></p>
<p>Suppose that you observe the shooter make two of her three shot attempts.</p>
<ol style="list-style-type: lower-alpha">
<li><p>By use of a Bayes’ table, perform all of the Bayesian calculations include values of the Likelihood, Product, and Posterior.</p></li>
<li><p>Graph the posterior distribution for <span class="math inline">\(P\)</span>.</p></li>
<li><p>If you define an “excellent” shooter as values of <span class="math inline">\(p\)</span> of 0.7 or 0.8, what is the posterior probability she is an excellent shooter.</p></li>
<li><p>Find the posterior mean of <span class="math inline">\(p\)</span>.</p></li>
</ol>
<p>E3. <strong>Does Frank Have ESP?</strong></p>
<p>Frank claims to have extra sensory perception (ESP). You are skeptical and so you ask Frank to participate in a card reading experiment. You have a deck of cards with equal numbers of <span class="math inline">\(\diamondsuit\)</span>’s, <span class="math inline">\(\heartsuit\)</span>’s, <span class="math inline">\(\clubsuit\)</span>’s, and <span class="math inline">\(\spadesuit\)</span>’s. You shuffle the deck and hold a card up with the face away from Frank. Frank will then tell you the suit of the card. For the next trial of the experiment, you put the card back into the deck, reshuffle, and draw a new card. The experiment is completed when ten cards have been used. Let <span class="math inline">\(p\)</span> be the proportion of cards in which he correctly identifies the suit. There are two models of interest: <span class="math inline">\(p=.25\)</span>, which says that Frank does not have ESP and is essentially guessing at the suit of the card, and <span class="math inline">\(p=.5\)</span> which says that Frank does have some ability to detect the suit.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose you will test Frank using 10 cards and let <span class="math inline">\(Y\)</span> denote the number of cards that are correctly identified. Suppose you are skeptical of Frank’s ability so you define the following prior.</li>
</ol>
<table>
<thead>
<tr class="header">
<th>P</th>
<th>Prior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.25</td>
<td>0.8</td>
</tr>
<tr class="even">
<td>0.50</td>
<td>0.2</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li><p>Find the marginal (predictive) pmf for <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Before you observe any results, find the probability that Frank will guess 5 or more correct (that is, <span class="math inline">\(Y \ge 5\)</span>).</p></li>
<li><p>Now the experiment is run and Frank correctly identifies <span class="math inline">\(Y = 5\)</span> of the 10 cards. By constructing a Bayes’ table, find the posterior distribution for <span class="math inline">\(p\)</span>.</p></li>
<li><p>Based on your experiment, do you think it is likely that Frank has ESP? Explain.</p></li>
</ol>
<p>E4. <strong>Sampling Students</strong></p>
<p>Suppose you are interested in learning about the proportion <span class="math inline">\(p\)</span> of undergraduate students who own Macintosh laptops. You place the following prior on the values of <span class="math inline">\(p\)</span>. (Note the prior weights, not the prior probabilities are shown.)</p>
<table>
<thead>
<tr class="header">
<th>P</th>
<th>Prior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.1</td>
<td>1</td>
</tr>
<tr class="even">
<td>0.2</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0.3</td>
<td>2</td>
</tr>
<tr class="even">
<td>0.4</td>
<td>4</td>
</tr>
<tr class="odd">
<td>0.5</td>
<td>2</td>
</tr>
<tr class="even">
<td>0.6</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0.7</td>
<td>1</td>
</tr>
<tr class="even">
<td>0.8</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0.9</td>
<td>1</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li><p>You plan on sampling 20 students. Based on your prior, find the (predictive) probability that 10 or more students will own Macs. [HINT: Compute this probability by a “model/data” simulation where you simulate values <span class="math inline">\(p\)</span> and <span class="math inline">\(y\)</span>.]</p></li>
<li><p>Suppose you now take your sample and 12 out of the 20 students own Macs. Find the posterior probability distribution for <span class="math inline">\(p\)</span> and find the posterior mean.</p></li>
<li><p>Find an interval that you believe contains <span class="math inline">\(p\)</span> with probability 0.9.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="joint-probability-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mean.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-proportion.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
