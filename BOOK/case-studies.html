<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Case Studies | Probability and Bayesian Modeling</title>
  <meta name="description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Case Studies | Probability and Bayesian Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Case Studies | Probability and Bayesian Modeling" />
  
  <meta name="twitter:description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  

<meta name="author" content="Jim Albert and Jingchen Hu" />


<meta name="date" content="2020-01-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-multiple-regression-and-logistic-models.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Probability and Bayesian Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html"><i class="fa fa-check"></i><b>1</b> Probability: A Measurement of Uncertainty</a><ul>
<li class="chapter" data-level="1.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-classical-view-of-a-probability"><i class="fa fa-check"></i><b>1.2</b> The Classical View of a Probability</a></li>
<li class="chapter" data-level="1.3" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-frequency-view-of-a-probability"><i class="fa fa-check"></i><b>1.3</b> The Frequency View of a Probability</a></li>
<li class="chapter" data-level="1.4" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-subjective-view-of-a-probability"><i class="fa fa-check"></i><b>1.4</b> The Subjective View of a Probability</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#measuring-probabilities-subjectively"><i class="fa fa-check"></i><b>1.4.1</b> Measuring probabilities subjectively</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-sample-space"><i class="fa fa-check"></i><b>1.5</b> The Sample Space</a><ul>
<li class="chapter" data-level="1.5.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#roll-two-fair-indistinguishable-dice"><i class="fa fa-check"></i><b>1.5.1</b> Roll two fair, indistinguishable dice</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#assigning-probabilities"><i class="fa fa-check"></i><b>1.6</b> Assigning Probabilities</a><ul>
<li class="chapter" data-level="1.6.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#events-and-event-operations"><i class="fa fa-check"></i><b>1.6.1</b> Events and Event Operations</a></li>
<li class="chapter" data-level="1.6.2" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-three-probability-axioms"><i class="fa fa-check"></i><b>1.6.2</b> The Three Probability Axioms</a></li>
<li class="chapter" data-level="1.6.3" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-complement-and-addition-properties"><i class="fa fa-check"></i><b>1.6.3</b> The Complement and Addition Properties</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#exercises"><i class="fa fa-check"></i><b>1.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="counting-methods.html"><a href="counting-methods.html"><i class="fa fa-check"></i><b>2</b> Counting Methods</a><ul>
<li class="chapter" data-level="2.1" data-path="counting-methods.html"><a href="counting-methods.html#introduction-rolling-dice-yahtzee-and-roulette"><i class="fa fa-check"></i><b>2.1</b> Introduction: Rolling Dice, Yahtzee, and Roulette</a></li>
<li class="chapter" data-level="2.2" data-path="counting-methods.html"><a href="counting-methods.html#equally-likely-outcomes"><i class="fa fa-check"></i><b>2.2</b> Equally Likely Outcomes</a></li>
<li class="chapter" data-level="2.3" data-path="counting-methods.html"><a href="counting-methods.html#the-multiplication-counting-rule"><i class="fa fa-check"></i><b>2.3</b> The Multiplication Counting Rule</a></li>
<li class="chapter" data-level="2.4" data-path="counting-methods.html"><a href="counting-methods.html#permutations"><i class="fa fa-check"></i><b>2.4</b> Permutations</a></li>
<li class="chapter" data-level="2.5" data-path="counting-methods.html"><a href="counting-methods.html#combinations"><i class="fa fa-check"></i><b>2.5</b> Combinations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="counting-methods.html"><a href="counting-methods.html#number-of-subsets"><i class="fa fa-check"></i><b>2.5.1</b> Number of subsets</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="counting-methods.html"><a href="counting-methods.html#arrangements-of-non-distinct-objects"><i class="fa fa-check"></i><b>2.6</b> Arrangements of Non-Distinct Objects</a></li>
<li class="chapter" data-level="2.7" data-path="counting-methods.html"><a href="counting-methods.html#playing-yahtzee"><i class="fa fa-check"></i><b>2.7</b> Playing Yahtzee</a></li>
<li class="chapter" data-level="2.8" data-path="counting-methods.html"><a href="counting-methods.html#exercises-1"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>3</b> Conditional Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="conditional-probability.html"><a href="conditional-probability.html#introduction-the-three-card-problem"><i class="fa fa-check"></i><b>3.1</b> Introduction: The Three Card Problem</a></li>
<li class="chapter" data-level="3.2" data-path="conditional-probability.html"><a href="conditional-probability.html#independent-events"><i class="fa fa-check"></i><b>3.2</b> Independent Events</a></li>
<li class="chapter" data-level="3.3" data-path="conditional-probability.html"><a href="conditional-probability.html#in-everyday-life"><i class="fa fa-check"></i><b>3.3</b> In Everyday Life</a></li>
<li class="chapter" data-level="3.4" data-path="conditional-probability.html"><a href="conditional-probability.html#in-a-two-way-table"><i class="fa fa-check"></i><b>3.4</b> In a Two-Way Table</a></li>
<li class="chapter" data-level="3.5" data-path="conditional-probability.html"><a href="conditional-probability.html#definition-and-the-multiplication-rule"><i class="fa fa-check"></i><b>3.5</b> Definition and the Multiplication Rule</a></li>
<li class="chapter" data-level="3.6" data-path="conditional-probability.html"><a href="conditional-probability.html#the-multiplication-rule"><i class="fa fa-check"></i><b>3.6</b> The Multiplication Rule</a></li>
<li class="chapter" data-level="3.7" data-path="conditional-probability.html"><a href="conditional-probability.html#the-multiplication-rule-under-independence"><i class="fa fa-check"></i><b>3.7</b> The Multiplication Rule Under Independence</a></li>
<li class="chapter" data-level="3.8" data-path="conditional-probability.html"><a href="conditional-probability.html#learning-using-bayes-rule"><i class="fa fa-check"></i><b>3.8</b> Learning Using Bayes’ Rule</a></li>
<li class="chapter" data-level="3.9" data-path="conditional-probability.html"><a href="conditional-probability.html#r-example-learning-about-a-spinner"><i class="fa fa-check"></i><b>3.9</b> R Example: Learning About a Spinner</a></li>
<li class="chapter" data-level="3.10" data-path="conditional-probability.html"><a href="conditional-probability.html#exercises-2"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discrete-distributions.html"><a href="discrete-distributions.html"><i class="fa fa-check"></i><b>4</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="4.1" data-path="discrete-distributions.html"><a href="discrete-distributions.html#introduction-the-hat-check-problem"><i class="fa fa-check"></i><b>4.1</b> Introduction: The Hat Check Problem</a></li>
<li class="chapter" data-level="4.2" data-path="discrete-distributions.html"><a href="discrete-distributions.html#random-variable-and-probability-distribution"><i class="fa fa-check"></i><b>4.2</b> Random Variable and Probability Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="discrete-distributions.html"><a href="discrete-distributions.html#probability-distribution"><i class="fa fa-check"></i><b>4.3</b> Probability distribution</a></li>
<li class="chapter" data-level="4.4" data-path="discrete-distributions.html"><a href="discrete-distributions.html#summarizing-a-probability-distribution"><i class="fa fa-check"></i><b>4.4</b> Summarizing a Probability Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="discrete-distributions.html"><a href="discrete-distributions.html#standard-deviation-of-a-probability-distribution"><i class="fa fa-check"></i><b>4.5</b> Standard Deviation of a Probability Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="discrete-distributions.html"><a href="discrete-distributions.html#coin-tossing-distributions"><i class="fa fa-check"></i><b>4.6</b> Coin-Tossing Distributions</a></li>
<li class="chapter" data-level="4.7" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Binomial probabilities</a></li>
<li class="chapter" data-level="4.8" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-experiments"><i class="fa fa-check"></i><b>4.8</b> Binomial experiments</a></li>
<li class="chapter" data-level="4.9" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-computations"><i class="fa fa-check"></i><b>4.9</b> Binomial computations</a></li>
<li class="chapter" data-level="4.10" data-path="discrete-distributions.html"><a href="discrete-distributions.html#mean-and-standard-deviation-of-a-binomial"><i class="fa fa-check"></i><b>4.10</b> Mean and standard deviation of a Binomial</a></li>
<li class="chapter" data-level="4.11" data-path="discrete-distributions.html"><a href="discrete-distributions.html#negative-binomial-experiments"><i class="fa fa-check"></i><b>4.11</b> Negative Binomial Experiments</a></li>
<li class="chapter" data-level="4.12" data-path="discrete-distributions.html"><a href="discrete-distributions.html#exercises-3"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="continuous-distributions.html"><a href="continuous-distributions.html"><i class="fa fa-check"></i><b>5</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="continuous-distributions.html"><a href="continuous-distributions.html#introduction-a-baseball-spinner-game"><i class="fa fa-check"></i><b>5.1</b> Introduction: A Baseball Spinner Game</a></li>
<li class="chapter" data-level="5.2" data-path="continuous-distributions.html"><a href="continuous-distributions.html#the-uniform-distribution"><i class="fa fa-check"></i><b>5.2</b> The Uniform Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="continuous-distributions.html"><a href="continuous-distributions.html#binomial-probabilities-and-the-normal-curve"><i class="fa fa-check"></i><b>5.3</b> Binomial Probabilities and the Normal Curve</a></li>
<li class="chapter" data-level="5.4" data-path="continuous-distributions.html"><a href="continuous-distributions.html#sampling-distribution-of-the-mean"><i class="fa fa-check"></i><b>5.4</b> Sampling Distribution of the Mean</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html"><i class="fa fa-check"></i><b>6</b> Joint Probability Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#joint-probability-mass-function-sampling-from-a-box"><i class="fa fa-check"></i><b>6.1</b> Joint Probability Mass Function: Sampling From a Box</a></li>
<li class="chapter" data-level="6.2" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#multinomial-experiments"><i class="fa fa-check"></i><b>6.2</b> Multinomial Experiments</a></li>
<li class="chapter" data-level="6.3" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#joint-density-functions"><i class="fa fa-check"></i><b>6.3</b> Joint Density Functions</a></li>
<li class="chapter" data-level="6.4" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#independence-and-measuring-association"><i class="fa fa-check"></i><b>6.4</b> Independence and Measuring Association</a></li>
<li class="chapter" data-level="6.5" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#flipping-a-random-coin-the-beta-binomial-distribution"><i class="fa fa-check"></i><b>6.5</b> Flipping a Random Coin: The Beta-Binomial Distribution</a></li>
<li class="chapter" data-level="6.6" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.7" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#exercises-4"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="proportion.html"><a href="proportion.html"><i class="fa fa-check"></i><b>7</b> Learning About a Binomial Probability</a><ul>
<li class="chapter" data-level="7.1" data-path="proportion.html"><a href="proportion.html#introduction-thinking-about-a-proportion-subjectively"><i class="fa fa-check"></i><b>7.1</b> Introduction: Thinking About a Proportion Subjectively</a></li>
<li class="chapter" data-level="7.2" data-path="proportion.html"><a href="proportion.html#bayesian-inference-with-discrete-priors"><i class="fa fa-check"></i><b>7.2</b> Bayesian Inference with Discrete Priors</a><ul>
<li class="chapter" data-level="7.2.1" data-path="proportion.html"><a href="proportion.html#example-students-dining-preference"><i class="fa fa-check"></i><b>7.2.1</b> Example: students’ dining preference</a></li>
<li class="chapter" data-level="7.2.2" data-path="proportion.html"><a href="proportion.html#discrete-prior-distributions-for-proportion-p"><i class="fa fa-check"></i><b>7.2.2</b> Discrete prior distributions for proportion <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="7.2.3" data-path="proportion.html"><a href="proportion.html#likelihood"><i class="fa fa-check"></i><b>7.2.3</b> Likelihood</a></li>
<li class="chapter" data-level="7.2.4" data-path="proportion.html"><a href="proportion.html#posterior-distribution-for-proportion-p"><i class="fa fa-check"></i><b>7.2.4</b> Posterior distribution for proportion <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="7.2.5" data-path="proportion.html"><a href="proportion.html#inference-students-dining-preference"><i class="fa fa-check"></i><b>7.2.5</b> Inference: students’ dining preference</a></li>
<li class="chapter" data-level="7.2.6" data-path="proportion.html"><a href="proportion.html#discussion-using-a-discrete-prior"><i class="fa fa-check"></i><b>7.2.6</b> Discussion: using a discrete prior</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="proportion.html"><a href="proportion.html#continuous-priors"><i class="fa fa-check"></i><b>7.3</b> Continuous Priors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="proportion.html"><a href="proportion.html#the-beta-distribution-and-probabilities"><i class="fa fa-check"></i><b>7.3.1</b> The Beta distribution and probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="proportion.html"><a href="proportion.html#updating-the-beta-prior"><i class="fa fa-check"></i><b>7.4</b> Updating the Beta Prior</a><ul>
<li class="chapter" data-level="7.4.1" data-path="proportion.html"><a href="proportion.html#bayes-rule-calculation"><i class="fa fa-check"></i><b>7.4.1</b> Bayes’ rule calculation</a></li>
<li class="chapter" data-level="7.4.2" data-path="proportion.html"><a href="proportion.html#from-beta-prior-to-beta-posterior"><i class="fa fa-check"></i><b>7.4.2</b> From Beta prior to Beta posterior</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="proportion.html"><a href="proportion.html#bayesian-inferences-with-continuous-priors"><i class="fa fa-check"></i><b>7.5</b> Bayesian Inferences with Continuous Priors</a><ul>
<li class="chapter" data-level="7.5.1" data-path="proportion.html"><a href="proportion.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>7.5.1</b> Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="7.5.2" data-path="proportion.html"><a href="proportion.html#bayesian-credible-intervals"><i class="fa fa-check"></i><b>7.5.2</b> Bayesian credible intervals</a></li>
<li class="chapter" data-level="7.5.3" data-path="proportion.html"><a href="proportion.html#bayesian-prediction"><i class="fa fa-check"></i><b>7.5.3</b> Bayesian prediction</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="proportion.html"><a href="proportion.html#predictive-checking"><i class="fa fa-check"></i><b>7.6</b> Predictive Checking</a><ul>
<li class="chapter" data-level="7.6.1" data-path="proportion.html"><a href="proportion.html#comparing-bayesian-models"><i class="fa fa-check"></i><b>7.6.1</b> Comparing Bayesian models</a></li>
<li class="chapter" data-level="7.6.2" data-path="proportion.html"><a href="proportion.html#posterior-predictive-checking"><i class="fa fa-check"></i><b>7.6.2</b> Posterior predictive checking</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="proportion.html"><a href="proportion.html#exercises-5"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mean.html"><a href="mean.html"><i class="fa fa-check"></i><b>8</b> Modeling Measurement and Count Data</a><ul>
<li class="chapter" data-level="8.1" data-path="mean.html"><a href="mean.html#introduction-1"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="mean.html"><a href="mean.html#modeling-measurements"><i class="fa fa-check"></i><b>8.2</b> Modeling Measurements</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mean.html"><a href="mean.html#examples"><i class="fa fa-check"></i><b>8.2.1</b> Examples</a></li>
<li class="chapter" data-level="8.2.2" data-path="mean.html"><a href="mean.html#the-general-approach"><i class="fa fa-check"></i><b>8.2.2</b> The general approach</a></li>
<li class="chapter" data-level="8.2.3" data-path="mean.html"><a href="mean.html#outline-of-chapter"><i class="fa fa-check"></i><b>8.2.3</b> Outline of chapter</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mean.html"><a href="mean.html#Normal:Discrete"><i class="fa fa-check"></i><b>8.3</b> Bayesian Inference with Discrete Priors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mean.html"><a href="mean.html#example-roger-federers-time-to-serve"><i class="fa fa-check"></i><b>8.3.1</b> Example: Roger Federer’s time-to-serve</a></li>
<li class="chapter" data-level="8.3.2" data-path="mean.html"><a href="mean.html#Normal:SamplingModel:derivation"><i class="fa fa-check"></i><b>8.3.2</b> Simplification of the likelihood</a></li>
<li class="chapter" data-level="8.3.3" data-path="mean.html"><a href="mean.html#Normal:SamplingModel:inference"><i class="fa fa-check"></i><b>8.3.3</b> Inference: Federer’s time-to-serve</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mean.html"><a href="mean.html#Normal:Continuous"><i class="fa fa-check"></i><b>8.4</b> Continuous Priors</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mean.html"><a href="mean.html#Normal:Continuous:prior"><i class="fa fa-check"></i><b>8.4.1</b> The Normal prior for mean <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="8.4.2" data-path="mean.html"><a href="mean.html#Normal:Continuous:choosing"><i class="fa fa-check"></i><b>8.4.2</b> Choosing a Normal prior</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate"><i class="fa fa-check"></i><b>8.5</b> Updating the Normal Prior</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mean.html"><a href="mean.html#introduction-2"><i class="fa fa-check"></i><b>8.5.1</b> Introduction</a></li>
<li class="chapter" data-level="8.5.2" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:Overview"><i class="fa fa-check"></i><b>8.5.2</b> A quick peak at the update procedure</a></li>
<li class="chapter" data-level="8.5.3" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:BayesRule"><i class="fa fa-check"></i><b>8.5.3</b> Bayes’ rule calculation</a></li>
<li class="chapter" data-level="8.5.4" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:Conjugate"><i class="fa fa-check"></i><b>8.5.4</b> Conjugate Normal prior</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference"><i class="fa fa-check"></i><b>8.6</b> Bayesian Inferences for Continuous Normal Mean</a><ul>
<li class="chapter" data-level="8.6.1" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference:HTandCI"><i class="fa fa-check"></i><b>8.6.1</b> Bayesian hypothesis testing and credible interval</a></li>
<li class="chapter" data-level="8.6.2" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference:Prediction"><i class="fa fa-check"></i><b>8.6.2</b> Bayesian prediction</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="mean.html"><a href="mean.html#Normal:PPC"><i class="fa fa-check"></i><b>8.7</b> Posterior Predictive Checking</a></li>
<li class="chapter" data-level="8.8" data-path="mean.html"><a href="mean.html#modeling-count-data"><i class="fa fa-check"></i><b>8.8</b> Modeling Count Data</a><ul>
<li class="chapter" data-level="8.8.1" data-path="mean.html"><a href="mean.html#examples-1"><i class="fa fa-check"></i><b>8.8.1</b> Examples</a></li>
<li class="chapter" data-level="8.8.2" data-path="mean.html"><a href="mean.html#the-poisson-distribution"><i class="fa fa-check"></i><b>8.8.2</b> The Poisson distribution</a></li>
<li class="chapter" data-level="8.8.3" data-path="mean.html"><a href="mean.html#bayesian-inferences"><i class="fa fa-check"></i><b>8.8.3</b> Bayesian inferences</a></li>
<li class="chapter" data-level="8.8.4" data-path="mean.html"><a href="mean.html#case-study-learning-about-website-counts"><i class="fa fa-check"></i><b>8.8.4</b> Case study: Learning about website counts</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="mean.html"><a href="mean.html#exercises-6"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>9</b> Simulation by Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="9.1" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#introduction-3"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#markov-chains"><i class="fa fa-check"></i><b>9.2</b> Markov Chains</a></li>
<li class="chapter" data-level="9.3" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>9.3</b> The Metropolis Algorithm</a></li>
<li class="chapter" data-level="9.4" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#example-cauchy-normal-problem"><i class="fa fa-check"></i><b>9.4</b> Example: Cauchy-Normal problem</a></li>
<li class="chapter" data-level="9.5" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#gibbs-sampling"><i class="fa fa-check"></i><b>9.5</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="9.6" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#mcmc-inputs-and-diagnostics"><i class="fa fa-check"></i><b>9.6</b> MCMC Inputs and Diagnostics</a></li>
<li class="chapter" data-level="9.7" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#using-jags"><i class="fa fa-check"></i><b>9.7</b> Using JAGS</a></li>
<li class="chapter" data-level="9.8" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#exercises-7"><i class="fa fa-check"></i><b>9.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html"><i class="fa fa-check"></i><b>10</b> Bayesian Hierarchical Modeling</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#introduction-4"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#hierarchical-normal-modeling"><i class="fa fa-check"></i><b>10.2</b> Hierarchical Normal Modeling</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#hierarchical-beta-binomial-modeling"><i class="fa fa-check"></i><b>10.3</b> Hierarchical Beta-Binomial Modeling</a></li>
<li class="chapter" data-level="10.4" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#exercises-8"><i class="fa fa-check"></i><b>10.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction-5"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#example-prices-and-areas-of-house-sales"><i class="fa fa-check"></i><b>11.2</b> Example: Prices and Areas of House Sales</a></li>
<li class="chapter" data-level="11.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-simple-linear-regression-model"><i class="fa fa-check"></i><b>11.3</b> A Simple Linear Regression Model</a></li>
<li class="chapter" data-level="11.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-weakly-informative-prior"><i class="fa fa-check"></i><b>11.4</b> A Weakly Informative Prior</a></li>
<li class="chapter" data-level="11.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#posterior-analysis"><i class="fa fa-check"></i><b>11.5</b> Posterior Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-through-mcmc"><i class="fa fa-check"></i><b>11.6</b> Inference through MCMC</a></li>
<li class="chapter" data-level="11.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#bayesian-inferences-with-simple-linear-regression"><i class="fa fa-check"></i><b>11.7</b> Bayesian Inferences with Simple Linear Regression</a></li>
<li class="chapter" data-level="11.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#informative-prior-1"><i class="fa fa-check"></i><b>11.8</b> Informative Prior</a></li>
<li class="chapter" data-level="11.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-conditional-means-prior"><i class="fa fa-check"></i><b>11.9</b> A Conditional Means Prior</a></li>
<li class="chapter" data-level="11.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-9"><i class="fa fa-check"></i><b>11.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html"><i class="fa fa-check"></i><b>12</b> Bayesian Multiple Regression and Logistic Models</a><ul>
<li class="chapter" data-level="12.1" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#introduction-6"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#bayesian-multiple-linear-regression"><i class="fa fa-check"></i><b>12.2</b> Bayesian Multiple Linear Regression</a></li>
<li class="chapter" data-level="12.3" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#bayesian-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Bayesian Logistic Regression </a></li>
<li class="chapter" data-level="12.4" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#exercises-10"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>13</b> Case Studies</a><ul>
<li class="chapter" data-level="13.1" data-path="case-studies.html"><a href="case-studies.html#introduction-7"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="case-studies.html"><a href="case-studies.html#federalist-papers-study"><i class="fa fa-check"></i><b>13.2</b> Federalist Papers Study</a></li>
<li class="chapter" data-level="13.3" data-path="case-studies.html"><a href="case-studies.html#negative-binomial-sampling"><i class="fa fa-check"></i><b>13.3</b> Negative Binomial sampling</a></li>
<li class="chapter" data-level="13.4" data-path="case-studies.html"><a href="case-studies.html#comparison-of-rates-for-two-authors"><i class="fa fa-check"></i><b>13.4</b> Comparison of rates for two authors</a></li>
<li class="chapter" data-level="13.5" data-path="case-studies.html"><a href="case-studies.html#which-words-distinguish-the-two-authors"><i class="fa fa-check"></i><b>13.5</b> Which words distinguish the two authors?</a></li>
<li class="chapter" data-level="13.6" data-path="case-studies.html"><a href="case-studies.html#career-trajectories"><i class="fa fa-check"></i><b>13.6</b> Career Trajectories</a></li>
<li class="chapter" data-level="13.7" data-path="case-studies.html"><a href="case-studies.html#latent-class-modeling"><i class="fa fa-check"></i><b>13.7</b> Latent Class Modeling</a></li>
<li class="chapter" data-level="13.8" data-path="case-studies.html"><a href="case-studies.html#exercises-11"><i class="fa fa-check"></i><b>13.8</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probability and Bayesian Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="case-studies" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Case Studies</h1>
<div id="introduction-7" class="section level2">
<h2><span class="header-section-number">13.1</span> Introduction</h2>
<p>This chapter provides several illustrations of Bayesian modeling that extend some of the models described in earlier chapters.<br />
Mosteller and Wallace (1963), in one of the early significant Bayesian applications, explore the frequencies of word use in the well-known Federalist Papers to determine the authorship between Alexander Hamilton and James Madison. Section 13.2 revisits this word use application. This study raises several interesting explorations such as determining a suitable sampling distribution and finding suitable ways of comparing the word use of several authors.</p>
<p>In sports, teams are very interested in learning about the pattern of increase and decrease in the performance of a player, commonly called a career trajectory. A baseball player is believed to reach a level of peak performance at age of 30, although this “peak age” may vary between players. Section 13.3 illustrates the use of a hierarchical model to simultaneously estimate the career trajectories for a group of baseball players using on-base percentage as the measure of performance.</p>
<p>Suppose a class is taking a multiple choice exam where there are two groups of students. Some students are well-prepared and are familiar with the exam content and other students have not studied and will essentially guess at the answers to the exam questions. Section 13.4 introduces a latent class model that assumes that the class consists of two groups of students with different success rates and the group identifications of the students are unknown. In the posterior analysis, one learns about the location of the two success rates and the group classifications of the students. Using this latent class framework, the Federalist Papers example is revisited and the frequencies of particular filler words is used to learn about the true author identity of some disputed authorship Federalist Papers.</p>
</div>
<div id="federalist-papers-study" class="section level2">
<h2><span class="header-section-number">13.2</span> Federalist Papers Study</h2>
<p>
<strong>Introduction</strong></p>
<p>The Federalist Papers were a collection of articles written in the late 18th century by Alexander Hamilton, James Madison and John Jay to promote the ratification of the United States Constitution. Some of these papers are known to be written by Hamilton, other papers were clearly written by Madison, and the true authorship of some of the remaining papers has been in doubt.</p>
<p>In one of the early significant applied Bayesian papers, Mosteller and Wallace (1963) illustrate the use of Bayesian reasoning in solving the authorship problem. They focused on the frequencies of word counts. Since the topic of the article may influence the frequencies of words used, Mosteller and Wallace were careful to focus on counts of so-called filler words such as “an”, “of”, and “upon” that are not influenced by the topics of the articles.</p>
<p>In this case study, the use of different sampling distributions is described to model word counts in a group of Federalist Papers. The Poisson distribution is perhaps a natural choice for modeling a group of word counts, but it will be seen that the Poisson can not accommodate the spread of the distribution of word counts. This motivates the use of a Negative Binomial sampling distribution and this model will be used to compare rates of use of some filler words by Hamilton and Madison.</p>
<p><strong>Data on word use</strong></p>
<p>To begin our study, let’s look at the occurrences of the word “can” in all of the Federalist Papers authored by Alexander Hamilton or James Madison. Table 13.1 shows the format of the data. For each paper, the total number of words, the number of occurrences of the word “can” and the rate of this word per 1000 words are recorded.</p>
<p>Table 13.1. Portion of the data table counting the number of words and occurrences of the word “can” in 74 Federalist papers.</p>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="left">Name</th>
<th align="right">Total</th>
<th align="left">word</th>
<th align="right">y</th>
<th align="right">Rate</th>
<th align="left">Authorship</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="left">Federalist No. 1</td>
<td align="right">1622</td>
<td align="left">can</td>
<td align="right">3</td>
<td align="right">1.85</td>
<td align="left">Hamilton</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">Federalist No. 10</td>
<td align="right">3008</td>
<td align="left">can</td>
<td align="right">4</td>
<td align="right">1.33</td>
<td align="left">Madison</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="left">Federalist No. 11</td>
<td align="right">2511</td>
<td align="left">can</td>
<td align="right">5</td>
<td align="right">1.99</td>
<td align="left">Hamilton</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="left">Federalist No. 12</td>
<td align="right">2171</td>
<td align="left">can</td>
<td align="right">2</td>
<td align="right">0.92</td>
<td align="left">Hamilton</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="left">Federalist No. 13</td>
<td align="right">970</td>
<td align="left">can</td>
<td align="right">4</td>
<td align="right">4.12</td>
<td align="left">Hamilton</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">Federalist No. 14</td>
<td align="right">2159</td>
<td align="left">can</td>
<td align="right">9</td>
<td align="right">4.17</td>
<td align="left">Madison</td>
</tr>
</tbody>
</table>
<p>Figure 13.1 displays parallel jittered dotplots of the rates (per 1000 words) of “can” for the Madison and Hamilton papers. Note the substantial variability in the rates across papers.
But it appears that this is a slight tendency for Hamilton to use this particular word more frequently than Madison. Later in this section we will formally perform inference about the ratio of the true rates of use of “can” for the two authors.</p>
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="../LATEX/figures/chapter13/two_authors_can.png" alt="Observed rates of the word can in Federalist Papers authored by Hamilton and Madison." width="500" />
<p class="caption">
Figure 13.1: Observed rates of the word can in Federalist Papers authored by Hamilton and Madison.
</p>
</div>

<p><strong>Poisson density sampling</strong></p>
<p>Consider first the word use of all of the Federalist Papers written by Hamilton.
The initial task is to find a suitable sampling distribution for the counts of a particular function word such as “can”. Since Poisson is a popular sampling distribution for counts, it is initially assumed that for the <span class="math inline">\(i\)</span>-th paper the count <span class="math inline">\(y_i\)</span> of the word “can” has a Poisson density with mean <span class="math inline">\(n_i \lambda /1000\)</span> where <span class="math inline">\(n_i\)</span> is the total number of words and <span class="math inline">\(\lambda\)</span> is the true rate of the word among 1000 words. There are <span class="math inline">\(N\)</span> papers in total. Using the Poisson density expression, one writes
<span class="math display">\[\begin{equation}
f(Y_i = y_i \mid \lambda) = \frac{(n_i \lambda / 1000)^{y_i} \exp(-n_i \lambda / 1000)}{y_i!}.
\end{equation}\]</span>
Assuming independence of word use between papers, the likelihood function is the product of Poisson densities
<span class="math display">\[\begin{equation}
L(\lambda) = \prod_{i = 1}^N f(y_i \mid \lambda),
\end{equation}\]</span>
and the posterior density of <span class="math inline">\(\lambda\)</span> is given by
<span class="math display">\[\begin{equation}
\pi(\lambda \mid y_1, \cdots, y_N) \propto  L(\lambda) \pi(\lambda),
\end{equation}\]</span>
where <span class="math inline">\(\pi()\)</span> is the prior density.</p>

<p><strong>R Work</strong> Suppose one knows little about the true rate of “can”s and to reflect this lack of information, one assigns <span class="math inline">\(\lambda\)</span> a Gamma density with parameters <span class="math inline">\(\alpha = 0.001\)</span> and <span class="math inline">\(\beta = 0.001\)</span>. Recall in Chapter 8 Section 8.8, a Gamma prior is conjugate to a Poisson sampling model. A JAGS script is written to specify this Bayesian model and by use of the <code>run.jags()</code> function, one obtains a simulated sample of 5000 draws from the posterior distribution.</p>
<pre><code>modelString = &quot;
model{
## sampling
for (i in 1:N) {
   y[i] ~ dpois(n[i] * lambda / 1000)
}
## prior
lambda ~ dgamma(0.001, 0.001)
}
&quot;</code></pre>
<p>When one observes count data such as these, one general concern is . Do the observed counts display more variability than one would anticipate with the use of this Poisson sampling model? One can check for overdispersion by use of a posterior predictive check. First one simulates one replicated dataset from the posterior predictive distribution. This is done in two steps: 1) one simulates a value of <span class="math inline">\(\lambda\)</span> from the posterior distribution; 2) given the simulated value <span class="math inline">\(\lambda = \lambda^*\)</span>, one simulates counts <span class="math inline">\(y^{R}_1, ..., y^{R}_N\)</span> from independent Poisson distribution with means <span class="math inline">\(n_1 \lambda^* / 1000, ..., n_N \lambda^* / 1000\)</span>. Given a replicated dataset of counts {<span class="math inline">\(y^{R}_i\)</span>}, one computes the standard deviation. In this setting a standard deviation is a reasonable choice of a testing function since one is concerned about the variation or spread in the data.</p>
<pre><code>one_rep &lt;- function(i){
  lambda &lt;- post[i]
  sd(rpois(length(y), n * lambda / 1000))
}
sapply(1:5000, one_rep) -&gt; SD</code></pre>
<p>One repeats this process 5000 times, obtaining 5000 replicated datasets from the posterior predictive distribution and 5000 values of the standard deviation. Figure 13.2 displays a histogram of the standard deviations from the predictive distribution and the standard deviation of the observed counts {<span class="math inline">\(y_i\)</span>} is displayed as a vertical line. Note that the observed standard deviation is very large relative to the standard deviations of the counts from the predictive distribution. The takeaway is that there is more variability in the observed counts of “can”s than one would predict from the Poisson sampling model.</p>

<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="../LATEX/figures/chapter13/hamilton_pp1.png" alt="Histogram of standard deviations from 5000 replicates from the posterior predictive distribution from the Poisson sampling model.  The observed standard deviation is displayed as a vertical line." width="500" />
<p class="caption">
Figure 13.2: Histogram of standard deviations from 5000 replicates from the posterior predictive distribution from the Poisson sampling model. The observed standard deviation is displayed as a vertical line.
</p>
</div>
</div>
<div id="negative-binomial-sampling" class="section level2">
<h2><span class="header-section-number">13.3</span> Negative Binomial sampling</h2>
<p>In the previous section, we presented evidence that the observed counts of “can” from a group of Federalist Papers of Alexander Hamilton were overdispersed in that there was more variability in the counts than predicted by the Poisson sampling model. One way of handling this overdispersion issue to find an alternative sampling density for the counts that is able to accommodate this additional variation.</p>
<p>One popular alternative density is the Negative Binomial density. Recall that <span class="math inline">\(y_i\)</span> represents the number of “can”s in the <span class="math inline">\(i\)</span>-th Federalist Papers. Conditional on parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, one assigns <span class="math inline">\(y_i\)</span> the Negative Binomial density defined as
<span class="math display">\[\begin{equation}
f(Y_i = y_i \mid \alpha, \beta) = \frac{\Gamma(y_i + \alpha)}{\Gamma(\alpha)} p_i^\alpha (1 - p_i)^{y_i},
\end{equation}\]</span>
where
<span class="math display">\[\begin{equation}
p_i = \frac{\beta}{\beta + n_i / 1000}.
\end{equation}\]</span>
One can show that this density is a natural generalization of the Poisson density. The mean count is given by <span class="math inline">\(E(y_i) = \mu_i\)</span> where
<span class="math display">\[\begin{equation}
\mu_i = \frac{n_i}{1000}\frac{\alpha}{\beta}.
\end{equation}\]</span>
Recall that the mean count for <span class="math inline">\(y_i\)</span> the Poisson model was <span class="math inline">\(n_i \lambda / 1000\)</span>, so the ratio <span class="math inline">\(\alpha / \beta\)</span> is playing the same role as <span class="math inline">\(\lambda\)</span> – one can regard <span class="math inline">\(\alpha / \beta\)</span> as the true rate of the particular word per 1000 words.</p>
<p>One can show that the variance of the count <span class="math inline">\(y_j\)</span> is given by
<span class="math display">\[\begin{equation}
Var(y_i) = \mu_i \left(1 + \frac{n_i}{1000 \beta}\right).
\end{equation}\]</span>
The variance for the Poisson model is equal to <span class="math inline">\(\mu_i\)</span>, so the Negative Binomial model has the extra multiplicative term <span class="math inline">\(\left(1 + \frac{n_i}{1000 \beta}\right)\)</span>. So the Negative Binomial family is able to accommodate the additional variability in the counts {<span class="math inline">\(y_i\)</span>}.</p>
<p>The posterior analysis using a Negative Binomial density is straightforward. The counts <span class="math inline">\(y_1, ..., y_N\)</span> are independent Negative Binomial with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> and the likelihood function is equal to
<span class="math display">\[\begin{equation}
L(\alpha, \beta) = \prod_{i=1}^N f(y_i \mid \alpha, \beta).
\end{equation}\]</span>
If little is known a priori about the locations of the positive parameter values <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, then it reasonable to assume the two parameters are independent and assign to each <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> a Gamma density with parameters 0.001 and 0.001. Then the posterior density is given by
<span class="math display">\[\begin{equation}
\pi(\alpha, \beta \mid y_1, \cdots, y_N) \propto L(\alpha, \beta) \pi(\alpha, \beta)
\end{equation}\]</span>
where <span class="math inline">\(\pi(\alpha, \beta)\)</span> is the product of Gamma densities.</p>

<p><strong>R Work</strong> One simulates the posterior with Negative Binomial sampling using JAGS. The Negative Binomial density is represented by the JAGS function <code>dnegbin()</code> with parameters <code>p[i]</code> and <code>alpha</code>. In the JAGS script below, note that one first defines <code>p[i]</code> in terms of the parameter <code>beta</code> and the sample size <code>n[i]</code>, and then expresses the Negative Binomial density in terms of <code>p[i]</code> and <code>alpha</code>.</p>
<pre><code>modelString = &quot;
model{
## sampling
for(i in 1:N){
   p[i] &lt;- beta / (beta + n[i] / 1000)
   y[i] ~ dnegbin(p[i], alpha)
}
## priors
mu &lt;- alpha / beta
alpha ~ dgamma(.001, .001)
beta ~ dgamma(.001, .001)
}
&quot;</code></pre>
<p>We earlier made a statement that the Negative Binomial density can accommodate the extra variability in the word counts. One can check this statement by a posterior predictive check. One replication of the posterior predictive checking method is implemented in the R function <code>one_rep()</code>. We start with a simulated value <span class="math inline">\((\alpha^*, \beta^*)\)</span> from the posterior distribution. Then we simulated a replicated dataset <span class="math inline">\(y^{R}_1, ..., y^{R}_N\)</span> where <span class="math inline">\(y^{R}_i\)</span> has a Negative Binomial distribution with parameters <span class="math inline">\(\alpha^*\)</span> and <span class="math inline">\(\beta^* / (\beta^* + n_i / 1000)\)</span>. Then we compute the standard deviation of the {<span class="math inline">\(y^{R}_i\)</span>}.</p>
<pre><code>one_rep &lt;- function(i){
  p &lt;- post$beta[i] / (post$beta[i] + n / 1000)
  sd(rnbinom(length(y), size = post$alpha[i], prob = p))
}</code></pre>
<p>By repeating this algorithm for 5000 iterations, one has 5000 draws of the standard deviation of samples from the predictive distribution stored in the R vector</p>
<pre><code>sapply(1:5000, one_rep) -&gt; SD</code></pre>
<p>Figure 13.3 displays a histogram of the standard deviations of samples from the predictive distribution and the observed standard deviation of the counts is shown as a vertical line. In this case the observed standard deviation value is in the middle of the predictive distribution. The interpretation is that predictions with a Negative Binomial sampling model are consistent with the spread in the observed word counts.</p>

<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="../LATEX/figures/chapter13/hamilton_pp2.png" alt="Histogram of standard deviations from 5000 replicates from the posterior predictive distribution in the Negative Binomial sampling model.  The observed standard deviation is displayed as a vertical line." width="500" />
<p class="caption">
Figure 13.3: Histogram of standard deviations from 5000 replicates from the posterior predictive distribution in the Negative Binomial sampling model. The observed standard deviation is displayed as a vertical line.
</p>
</div>
<p>Now that the Negative Binomial model seems reasonable, one performs inferences about the mean use of the word “can” in Hamilton essays. The parameter <span class="math inline">\(\mu = \alpha / \beta\)</span> represents the true rate of use of this word per 1000 words. Figure 13.4 displays MCMC diagnostic plots for the parameter <span class="math inline">\(\mu\)</span>. The trace plot and autocorrelation plot indicate good mixing and so one believes the histogram in the lower-left section represents the marginal posterior density for <span class="math inline">\(\mu\)</span>. A 90% posterior interval estimate for the rate of “can” is (2.20, 3.29).</p>

<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="../LATEX/figures/chapter13/hamilton_post1.png" alt="MCMC diagnostic plots for the rate $\mu$ of use of the word can in Hamilton essays." width="500" />
<p class="caption">
Figure 13.4: MCMC diagnostic plots for the rate <span class="math inline">\(\mu\)</span> of use of the word can in Hamilton essays.
</p>
</div>
</div>
<div id="comparison-of-rates-for-two-authors" class="section level2">
<h2><span class="header-section-number">13.4</span> Comparison of rates for two authors</h2>
<p>Recall that the original problem was to compare the word use of Alexander Hamilton with that of James Madison. Suppose we collect the counts {<span class="math inline">\(y_{1i}\)</span>} of the word “can” in the Federalist Papers authored by Hamilton and the counts {<span class="math inline">\(y_{2i}\)</span>} of “can” in the Federalist Papers authored by Madison. The general problem is to compare the true rates per 1000 words of the two authors.</p>
<p>Since a Negative Binomial sampling model appears to be suitable in the one-sample situation, we extend this in a straightforward away to the two-sample case. The Hamilton counts <span class="math inline">\(y_{11}, ..., y_{1N_1}\)</span>, conditional on parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> are assumed to be independent Negative Binomial, where <span class="math inline">\(y_{1i}\)</span> is Negative Binomial(<span class="math inline">\(p_{1i}, \alpha_1\)</span>) with
<span class="math display">\[\begin{equation}
p_{1i} = \frac{\beta_1}{\beta_1 + n_{1i}/1000},
\end{equation}\]</span>
and <span class="math inline">\(\{n_{1i}\}\)</span> are the word counts for the Hamilton essays. Similarly, the Madison counts <span class="math inline">\(y_{21}, ..., y_{2N_2}\)</span>, conditional on parameters <span class="math inline">\(\alpha_2\)</span> and <span class="math inline">\(\beta_2\)</span> are assumed to be independent Negative Binomial, where <span class="math inline">\(y_{2i}\)</span> is Negative Binomial(<span class="math inline">\(p_{2i}, \alpha_2\)</span>) with
<span class="math display">\[\begin{equation}
p_{2i} = \frac{\beta_2}{\beta_2 + n_{2i}/1000},
\end{equation}\]</span>
and {<span class="math inline">\(n_{2i}\)</span>} are the word counts for the Madison essays. The focus will be to learn about <span class="math inline">\(\mu_M / \mu_H\)</span>, the ratio of the rates (per 1000 words) of use of the word “can” of the two authors, where <span class="math inline">\(\mu_M = \alpha_2 / \beta_2\)</span> and <span class="math inline">\(\mu_H = \alpha_1 / \beta_1\)</span>.</p>
<p>Assume that the observed counts of word “can” of the two authors are independent. Moreover, assume that the prior distributions of the parameters <span class="math inline">\((\alpha_1, \beta_1)\)</span> and <span class="math inline">\((\alpha_2, \beta_2)\)</span> are independent. Then the posterior distribution is given, up to an unknown proportionality constant, by
<span class="math display">\[\begin{equation}
\pi(\alpha_1, \beta_1, \alpha_2, \beta_2 \mid \{y_{1i}\}, \{y_{12}\}) \propto \prod_{k=1}^2 \left(
\prod_{i=1}^{n_{ki}} f(y_{ki} \mid \alpha_k, \beta_k) \pi(\alpha_k, \beta_k) \right).
\end{equation}\]</span>
We assume that the user has little prior information about the location of the Negative Binomial parameters and we assume they are independent with each parameter assigned a Gamma prior with parameters 0.001 and 0.001.</p>

<p><strong>R Work</strong> The posterior sampling is implemented using the JAGS software. The model description script is an extension of the previous script for a single Negative Binomial sample. Note that the <code>ratio</code> parameter is defined to be the ratio of the word rates for the two samples.</p>
<pre><code>modelString = &quot;
model{
## sampling
for(i in 1:N1){
   p1[i] &lt;- beta1 / (beta1 + n1[i] / 1000)
   y1[i] ~ dnegbin(p1[i], alpha1)
}
for(i in 1:N2){
   p2[i] &lt;- beta2 / (beta2 + n2[i] / 1000)
   y2[i] ~ dnegbin(p2[i], alpha2)
}
## priors
alpha1 ~ dgamma(.001, .001)
beta1 ~ dgamma(.001, .001)
alpha2 ~ dgamma(.001, .001)
beta2 ~ dgamma(.001, .001)
ratio &lt;- (alpha2 / beta2) / (alpha1 / beta1)
}&quot;</code></pre>
<p>Since the focus is to compare the word use of the two authors, Figure 13.5 displays MCMC diagnostics for the ratio of “can” rates <span class="math inline">\(R = \mu_M / \mu_H\)</span>. Note that most of the posterior probability of <span class="math inline">\(R\)</span> is found in an interval about the value one.
From the simulated draws, one finds the posterior median is 0.92 and a 95% probability interval for <span class="math inline">\(R\)</span> is found to be (0.71, 1.19). Since this interval contains the value one, there is no significant evidence to conclude that Hamilton and Madison have different rates of the word “can”.</p>

<div class="figure"><span id="fig:unnamed-chunk-5"></span>
<img src="../LATEX/figures/chapter13/two_authors_post2.png" alt="MCMC diagnostic plots for the ratio of rates of use of the word can in Federalist Papers essays written by Hamilton and Madison." width="500" />
<p class="caption">
Figure 13.5: MCMC diagnostic plots for the ratio of rates of use of the word can in Federalist Papers essays written by Hamilton and Madison.
</p>
</div>
</div>
<div id="which-words-distinguish-the-two-authors" class="section level2">
<h2><span class="header-section-number">13.5</span> Which words distinguish the two authors?</h2>
<p>In the previous section, it was found that the word “can” was not a helpful discriminator between the essays written by Hamilton and the essays written by Madison. However, other words may be useful in this discrimination task. Following suggestions in Mosteller and Wallace (1963), the previous two-sample analysis was repeated for each of the following words: also, an, any, by, can, from, his, may, of, on, there, this, to, and upon. For a given word, the counts of occurrence of that word was collected for each of the essays authored by Hamilton and Madison. For each word, we focus on inferences about the parameter <span class="math inline">\(R\)</span>, the ratio of mean rates of the particular word by Madison and Hamilton. A ratio value of <span class="math inline">\(R &gt; 1\)</span> indicates that Madison was a more frequent user of the word, and a ratio value <span class="math inline">\(R &lt; 1\)</span> indicates that Hamilton used a higher rate of that word. Fourteen separate two-sample analyses were conducted and the posterior distributions of <span class="math inline">\(R\)</span> were summarized by posterior medians and 95% probability intervals.</p>
<p>Figure 13.6 displays the locations of the posterior medians and interval estimates for all of the 14 analyses. Intervals that are completely on one side of the value <span class="math inline">\(R = 1\)</span> indicate that one author was more likely to use that particular word. Looking at the figure, one sees that the words upon, to, this, there, any, and an were more likely be used by Hamilton, and the words on, by, and also were more likely be used by Madison. The posterior intervals for the remaining words (may, his, from, can, and also) cover the value one, and so one cannot say from these data that one author was more likely to use those particular words.</p>

<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="../LATEX/figures/chapter13/two_authors_many.png" alt="Display of posterior median and 95% interval estimates for the ratio of rates for 14 different words in Federalist Papers essays written by Hamilton and Madison." width="500" />
<p class="caption">
Figure 13.6: Display of posterior median and 95% interval estimates for the ratio of rates for 14 different words in Federalist Papers essays written by Hamilton and Madison.
</p>
</div>
</div>
<div id="career-trajectories" class="section level2">
<h2><span class="header-section-number">13.6</span> Career Trajectories</h2>
<p></p>
<p><strong>Introduction</strong></p>
<p>For an athlete in a professional sport, his or her performance typically begins at a small level, increases to a level in the middle of his or her career where the player has peak performance, and then decreases until the player’s retirement. This pattern of performance over a player’s career is called the <strong>career trajectory.</strong> A general problem in sports is to predict future performance of a player and one relevant variable in this prediction is the player’s age. Due to the ready availability of baseball data, it is convenient to study career trajectories for baseball players, although the methodology will apply to athletes in other sports.</p>
<p><strong>Measuring hitting performance in baseball</strong></p>
<p>Baseball is a bat and ball game first played professionally in the United States in the mid 19th century. Players are measured by their ability to hit, pitch, and field, and a wide variety of statistical measures have been developed. One of the more popular measures of batting performance is the on-base percentage or OBP. A player comes to bat during a <strong>plate appearance</strong> and it is desirable for the batter to get on base. The OBP is defined to be the fraction of plate appearances where the batter reaches a base. As an example, during the 2003 season, Chase Utley had 49 on-base events in 152 plate appearances and his OBP was <span class="math inline">\(49 / 152 = 0.322\)</span>.</p>
<p><strong>A hitter’s career trajectory</strong></p>
<p>A baseball player typically plays between 5 to 20 years in Major League Baseball (MLB), the top-tier professional baseball league in the United States. In this case study, we explore career trajectories of the OBP measure of baseball players as a function of their ages.
To illustrate a career trajectory, consider Chase Utley who played in the Major Leagues from 2003 through 2018. Figure 13.7 displays Utley’s OBP as a function of his age for all of the seasons of his career. A quadratic smoothing curve is added to the scatterplot. One sees that Utley’s OBP measure increases until about age 30 and then steadily decreases towards the end of his career.</p>
<div class="figure"><span id="fig:unnamed-chunk-7"></span>
<img src="../LATEX/figures/chapter13/utley1.png" alt="Career trajectory of Chase Utley's on-base percentages.  A quadratic smoothing curve is added to the plot." width="500" />
<p class="caption">
Figure 13.7: Career trajectory of Chase Utley’s on-base percentages. A quadratic smoothing curve is added to the plot.
</p>
</div>

<p>Figure 13.8 displays the career trajectory of OBP for another player Josh Phelps who had a relatively short baseball career. In contrast, Phelps does not have a clearly defined career trajectory. In fact, Phelps’ OBP values appear to be relatively constant from ages 24 to 30 and the quadratic smoothing curve indicates that Phelps had a minimum OBP at age 26. The purpose of this case study is to see if one can improve the career trajectory smooth of this player by a hierarchical Bayesian model that combines data from a number of baseball players. Recall in Chapter 10, we have seen how hierarchical Bayesian models have the pooling effect that could borrow information from other groups to improve the estimation of one group, especially for groups with small sample size.</p>
<div class="figure"><span id="fig:unnamed-chunk-8"></span>
<img src="../LATEX/figures/chapter13/phelps1.png" alt="Career trajectory of Josh Phelps' on-base percentages.  A quadratic smoothing curve is added to the plot." width="500" />
<p class="caption">
Figure 13.8: Career trajectory of Josh Phelps’ on-base percentages. A quadratic smoothing curve is added to the plot.
</p>
</div>

<p><strong>Estimating a single trajectory</strong>
</p>
<p>First we consider learning about a single hitter’s OBP career trajectory. Let <span class="math inline">\(y_j\)</span> denote the number of on-base events in <span class="math inline">\(n_j\)</span> plate appearances during a hitter’s <span class="math inline">\(j\)</span>-th season. It is reasonable to assume that <span class="math inline">\(y_j\)</span> has a Binomial distribution with parameters <span class="math inline">\(n_j\)</span> and probability of success <span class="math inline">\(p_j\)</span>. One represents the logit of the success probability as a quadratic function of the player’s age:
<span class="math display">\[\begin{equation}
\log \left(\frac{p_j}{1-p_j}\right) = \beta_0 + \beta_1 (x_j - 30) + \beta_2 (x_j - 30)^2,
\end{equation}\]</span>
where <span class="math inline">\(x_j\)</span> represents the age of the player in the <span class="math inline">\(j\)</span>-th season.</p>
<p>Note that the age value is centered by 30 in the logistic model – this is done for ease of interpretation. The intercept <span class="math inline">\(\beta_0\)</span> is an estimate of the player’s OBP performance at age 30. Specific functions of the regression vector <span class="math inline">\(\beta = (\beta_0, \beta_1, \beta_2)\)</span> are of specific interest in this application.</p>
<ol style="list-style-type: decimal">
<li>The quadratic function reaches its largest value at
<span class="math display">\[\begin{equation*}
h_1(\beta) = 30 -  \frac{\beta_1}{2 \beta_2}.
\end{equation*}\]</span>
This is the age where the player is estimated to have his peak on-base performance during his career.</li>
<li>The maximum value of the curve, on the logistic scale, is
<span class="math display">\[\begin{equation*}
h_2(\beta) = \beta_0 - \frac{\beta_1^2}{4 \beta_2}. 
\end{equation*}\]</span>
The maximum value of the curve on the probability scale is
<span class="math display">\[\begin{equation}
p_{max} = \exp(h_2(\beta)) / (1 +  \exp(h_2(\beta))).
\end{equation}\]</span>
The parameter <span class="math inline">\(p_{max}\)</span> is the estimated largest OBP of the player over his career.</li>
<li>The coefficient <span class="math inline">\(\beta_2\)</span>, typically a negative value, tells us about the degree of curvature in the quadratic function.<br />
If a player has a “large” value of <span class="math inline">\(\beta_2\)</span>, this indicates that he more rapidly reaches his peak level and more rapidly decreases in ability until retirement. One simple interpretation is that <span class="math inline">\(\beta_2\)</span> represents the change in OBP from his peak age to one year later.</li>
</ol>
<p>It is straightforward to fit this Bayesian logistic model using the JAGS software. Suppose one has little prior information about the location of the regression vector <span class="math inline">\(\beta\)</span>. Then one assumes the regression coefficients are independent with each coefficient assigned a Normal prior with mean 0 and precision 0.0001. The posterior density of <span class="math inline">\(\beta\)</span> is given, up to an unknown proportionality constant, by
<span class="math display">\[\begin{equation}
\pi(\beta \mid \{y_j\}) \propto \prod_j \left( p_j ^{y_j}(1 - p_j)^{n_j - y_j} \right) \pi(\beta),
\end{equation}\]</span>
where <span class="math inline">\(p_j\)</span> is defined by the logistic model and <span class="math inline">\(\pi(\beta)\)</span> is the prior density.</p>

<p><strong>R Work</strong> The JAGS model script is shown below. The <code>dbin()</code> function is used to define the Binomial distribution and the <code>logit()</code> function describes the log odds reexpression.</p>
<pre><code>modelString = &quot;
model {
## sampling
for (j in 1:N){
   y[j] ~ dbin(p[j], n[j])
   logit(p[j]) &lt;- beta0 + beta1 * (x[j] - 30) +
            beta2 * (x[j] - 30) * (x[j] - 30)
}
## priors
beta0 ~ dnorm(0, 0.0001)
beta1 ~ dnorm(0, 0.0001)
beta2 ~ dnorm(0, 0.0001)
}
&quot;</code></pre>
<p>The JAGS software is used to simulate a sample from the posterior distribution of the regression vector <span class="math inline">\(\beta\)</span>. From this sample, it is straightforward to learn about any function of the regression vector of interest. To illustrate, one performs inference about the peak age function <span class="math inline">\(h_1(\beta)\)</span> by computing this function on the simulated <span class="math inline">\(\beta\)</span> draws – the output is a posterior sample from the peak age function. In a similar fashion, one obtains a sample from the posterior of the maximum value function <span class="math inline">\(p_{max}\)</span> by computing this function on the simulated <span class="math inline">\(\beta\)</span> values. Figure 13.9 displays density estimates of the simulated values of <span class="math inline">\(h_1(\beta)\)</span> and <span class="math inline">\(p_{max}\)</span>. From this graph, one sees that Utley’s peak performance was most likely achieved at age 29, although there is uncertainty about this most likely peak age. Also the posterior of the peak value <span class="math inline">\(p_{max}\)</span> indicates that Utley’s peak on-base probability ranged from 0.38 and 0.40.</p>
<div class="figure"><span id="fig:unnamed-chunk-9"></span>
<img src="../LATEX/figures/chapter13/utley2.png" alt="Density estimates of the peak age and peak for logistic model on Chase Utley's trajectory." width="500" />
<p class="caption">
Figure 13.9: Density estimates of the peak age and peak for logistic model on Chase Utley’s trajectory.
</p>
</div>

<p><strong>Estimating many trajectories by a hierarchical model</strong></p>
<p>We have focused on estimating the career trajectory of a single baseball player such as Chase Utley. But there are many baseball players and it is reasonable to want to simultaneously estimate the career trajectories for a group of players. As an example, suppose one focuses on the Major League players who were born in the year 1978 and had at least 1000 career at-bats. Figure 13.10 displays scatterplots of age and OBP with quadratic smoothing curves for the 36 players in this group. Looking at these curves, one notices that many of the curves follow a familiar concave down shape with the player achieving peak performance near an age of 30. But for some players, especially for those players who played a small number of seasons, note that the trajectories have different shapes. Some trajectories are relatively constant over the age variable and other trajectories have an unusual concave up appearance.</p>
<div class="figure"><span id="fig:unnamed-chunk-10"></span>
<img src="../LATEX/figures/chapter13/traj_all.png" alt="Career trajectories and individual quadratic fits for all players born in the year 1978 and having at least 1000 career at-bats." width="500" />
<p class="caption">
Figure 13.10: Career trajectories and individual quadratic fits for all players born in the year 1978 and having at least 1000 career at-bats.
</p>
</div>

<p>In this situation, it may be desirable to partially pool the data from the 36 players using a hierarchical model to obtain improved trajectory estimates for all players. For the <span class="math inline">\(i\)</span>-th player, one observes the on-base events {<span class="math inline">\(y_{ij}\)</span>} where <span class="math inline">\(y_{ij}\)</span> is Binomial with sample size <span class="math inline">\(n_{ij}\)</span> and probability of on-base success <span class="math inline">\(p_{ij}\)</span>. The logit of the on-base probability for the <span class="math inline">\(i\)</span>-th player during the <span class="math inline">\(j\)</span>-th season is given by
<span class="math display">\[\begin{equation}
\log \left(\frac{p_{ij}}{1-p_{ij}}\right) = \beta_{i0} + \beta_{i1} (x_{ij} - 30) + \beta_{i2} (x_{ij} - 30)^2,
\end{equation}\]</span>
where <span class="math inline">\(x_{ij}\)</span> is the age of the <span class="math inline">\(i\)</span>-th player during the <span class="math inline">\(j\)</span>-th season. If <span class="math inline">\(\beta_i = (\beta_{i0}, \beta_{i1}, \beta_{i2})\)</span> represents the vector regression coefficients for the <span class="math inline">\(i\)</span>-th player, then one is interested in estimating the regression vectors <span class="math inline">\((\beta_1, ..., \beta_N)\)</span> for the <span class="math inline">\(N\)</span> players in the study.</p>
<p>One constructs a two-stage prior on these regression vectors. In Chapter 10, one assumed that the Normal means were distributed according to a common normal distribution. In this setting, since each regression vector has three components, at the first stage of the prior, one assumes that
<span class="math inline">\(\beta_1, ..., \beta_N\)</span> are independent distributed from a common multivariate Normal distribution with mean vector <span class="math inline">\(\mu_\beta\)</span> and precision matrix <span class="math inline">\(\tau_\beta\)</span>. Then, at the second stage, vague prior distributions are assigned to the unknown values of <span class="math inline">\(\mu_\beta\)</span> and <span class="math inline">\(\tau_\beta\)</span>.</p>

<p><strong>R Work</strong> In our application, there are <span class="math inline">\(N = 36\)</span> players, so one is estimating 36 <span class="math inline">\(\times\)</span> 3 = 108 regression parameters together with unknown parameters in the prior distributions of <span class="math inline">\(\mu_\beta\)</span> and <span class="math inline">\(\tau_\beta\)</span> at the second stage. Fortunately the JAGS script defining this model is a straightforward extension of the JAGS script for a logistic regression model for a single career trajectory. The variable <code>player</code> indicates the player number, and the variables <code>beta0[i]</code>, <code>beta1[i]</code>, and <code>beta2[i]</code> represent the logistic regression parameters for the <span class="math inline">\(i\)</span>-th player. The vector <code>B[j, 1:3]</code> represents a vector of parameters for one player and <code>mu.beta</code> and <code>Tau.B</code> represent respectively the second-stage prior mean vector and precision matrix values. The variables <code>mean</code>, <code>prec</code>, <code>Omega</code> are specified parameters that indicate weak information about the parameters at the second stage.</p>
<pre><code>modelString = &quot;
model {
## sampling
for (i in 1:N){
   y[i] ~ dbin(p[i], n[i])
   logit(p[i]) &lt;- beta0[player[i]] + 
                beta1[player[i]] * (x[i] - 30) +
                beta2[player[i]] * (x[i] - 30) * (x[i] - 30)
}
## priors
for (j in 1:J){
   beta0[j] &lt;- B[j,1]
   beta1[j] &lt;- B[j,2]
   beta2[j] &lt;- B[j,3]
   B[j,1:3] ~ dmnorm (mu.beta[], Tau.B[,])
}
mu.beta[1:3] ~ dmnorm(mean[1:3],prec[1:3 ,1:3 ])
Tau.B[1:3 , 1:3] ~ dwish(Omega[1:3 ,1:3 ], 3)
}
&quot;</code></pre>
<p>After JAGS is used to simulate from the posterior distribution of this hierarchical model, a variety of inferences are possible. The player trajectories <span class="math inline">\(\beta_1, ..., \beta_{36}\)</span> are a sample from a Normal distribution with mean <span class="math inline">\(\mu_\beta\)</span>. Figure 13.11 displays draws of the posterior of the mean peak age <span class="math inline">\(h_1(\mu_\beta)\)</span> expressed as probabilities over a grid of age values from 23 to 37. The takeaway if that the career trajectories appear to be centered about 29.5 – a typical MLB player in this group peaks in on-base performance about age 29.5.</p>
<div class="figure"><span id="fig:unnamed-chunk-11"></span>
<img src="../LATEX/figures/chapter13/meantrajectory.png" alt="Samples from the posterior distribution of the mean peak age." width="500" />
<p class="caption">
Figure 13.11: Samples from the posterior distribution of the mean peak age.
</p>
</div>

<p>By combining data across players, the Bayesian hierarchical model is helpful in borrowing information for estimating the career trajectories of players with limited career data. This is illustrated in Figure 13.12 that shows individual and hierarchical posterior mean fits of the career trajectories for two players. For Chase Utley, the two fits are very similar since Utley’s career trajectory was well-estimated just using his data. In contrast, we saw that Phelps had an unrealistic concave up individual estimated trajectory. In the hierarchical model, this career trajectory is corrected to be more similar to the concave down trajectory for most players.</p>

<div class="figure"><span id="fig:unnamed-chunk-12"></span>
<img src="../LATEX/figures/chapter13/twoplayers.png" alt="Individual (solid line) and hierarchical (dashed line) fits of the career trajectories for  Josh Phelps and Chase Utley." width="500" />
<p class="caption">
Figure 13.12: Individual (solid line) and hierarchical (dashed line) fits of the career trajectories for Josh Phelps and Chase Utley.
</p>
</div>
</div>
<div id="latent-class-modeling" class="section level2">
<h2><span class="header-section-number">13.7</span> Latent Class Modeling</h2>
<p></p>
<p><strong>Two classes of test takers</strong></p>
<p>Suppose thirty people are given a 20-question true/false exam and the number of correct responses for all people are graphed in Figure 13.13.
From this figure note that test takers 1 through 10 appear to have a low level of knowledge about the subject matter as their scores are centered around 10. The remaining test takers 11 through 30 seem to have a higher level of knowledge as their scores range from 15 to 20.</p>
<div class="figure"><span id="fig:unnamed-chunk-13"></span>
<img src="../LATEX/figures/chapter13/ScorePlotText.png" alt="Scatterplot of test scores of 20 test takers. The number next to each point is the person index." width="500" />
<p class="caption">
Figure 13.13: Scatterplot of test scores of 20 test takers. The number next to each point is the person index.
</p>
</div>

<p>Are there really two groups of test takers, a random-guessing group and a knowledgeable group? If so, how can one separate the people in the two ability groups, and how can one make inferences about the correct rate for each group? Furthermore, can one be sure that there exists only two ability groups? Is it possible to have more than two groups of people by ability level?</p>
<p>The above questions relate to the classification of observations and the number of classes. In the introduction of hierarchical models in Chapter 10, there was a natural grouping of the observations. For example, in the animation movie ratings example in Chapter 10, each rating was made on one animation movie, so grouping based on movie is natural, and the group assignment of the observations was known. It was then reasonable to specify a two-stage prior where the rating means shared the same prior distribution at the first stage.</p>
<p>In contrast, in the true/false exam example, since the group assignment is not known, it not possible to proceed with a hierarchical model with a common prior at the first stage.
In this testing example one believes the people fall in two ability groups, however one does not observe the actual classification of the people into groups. So it is assumed that there exists <strong>latent</strong> or unobserved classification of observations. The class assignments of the individuals are unknown and can be treated as random parameters in our Bayesian approach.</p>
<p>If there exists two classes, the class assignment parameter for the <span class="math inline">\(i\)</span>-th observation <span class="math inline">\(z_i\)</span> is unknown and assumed to follow a Bernoulli distribution with probability <span class="math inline">\(\pi\)</span> belonging to the first class, i.e. <span class="math inline">\(z_i = 1\)</span>. With probability <span class="math inline">\(1 - \pi\)</span> the <span class="math inline">\(i\)</span>-th observation belongs to the second class, i.e. <span class="math inline">\(z_i = 0\)</span>.
<span class="math display">\[\begin{equation}
z_i \mid \pi \sim \textrm{Bernoulli}(\pi).
\label{eq:Bern}
\end{equation}\]</span>
If one believes there are more than two classes, the class assignment parameter follows a Multinomial distribution. For ease of description of the model, we focus on the two classes situation.</p>
<p>Once the class assignment <span class="math inline">\(z_i\)</span> is known for observation <span class="math inline">\(i\)</span>, the response variable <span class="math inline">\(Y_i\)</span> follows a data model with a group-specific parameter. In the case of a true/false exam
where the outcome variable <span class="math inline">\(Y_i\)</span> is the number of correct answers, the Binomial model is a good choice for a sampling model. The response variable <span class="math inline">\(Y_i\)</span> conditional on the class assignment variable <span class="math inline">\(z_i\)</span> is assigned a Binomial distribution with probability of success <span class="math inline">\(p_{z_i}\)</span>.
<span class="math display">\[\begin{equation}
Y_i = y_i \mid z_i, p_{z_i} \sim \textrm{Binomial}(20, p_{z_i}).
\label{eq:BinoLik}
\end{equation}\]</span>
One writes the success probability <span class="math inline">\(p_{z_i}\)</span> with subscript <span class="math inline">\(z_i\)</span> since this probability is class-specific. For the guessing group, the number of correct answers is Binomial with parameter <span class="math inline">\(p_1\)</span>, and for the knowledgable group the number of correct answers is Binomial with parameter <span class="math inline">\(p_0\)</span>.</p>
<p>This model for responses to a true/false with unknown ability levels illustrates latent class modeling. The fundamental assumption is that there exists unobserved two latent classes of observations, and each latent class has its own sampling model with class-specific parameters.
All <span class="math inline">\(n\)</span> observations belong to one of the two latent classes and each observations is assigned to the latent classes one and two with respective probabilities <span class="math inline">\(\pi\)</span> and <span class="math inline">\((1 - \pi)\)</span>. From Equation (13.17), once the latent class assignment is determined, the outcome variable <span class="math inline">\(y_i\)</span> follows a class-specific data model as in Equation (13.18).</p>
<p>The tree diagram below illustrates the latent class model.</p>
<p><img src="../LATEX/figures/chapter13/treediagram.png" width="500" /></p>
<p>To better understand this latent class model,
consider in a thought experiment where one simulates outcomes <span class="math inline">\(y_1, \cdots, y_n\)</span> from this model.</p>
<ul>
<li><p>Step 1: First simulate the latent class assignments of the <span class="math inline">\(n\)</span> test takers. One samples <span class="math inline">\(n\)</span> values, <span class="math inline">\(z_1, \cdots, z_n\)</span>, from a Bernoulli distribution with probability <span class="math inline">\(\pi\)</span>. Once the latent class assignments are simulated, one has partitioned the test takers into the random-guessing group where <span class="math inline">\(z_i = 1\)</span> and the knowledgeable group where <span class="math inline">\(z_i = 0\)</span>.</p></li>
<li><p>Step 2: Now that the test takers’ classifications are known, the outcomes are simulated by the use of Binomial distributions.
If a test taker’s classification is <span class="math inline">\(z_i = 1\)</span>, she guesses at each question with success probability <span class="math inline">\(p_1\)</span> and one observes the test score which is the Binomial outcome <span class="math inline">\(Y_i \sim \textrm{Binomial}(20, p_1)\)</span>. Otherwise if the classification is <span class="math inline">\(z_i = 0\)</span>, she answers a question correctly with probability <span class="math inline">\(p_0\)</span> and one observes the test score <span class="math inline">\(Y_i \sim \textrm{Binomial}(20, p_0)\)</span>.</p></li>
</ul>
<p>Latent class models provide the flexibility of allowing unknown class assignments of observations and the ability to cluster observations with similar characteristics. In the true/false exam example, the fitted latent class model will pool one class of observations with a lower success rate and pool other class with a higher success rate. This fitted model also estimates model parameters for each class, providing insight of features of each latent class.</p>
<p><strong>A latent class model with two classes</strong></p>
<p>This section builds on the previous section to describe the details of the model specification of a latent class model with two classes for the true/false exam example. The JAGS software is used for MCMC simulation and several inferences are described such as identifying the class for each test taker and learning about the success rate for each class.</p>
<p>Suppose the true/false exam has <span class="math inline">\(m\)</span> questions and <span class="math inline">\(y_i\)</span> denotes the score of observation <span class="math inline">\(i\)</span>, <span class="math inline">\(i = 1, \cdots, n\)</span>. Assume there are two latent classes and each observation belongs to one of the two latent classes. Let <span class="math inline">\(z_i\)</span> be the class assignment for observation <span class="math inline">\(i\)</span> and <span class="math inline">\(\pi\)</span> be the probability of being assigned to class 1.
Given the latent class assignment <span class="math inline">\(z_i\)</span> for observation <span class="math inline">\(i\)</span>, the score <span class="math inline">\(Y_i\)</span> follows a Binomial distribution with <span class="math inline">\(m\)</span> trials and a class-specific success probability. Since there are only two possible class assignments, all observations assigned to class 1 share the same correct success parameter <span class="math inline">\(p_1\)</span> and all observations assigned to class 0 share the same success rate parameter <span class="math inline">\(p_0\)</span>.
The specification of the data model is expressed as follows:
<span class="math display">\[\begin{eqnarray}
Y_i = y_i \mid z_i, p_{z_i} &amp;\sim&amp; \textrm{Binomial}(m, p_{z_i}), \\
\label{eq:Bern2} z_i \mid \pi &amp;\sim&amp; \textrm{Bernoulli}(\pi). 
\end{eqnarray}\]</span></p>
<p>In this latent class model there are many unknown parameters. One does not know the class assignment probability <span class="math inline">\(\pi\)</span>, the class assignments <span class="math inline">\(z_1, ..., z_n\)</span>, and the probabilities <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_0\)</span> for the two Binomial distributions. Some possible choices for prior distributions are discussed in this section.</p>
<ul>
<li><p>The parameters <span class="math inline">\(\pi\)</span> and <span class="math inline">\((1 - \pi)\)</span> are the latent class assignment probabilities for the two classes. If additional information is available which indicates, for example, that 1/3 of the observations belonging to class 1, then <span class="math inline">\(\pi\)</span> is considered as fixed and set to the value of 1/3.
If no such information is available, one can consider <span class="math inline">\(\pi\)</span> as unknown and assign this parameter a prior distribution. A natural choice for prior on a success probability is a Beta prior distribution
with shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p></li>
<li><p>The parameters <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_0\)</span> are the success rates in the Binomial model in the two classes. If one believes that the test takers in class 1 are simply random guessers, then one fixes <span class="math inline">\(p_1\)</span> to the value of 0.5. Similarly, if one believes that test takers in class 0 have a higher success rate of 0.9, then one sets <span class="math inline">\(p_0\)</span> to the value 0.9. However, if one is uncertain about the values of <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_0\)</span>, one lets either or both success rates be random and assigned prior distributions.</p></li>
</ul>
<p><strong>Scenario 1: known parameter values</strong></p>
<p>We begin with a simplified version of this latent class model.
Consider the use of the fixed values <span class="math inline">\(\pi = 1/3\)</span> and <span class="math inline">\(p_1 = 0.5\)</span>, and a random <span class="math inline">\(p_0\)</span> from a Uniform distribution between 0.5 and 1. This setup indicates that one believes strongly that one third of the test takers belong to the random-guessing class, while the remaining two thirds of the test takers belong to the knowledgeable class. One is certain about the success rate of the guessing class, but the location of the correct rate of the knowledgeable class is unknown in the interval (0.5, 1).</p>

<p><strong>R Work</strong> The JAGS model script is shown below. One introduces a new variable <code>theta[i]</code> that indicates the correct rate value for observation <code>i</code>.
In the sampling section of the JAGS script, the first block is a loop over all observations, where one first determines the rate <code>theta[i]</code> based on the classification value <code>z[i]</code>. The <code>equals</code> command evaluates equality, for example, <code>equals(z[i], 0)</code> returns 1 if <code>z[i]</code> equals to <code>0</code>, and returns 0 otherwise. This indicates that the rate <code>theta[i]</code> will either be equal to <code>p1</code> or <code>p0</code> depending on the value <code>z[i]</code>.</p>
<p>One should note in JAGS, the classification variable <code>z[i]</code> takes values of 0 and 1, corresponding to the knowledgeable and guessing classes, respectively. As <span class="math inline">\(\pi\)</span> is considered fixed and set to 1/3, the variable <code>z[i]</code> is assigned a Bernoulli distribution with probability 1/3. To conclude the script, in the prior section the guessing rate parameter <code>p1</code> is assigned the value 0.5 and the rate parameter <code>p0</code> is assigned a Beta(1, 1) distribution truncated to the interval (0.5, 1) using <code>T(0.5, 1)</code>.</p>
<pre><code>modelString&lt;-&quot;
model {
## sampling
for (i in 1:N){
   theta[i] &lt;- equals(z[i], 1) * p1 + equals(z[i], 0) * p0
y[i] ~ dbin(theta[i], m)
}
for (i in 1:N){
   z[i] ~ dbern(1/3)
}
## priors
p1 &lt;- 0.5
p0 ~ dbeta(1,1) T(0.5, 1)
}&quot;</code></pre>
<p>One performs inference for <code>theta</code> and <code>p0</code> in JAGS by looking at their posterior summaries. Note that there are <span class="math inline">\(n = 30\)</span> test takers, each with an associated <code>theta</code> indicating the correct success rate of test taker <code>i</code>. The variable <code>p0</code> is the estimate of the correct rate of the knowledgeable class.</p>
<p>How are the correct rates estimated for different test takers by the latent class model? Before looking at the results, let’s revisit the dataset as shown in Figure 13.13. Among the test takers with lower scores, it is obvious that test taker # 6 with a score of 6 is likely to be assigned to the random-guessing class, whereas test takers # 4 and # 5 with a score of 13 are probably assigned to the knowledgeable class. Among test takers with higher scores, test takers # 15 and # 17 with respective scores of 20 and 19 are most likely to be assigned to the knowledgeable class, and test taker # 24 with a score of 14 is also likely assigned to the knowledgeable class.</p>
<p>Table 13.2. Posterior summaries of the correct rates <span class="math inline">\(\theta_i\)</span> of six selected test takers.</p>
<table>
<thead>
<tr class="header">
<th align="center">Test Taker</th>
<th align="center">Score</th>
<th align="center">Mean</th>
<th align="center">Median</th>
<th align="center">90% Credible Interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"># 4</td>
<td align="center">13</td>
<td align="center">0.553</td>
<td align="center">0.500</td>
<td align="center">(0.500, 0.876)</td>
</tr>
<tr class="even">
<td align="center"># 5</td>
<td align="center">13</td>
<td align="center">0.555</td>
<td align="center">0.500</td>
<td align="center">(0.500, 0.875)</td>
</tr>
<tr class="odd">
<td align="center"># 6</td>
<td align="center">6</td>
<td align="center">0.500</td>
<td align="center">0.500</td>
<td align="center">(0.500, 0.500)</td>
</tr>
<tr class="even">
<td align="center"># 15</td>
<td align="center">20</td>
<td align="center">0.879</td>
<td align="center">0.879</td>
<td align="center">(0.841, 0.917)</td>
</tr>
<tr class="odd">
<td align="center"># 17</td>
<td align="center">19</td>
<td align="center">0.878</td>
<td align="center">0.879</td>
<td align="center">(0.841, 0.917)</td>
</tr>
<tr class="even">
<td align="center"># 24</td>
<td align="center">14</td>
<td align="center">0.690</td>
<td align="center">0.831</td>
<td align="center">(0.500, 0.897)</td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:unnamed-chunk-15"></span>
<img src="../LATEX/figures/chapter13/LCM_post3.png" alt="MCMC diagnostic plots for correct rate of the knowledgeable class, $p_0$." width="500" />
<p class="caption">
Figure 13.14: MCMC diagnostic plots for correct rate of the knowledgeable class, <span class="math inline">\(p_0\)</span>.
</p>
</div>

<p>The latent class model assigns observations to one of the two latent classes at each MCMC iteration, and the posterior summaries of <code>theta</code> provide estimates of the correct rate of each test taker. Table 13.2 provides posterior summaries for six specific test takers.
The posterior summaries of the correct rate of test taker # 6 indicate that the model assigns this test taker to the random-guessing group and the posterior mean and median of the correct rate is at 0.5. Test takers # 4 and # 5 have similar posterior summaries and are classified as random-guessing most of the time with posterior mean of correct rate around 0.55. Test taker # 24 has a higher posterior mean than the test takers # 4 and # 5. But with a posterior mean 0.69, the posterior probability for the true rate for # 24 is somewhat split between random guessing and knowledgeable states.
Test takers # 15 and # 17 are always classified as knowledgeable with posterior mean and median of correct rate around 0.88.</p>
<p>One also summarizes the posterior draws of <span class="math inline">\(p_0\)</span> corresponding to the success rate for the knowledgeable students. Figure 13.14 provides MCMC diagnostics of <span class="math inline">\(p_0\)</span>. Its posterior mean, median, and 90% credible interval are 0.879, 0.879, and (0.841, 0.917). These estimates are very close to the correct rate of test takers # 15 and # 17. These test takers are always classified in the knowledgeable class and their correct rate estimates are the same as <span class="math inline">\(p_0\)</span>.</p>
<p><strong>Scenario 2: all parameters unknown</strong></p>
<p>It is straightforward to generalize this latent class model relaxing some of the fixed parameter assumptions in Scenario 1. It was originally assumed that the class assignment parameter <span class="math inline">\(\pi = 1/3\)</span>. It is more realistic to assume that the probability of assigning an individual into the first class <span class="math inline">\(\pi\)</span> is unknown and assign this parameter a Beta distribution with specific shape parameters. Here one assumes little is known about this classification parameter and so <span class="math inline">\(\pi\)</span> is assigned a <span class="math inline">\(\textrm{Beta}(1, 1)\)</span>, i.e. a Uniform distribution on (0, 1). In addition, previously it was assumed that it was known that the success rate for the “guessing” group <span class="math inline">\(p_1\)</span> was equal to 1/2. Here this assumption is relaxed by assigning the success rate <span class="math inline">\(p_1\)</span> a Uniform prior on the interval (0.4, 0.6). If one knows only that that the success rate for the “knowing” group is <span class="math inline">\(p_0\)</span> is larger than <span class="math inline">\(p_1\)</span>, then one assumes <span class="math inline">\(p_0\)</span> is Uniform in the interval (<span class="math inline">\(p_1, 1\)</span>).</p>

<p><strong>R Work</strong> The JAGS script for this more general model follows. We introduce the parameter <code>q</code> as <span class="math inline">\(\pi\)</span>, that is the class assignment parameter and assign it a Beta distribution with parameters 1 and 1. The prior distributions for <code>p1</code> and <code>p0</code> are modified to reflect the new assumptions.</p>
<pre><code>modelString&lt;-&quot;
model {
## sampling
for (i in 1:N){
   theta[i] &lt;- equals(z[i], 1) * p1 + equals(z[i], 0) * p0
   y[i] ~ dbin(theta[i], m)
}
for (i in 1:N){
   z[i] ~ dbern(q)
}
## priors
p1 ~ dbeta(1, 1) T(0.4, 0.6)
p0 ~ dbeta(1,1) T(p1, 1)
q ~ dbeta(1, 1)
}
&quot;</code></pre>
<p>In Scenario 1, the posterior distributions of the correct rates <code>theta[i]</code> were summarized for all individuals. Here we instead focus on the classification parameters <code>z[i]</code> where <code>z[i] = 1</code> indicates a person classified into the random-guessing group. Figure 13.15 displays the posterior means of the <span class="math inline">\(z_i\)</span> for all individuals. As expected, individuals #1 through # 10 are classified as guessers and most individuals with labels 12 and higher are classified as knowledgeable. Individuals # 11 and # 24 have posterior classification means between 0.25 and 0.75 indicating some uncertainty about the correct classification for these people.</p>

<div class="figure"><span id="fig:unnamed-chunk-16"></span>
<img src="../LATEX/figures/chapter13/ClassificationPlot.png" alt="Posterior means of classification parameters for all test takers" width="500" />
<p class="caption">
Figure 13.15: Posterior means of classification parameters for all test takers
</p>
</div>
<p>Figure 13.16 displays density estimates of the simulated draws from the posterior distributions of the class assignment parameter <span class="math inline">\(\pi\)</span> and the rate parameters <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_0\)</span>. As one might expect, the posterior distributions of <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_0\)</span> are centered about values of 0.54 and 0.89. There is some uncertainty about the class assignment parameter as reflected in a wide density estimate for <span class="math inline">\(\pi\)</span> (<span class="math inline">\(q\)</span> in the figure).</p>

<div class="figure"><span id="fig:unnamed-chunk-17"></span>
<img src="../LATEX/figures/chapter13/threevarplot.png" alt="Posterior density plots of class assignment and rate parameters." width="500" />
<p class="caption">
Figure 13.16: Posterior density plots of class assignment and rate parameters.
</p>
</div>
<p><strong>Disputed authorship of the Federalist Papers</strong>
</p>
<p>Returning to the Federalist Papers example of Section 13.2, the discussion focused on learning about the true rates of filler words for papers written by Alexander Hamilton and James Madison. But actually the true authorship of some of the papers was in doubt, and the primary task in Mosteller and Wallace (1963) was to learn about the true authorship of these disputed authorship papers from the data. This problem of disputed authorship can be considered a special case of latent data modeling where the latent variable is the authorship of a disputed paper. We describe how the Bayesian model of Section 13.3 can be generalized to learn about both the rates of a particular filler word and the identity of the papers of disputed authorship.</p>
<p>In our sample there are a total of 74 Federalist Papers. We assume that 49 of these papers are known to be written by Hamilton, 15 of the papers are known to be written by Madison, and the authorship of the remaining 10 papers is disputed between the two authors. We focus on the use of the filler word “can” in these papers. Let {<span class="math inline">\((y_{1i}, n_{1i})\)</span>} denote the frequencies of “can” and total words in the Hamilton papers, {<span class="math inline">\((y_{2i}, n_{2i}\)</span>)} denote the frequencies and total words in the Madison papers, and {<span class="math inline">\((y_{i}, n_{i}\)</span>)} denote the corresponding quantities in the disputed papers. As in Section 13.2, we assume {<span class="math inline">\(y_{1i}\)</span>} are Negative Binomial(<span class="math inline">\(p_{1i}, \alpha_1\)</span>) where <span class="math inline">\(p_{1i} = \beta_1 / (\beta_1 + n_{1i}/1000)\)</span>, and {<span class="math inline">\(y_{2i}\)</span>} are Negative Binomial(<span class="math inline">\(p_{2i}, \alpha_2\)</span>) where <span class="math inline">\(p_{2i} = \beta_2 / (\beta_2 + n_{2i}/1000)\)</span>.</p>
<p>The distribution of the frequencies {<span class="math inline">\(y_{i}\)</span>} is unknown (out of the total number of words {<span class="math inline">\(n_{i}\)</span>}) since these correspond to the papers of disputed authorship. Let <span class="math inline">\(z_i\)</span> denote the unknown authorship of paper <span class="math inline">\(i\)</span> among the disputed papers – if <span class="math inline">\(z_i = 0\)</span>, the paper was written by Hamilton and if <span class="math inline">\(z_i = 1\)</span>, the paper was written by Madison. If one knows the value of <span class="math inline">\(z_i\)</span>, the distribution of the frequency <span class="math inline">\(y_i\)</span> is known. If <span class="math inline">\(z_i = 0\)</span>, then <span class="math inline">\(y_i\)</span> is Negative Binomial(<span class="math inline">\(p_{i}, \alpha_1\)</span>) where <span class="math inline">\(p_{i} = \beta_1 / (\beta_1 + n_{i}/1000)\)</span>, and <span class="math inline">\(z_i = 1\)</span>, then <span class="math inline">\(y_i\)</span> is Negative Binomial(<span class="math inline">\(p_{i}, \alpha_2\)</span>) where <span class="math inline">\(p_{i} = \beta_2 / (\beta_2 + n_{i}/1000)\)</span>. To complete the model, one needs to assign a prior distribution to the latent authorship indicators {<span class="math inline">\(z_i\)</span>}. It is assumed <span class="math inline">\(z_i \sim \textrm{Bernoulli}(0.5)\)</span> which means that <span class="math inline">\(z_i\)</span> from the prior is equally likely to be 0 or 1.</p>
<p>The JAGS script for the disputed authorship problem is shown below. The data is structured so that <code>N1</code> papers are known to be written by Hamilton, <code>N2</code> papers are known to be written by Madison, and the authorship of the remaining <code>N3</code> papers are in doubt. The data includes the number of occurrences of the word “can” and the total number or words in each group of papers. Note that, as in Section 13.3, weakly informative priors are placed on the Gamma priors for <span class="math inline">\(\alpha_1, \beta_1, \alpha_2\)</span> and <span class="math inline">\(\beta_2\)</span>.</p>
<pre><code>modelString = &quot;
model{
for(i in 1:N1){
   p1[i] &lt;- beta1 / (beta1 + n1[i] / 1000)
   y1[i] ~ dnegbin(p1[i], alpha1)
}
for(i in 1:N2){
   p2[i] &lt;- beta2 / (beta2 + n2[i] / 1000)
   y2[i] ~ dnegbin(p2[i], alpha2)
}
for(i in 1:N3){
   theta[i] &lt;- equals(z[i], 0) * alpha1 +
            equals(z[i], 1) * alpha2
   gamma[i] &lt;- equals(z[i], 0) * beta1 +
            equals(z[i], 1) * beta2
   p[i] &lt;- gamma[i] / (gamma[i] + n[i] / 1000)
   y[i] ~ dnegbin(p[i], theta[i])
   z[i] ~ dbern(0.5)
}
alpha1 ~ dgamma(.001, .001)
beta1 ~ dgamma(.001, .001)
alpha2 ~ dgamma(.001, .001)
beta2 ~ dgamma(.001, .001)
}</code></pre>
<p>Using this script, a sample of 5000 draws were taken from the posterior distribution and Figure 13.17 displays posterior means of the classification parameters <span class="math inline">\(z_1, ..., z_{10}\)</span> for the ten disputed authorship parameters. Since <span class="math inline">\(z_i = 1\)</span> if the author is Madison, this graph is showing the posterior probability the author is James Madison for each paper. Note that most of these posterior means are located near 0.5, with the one exception of Paper 4 where the posterior probability of Madison authorship is 0.174. So really one has not learned much about the identity of the true author from this data.</p>

<div class="figure"><span id="fig:unnamed-chunk-18"></span>
<img src="../LATEX/figures/chapter13/fed_authorship.png" alt="Posterior means of  classification parameters for authorship problem using rates of the filler word can." width="500" />
<p class="caption">
Figure 13.17: Posterior means of classification parameters for authorship problem using rates of the filler word can.
</p>
</div>
<p>But we have only looked at the frequencies of one particular filler word in our analysis. In a typical study such as the one done by Mosteller and Wallace (1963), a number of filler words are used. One can extend the analysis to include a number of filler words; the approach is outlined below and the implementation details are left to the end-of-chapter exercises.</p>
<p>Suppose <span class="math inline">\(y_{1i}^w\)</span> denotes the number of occurrences of the word <span class="math inline">\(w\)</span> in the <span class="math inline">\(i\)</span>-th paper written by Hamilton. Similarly, <span class="math inline">\(y_{2i}^w\)</span> denotes the word count of <span class="math inline">\(w\)</span> in the <span class="math inline">\(i\)</span>-th paper written by Madison and <span class="math inline">\(y_i^w\)</span> denotes the word count of <span class="math inline">\(w\)</span> in the <span class="math inline">\(i\)</span>-th paper of disputed authorship. It is assumed that each word count follows a Negative Binomial distribution where the parameters of the distribution depend on the author and the word. So, for example, for a Hamilton paper, <span class="math inline">\(y_{1i}^w\)</span> is distributed Negative Binomial(<span class="math inline">\(p_1^w, \alpha_1^w\)</span>) where <span class="math inline">\(p_1^w = \beta_1^w / (\beta_1^w + n_{1i}/1000)\)</span>. For a Madison paper, <span class="math inline">\(y_{2i}^w\)</span> is distributed Negative Binomial(<span class="math inline">\(p_2^w, \alpha_2^w\)</span>) where <span class="math inline">\(p_2^w = \beta_2^w / (\beta_2^w + n_{2i}/1000)\)</span>. For a paper of disputed authorship, the count <span class="math inline">\(y_i^w\)</span> will either be distributed according to one of the Negative Binomial distributions where the distribution depends on the value of the classification variable <span class="math inline">\(z_i\)</span>.</p>
<p>A JAGS script can be written to fit this model with multiple filler words. In the script, one defines the matrix variable <code>y1</code> where <code>y1[i, j]</code> is defined to be the number of words of type <span class="math inline">\(j\)</span> in the <span class="math inline">\(i\)</span>-th paper of Hamilton. In a similar fashion one defines the matrices <code>y2</code> and <code>y</code> where <code>y2[i, j]</code> and <code>y[i, j]</code> denote respectively the counts of the <span class="math inline">\(j\)</span>-th word of the <span class="math inline">\(i\)</span>-th Madison and <span class="math inline">\(i\)</span>-th disputed authorship paper. One will be learning about vectors <span class="math inline">\(\alpha_1, \beta_1, \alpha_2, \beta_2\)</span> where each vector has <span class="math inline">\(W\)</span> values where <span class="math inline">\(W\)</span> is the number of words in the study. As before <code>z[i]</code> denotes the classification variable where <code>z[i] = 1</code> denotes authorship of the <span class="math inline">\(i\)</span>-th disputed paper by Madison. In an end-of-chapter exercise, the reader will be asked to implement the model fitting using a selection of filler words. One would anticipate that one would be able to discriminate between the two authors on the basis of a large group of filler words.</p>
</div>
<div id="exercises-11" class="section level2">
<h2><span class="header-section-number">13.8</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><strong>Federalist Papers Word Study</strong>
</li>
</ol>
<p>The frequencies of word use of Madison and Hamilton are stored in the data file <code>fed_word_data.csv</code>. Consider the counts of the word “there” in the 50 Federalist Papers authored by Hamilton.
Suppose the count <span class="math inline">\(y_i\)</span> in the <span class="math inline">\(i\)</span>-th paper is distributed Poisson with mean <span class="math inline">\(n_i \lambda / 1000\)</span> where <span class="math inline">\(n_i\)</span> is the number of words in the paper and <span class="math inline">\(\lambda\)</span> is the rate of the word “there” per 1000 words.</p>
<ol style="list-style-type: lower-alpha">
<li>Assuming a weakly informative prior for <span class="math inline">\(\lambda\)</span>, use JAGS to fit this Poisson sampling model.<br />
</li>
<li>Compute a 90% probability interval for the rate <span class="math inline">\(\lambda\)</span>.</li>
<li>Consider a new essay with 1000 words. By simulating 1000 draws from the posterior predictive distribution, construct a 90% prediction interval for the number of occurrences of the word “there” in this essay.</li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li><strong>Federalist Papers Word Study (continued)</strong></li>
</ol>
<p>Instead of Poisson sampling, suppose the count of the word <span class="math inline">\(y_i\)</span> “there” in the <span class="math inline">\(i\)</span>-th Federalist paper is distributed Negative Binomial with parameters <span class="math inline">\(p_i\)</span> and <span class="math inline">\(\alpha\)</span>, where <span class="math inline">\(p_i = \beta / (\beta + n_i/1000)\)</span> where <span class="math inline">\(n_i\)</span> is the number of words in the paper and <span class="math inline">\(\alpha / \beta\)</span> is the rate of the word “there” per 1000 words.</p>
<ol style="list-style-type: lower-alpha">
<li>Using a suitable weakly informative prior for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, use JAGS to simulate 1000 draws from the posterior distribution.</li>
<li>Construct a 90% interval estimate for the rate parameter <span class="math inline">\(\alpha/\beta\)</span>.</li>
<li>By simulating from the posterior predictive distribution, construct a 90% prediction interval for the number of there’s in a new essay of 1000 words.</li>
<li>Compare your answers with the answers in Exercise 1 assuming Poisson sampling.</li>
</ol>
<ol start="3" style="list-style-type: decimal">
<li><strong>Comparing Word Use</strong>
</li>
</ol>
<p>Using Negative Binomial sampling models, compare the average word use of Hamilton and Madison for the words “this”, “on”, “his”, and “by”. Suppose the mean rate per 1000 words is measured by <span class="math inline">\(\alpha_1 / \beta_1\)</span> and <span class="math inline">\(\alpha_2/\beta_2\)</span> for Hamilton and Madison, respectively. For each word, construct a 90% interval estimate for the difference in use rates <span class="math inline">\(D = \alpha_1 / \beta_1 - \alpha_2/\beta_2\)</span>. By looking at the locations of these interval estimates, which words were more often used by Hamilton and which ones were more likely to be used by Madison? The data file is <code>fed_word_data.csv</code>.</p>
<ol start="4" style="list-style-type: decimal">
<li><strong>Comparing Word Use (continued)</strong></li>
</ol>
<p>As in Exercise 3, using Negative Binomial sampling models, compare the average word use of Hamilton and Madison for the words “this”, “on”, “his”, and “by”. If the mean rate per 1000 words is measured by <span class="math inline">\(\alpha_1 / \beta_1\)</span> and <span class="math inline">\(\alpha_2/\beta_2\)</span> for Hamilton and Madison, respectively,
suppose one is interested in comparing the rates using the ratio
<span class="math display">\[
R = \frac{\alpha_1 / \beta_1}{\alpha_2/\beta_2}.
\]</span>
Construct and graph 90% interval estimates for <span class="math inline">\(R\)</span> for each word in the study.</p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Basketball Shooting Data</strong></li>
</ol>
<p>Table 13.3 displays the number of free throw attempts FTA and the number of successful free throws FT for all the seasons of Isiah Thomas, a great basketball point guard who played in the National Basketball Association from 1982 to 1994. This data is contained in the file <code>nba_guards.csv</code> where the <code>Player</code> variable is equal to “THOMAS.” Let <span class="math inline">\(p_j\)</span> denote the probability of a successful free throw for the <span class="math inline">\(j\)</span>-th season. Consider the quadratic logistic model
<span class="math display">\[\begin{equation*}
\log \left(\frac{p_j}{1-p_j}\right) = \beta_0 + \beta_1 (x_j - 30) + \beta_2 (x_j - 30)^2,
\end{equation*}\]</span>
where <span class="math inline">\(x_j\)</span> is Thomas’ age during the <span class="math inline">\(j\)</span>-th season.</p>
<p>Table 13.3. Free throw shooting data for the basketball player Isiah Thomas.</p>
<table>
<thead>
<tr class="header">
<th align="right">Age</th>
<th align="right">FTA</th>
<th align="right">FT</th>
<th align="right">Age</th>
<th align="right">FTA</th>
<th align="right">FT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">20</td>
<td align="right">429</td>
<td align="right">302</td>
<td align="right">27</td>
<td align="right">351</td>
<td align="right">287</td>
</tr>
<tr class="even">
<td align="right">21</td>
<td align="right">518</td>
<td align="right">368</td>
<td align="right">28</td>
<td align="right">377</td>
<td align="right">292</td>
</tr>
<tr class="odd">
<td align="right">22</td>
<td align="right">529</td>
<td align="right">388</td>
<td align="right">29</td>
<td align="right">229</td>
<td align="right">179</td>
</tr>
<tr class="even">
<td align="right">23</td>
<td align="right">493</td>
<td align="right">399</td>
<td align="right">30</td>
<td align="right">378</td>
<td align="right">292</td>
</tr>
<tr class="odd">
<td align="right">24</td>
<td align="right">462</td>
<td align="right">365</td>
<td align="right">31</td>
<td align="right">377</td>
<td align="right">278</td>
</tr>
<tr class="even">
<td align="right">25</td>
<td align="right">521</td>
<td align="right">400</td>
<td align="right">32</td>
<td align="right">258</td>
<td align="right">181</td>
</tr>
<tr class="odd">
<td align="right">26</td>
<td align="right">394</td>
<td align="right">305</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<ol style="list-style-type: lower-alpha">
<li>By using JAGS with a reasonable choice of weakly informative prior on the regression parameters, collect 5000 draws from the posterior distribution on <span class="math inline">\(\beta = (\beta_0, \beta_1, \beta_2)\)</span>.</li>
<li>Construct a density estimate and a 90% interval estimate for the age <span class="math inline">\(h_1(\beta)\)</span> where Thomas attained peak performance.</li>
<li>Construct a density estimate and a 90% interval estimate for the probability <span class="math inline">\(p\)</span> that Thomas makes a free throw at age 28.</li>
</ol>
<ol start="6" style="list-style-type: decimal">
<li><strong>Basketball Shooting Data (continued)}</strong></li>
</ol>
<p>The dataset <code>nba_guards.csv</code> contains the number of free throw attempts FTA and the number of successful free throws FT for all of the seasons played by fifteen great point guards in the National Basketball Association. Let <span class="math inline">\(p_{ij}\)</span> denote the probability of a successful free throw of the <span class="math inline">\(i\)</span>-th player during the <span class="math inline">\(j\)</span>-th season. Suppose the probabilities {<span class="math inline">\(p_{ij}\)</span>} for the <span class="math inline">\(i\)</span>-th player satisfy the quadratic model
<span class="math display">\[\begin{equation*}
\log \left(\frac{p_{ij}}{1-p_{ij}}\right) = \beta_{i0} + \beta_{i1} (x_{ij} - 30) + \beta_{i2} (x_{ij} - 30)^2,
\end{equation*}\]</span>
where <span class="math inline">\(x_{ij}\)</span> is the age of the <span class="math inline">\(i\)</span>-th player during the <span class="math inline">\(j\)</span>-th season and <span class="math inline">\(\beta_i = (\beta_{i0}, \beta_{i1}, \beta_{i2})\)</span> denotes the vector of regression coefficients for the <span class="math inline">\(i\)</span>-th player.</p>
<ol style="list-style-type: lower-alpha">
<li>Construct a hierarchical prior for the regression vectors <span class="math inline">\(\beta_1, ..., \beta_{15}\)</span> analogous to the one used for baseball hitters in the chapter.</li>
<li>Use JAGS to simulate a sample of 5000 draws from the posterior distribution of the <span class="math inline">\(\beta_j\)</span> and also of the second stage prior <span class="math inline">\(\mu_\beta\)</span>.</li>
<li>For one player, consider the age <span class="math inline">\(h_1(\beta)\)</span> where he attained peak performance in free-throw shooting. Compare the posterior distributions of <span class="math inline">\(h_1(\beta)\)</span> using an individual logistic model and using the hierarchical model.</li>
</ol>
<ol start="7" style="list-style-type: decimal">
<li><strong>Football Field Goal Kicking</strong>
</li>
</ol>
<p>The data file <code>football_field_goal.csv</code> contains data on field goal attempts for professional football kickers. Let <span class="math inline">\(y_j\)</span> denote the response (success or failure) of a field goal attempt from <span class="math inline">\(x_j\)</span> yards. One is interested in fitting the logistic model
<span class="math display">\[
\log\left(\frac{p_j}{1-p_j}\right) = \beta_0 + \beta_1 x_j,
\]</span>
where <span class="math inline">\(p_j\)</span> is the probability of a successful attempt. Figure 13.18 displays individual logistic fits for ten kickers in the 2005 season. These fits were found using weakly informative priors on the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> on individual fits for each player.</p>

<div class="figure"><span id="fig:unnamed-chunk-19"></span>
<img src="../LATEX/figures/chapter13/manykickers.png" alt="Individual logistic model fits for ten professional football kickers in the 2005 season." width="500" />
<p class="caption">
Figure 13.18: Individual logistic model fits for ten professional football kickers in the 2005 season.
</p>
</div>
<ol style="list-style-type: lower-alpha">
<li><p>Looking at Figure 13.18, do you believe these individual fits of success probability against distance are suitable for all players? Explain.</p></li>
<li><p>For the player Brown, assuming this logistic model and a weakly informative prior on the regression coefficients, use JAGS to simulate from the posterior distribution. From the output to construct a 90 percent interval estimate for the probability of success at 30 yards.</p></li>
<li><p>Pool the data for all 10 players, and use JAGS to fit from the logistic model where the probability of success is a function of the distance. Use JAGS to simulate from the posterior and from the output
construct a 90 percent interval estimate for the probability of success at 30 yards.</p></li>
<li><p>Compare your answers to parts (b) and (c).</p></li>
</ol>
<ol start="8" style="list-style-type: decimal">
<li><strong>Football Field Goal Kicking (continued)</strong></li>
</ol>
<p>In the logistic model predicting success on a field goal attempt based on the distance in feet, suppose <span class="math inline">\(\beta_i = (\beta_{i0}, \beta_{i1})\)</span> denotes the regression vector for the logistic model on the <span class="math inline">\(i\)</span>-th player.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Write down a hierarchical prior for the ten regression vectors <span class="math inline">\(\beta_1, ..., \beta_{10}\)</span> similar to what was used for the baseball hitters in the chapter.</p></li>
<li><p>Using JAGS, simulate a sample of 5000 from the posterior distribution of <span class="math inline">\(\beta_1, ..., \beta_{10}\)</span>.</p></li>
<li><p>Display the posterior means of the probabilities of success for all kickers as a function of distance similar to Figure 13.18.</p></li>
<li><p>For the player Brown, construct a 90 percent interval estimate for the probability of success at 30 yards. Compare your answer to the individual fit (part (b) of Exercise 7 and the pooled data fit (part (c) of Exercise 7).</p></li>
</ol>
<ol start="9" style="list-style-type: decimal">
<li><strong>Checking for Overdispersion</strong></li>
</ol>
<p>In the hitter’s career trajectory example in Section 13.3.4, it was assumed that the number of on-base events in season <span class="math inline">\(y_j\)</span> was distributed Binomial with a specific probability of success <span class="math inline">\(p_j\)</span> where the {<span class="math inline">\(p_j\}\)</span> satisfy a logistic quadratic model. If one views a scatterplot of the observed rates <span class="math inline">\(OB / PA\)</span> against age for Chase Utley (Figure 13.7), one notices some variation about the fitted curve. It is natural to ask if the variability about this line is greater than one would predict from the Binomial model.</p>
<ol style="list-style-type: lower-alpha">
<li>Following the example in Section 13.3.4, obtain a posterior sample from the posterior distribution of <span class="math inline">\(\beta\)</span> using Utley’s data and a weakly informative choice of prior distribution.</li>
<li>Compute the posterior mean of <span class="math inline">\(\beta\)</span> and obtain an estimate for the on-base probability for all of Utley’s ages.</li>
<li>Write a function to simulate one replicated sample of <span class="math inline">\(y_j^{R}\)</span> from the posterior predictive distribution. Compute the sum of squares of the rates <span class="math inline">\(y_j^{R} / n_j\)</span> about the fitted on-base probabilities.</li>
<li>Using the function written in part (c), repeat this simulation 1000 times, obtaining 1000 sum of squares values from the posterior predictive distribution.</li>
<li>By comparing the posterior predictive sample in part (d) with the observed sum of squares about the fitted curve, comment about the suitability of the Binomial sampling model.</li>
</ol>
<ol start="10" style="list-style-type: decimal">
<li><strong>Moby Dick Word Study</strong></li>
</ol>
<p>Project Gutenberg offers a large number of free electronic books and the <code>gutenbergr</code> R package facilitates reading in these books into R for textual analysis. Use this package to download the famous novel <strong>Moby Dick</strong> by Herman Melville and collect all of the distinct words in this novel in a data frame.</p>
<ol style="list-style-type: lower-alpha">
<li>Divide the words of the novel into 1000-word groups and count the number of occurrences of a particular filler word in each group.</li>
<li>Use a Negative Binomial distribution to model the counts of the filler word across groups.</li>
<li>Consider the use of a different filler word, and use a Negative Binomial distribution to model the counts of this new word across 1000-word groups.<br />
</li>
<li>By use of a suitable model, compare the rates (per 1000 words) of the two types of filler words. Construct a 90% interval estimate for the ratio of rates of use of the two words.</li>
</ol>
<ol start="11" style="list-style-type: decimal">
<li><strong>An Outlier Model</strong></li>
</ol>
<p>Suppose one observes a sample measurement data where there is a small possibility of an outlier. One observes <span class="math inline">\(y_i\)</span> which is either Normal(<span class="math inline">\(\mu, \sigma\)</span>) with probability <span class="math inline">\(p\)</span> or Normal(<span class="math inline">\(\mu, 3 \sigma\)</span>) with probability <span class="math inline">\(1 - p\)</span>. Assume a weakly informative prior on <span class="math inline">\(\mu\)</span> and <span class="math inline">\(p\)</span> is Beta(2, 20). Introduce latent class assignments where <span class="math inline">\(z_i = 0\)</span> or <span class="math inline">\(z_i = 1\)</span> depending if the observation is Normal(<span class="math inline">\(\mu, \sigma\)</span>) or Normal(<span class="math inline">\(\mu, 3\sigma\)</span>). Use JAGS with a script similar to used to Section 13.4 to fit this model. The dataset <code>darwin.csv</code> in the <code>ProbBayes</code> R package contains measurements on the differences of the heights of cross and self fertilized plants quoted by Fisher (1960). Compute the posterior probability that each observation is an outlier. Plot the measurements against these posterior outlier probabilities and comment on the pattern in the scatterplot.</p>
<ol start="12" style="list-style-type: decimal">
<li><strong>Another Latent Data Model</strong></li>
</ol>
<p>Suppose <span class="math inline">\(n\)</span> students are taking a multiple-choice exam of <span class="math inline">\(m\)</span> questions. As in Section 13.4, suppose there are two types of students, the “guessing” students and the “knowledgeable” students who answer each question correct with respective probabilities of <span class="math inline">\(p_0\)</span> and <span class="math inline">\(p_1\)</span>. The following R code will simulate some data in this scenario where <span class="math inline">\(n = 50\)</span>, <span class="math inline">\(m = 20\)</span>, <span class="math inline">\(p_0 = 0.4\)</span>, <span class="math inline">\(p_1 = 0.7\)</span> and the probability that a student is a guesser is <span class="math inline">\(\pi = 0.2\)</span> (<code>p</code> in the JAGS script).</p>
<pre><code>set.seed(123)
p0 &lt;- 0.40; p1 &lt;- 0.70
m &lt;- 20; n &lt;- 50; p &lt;- 0.2
z &lt;- sample(0:1, size = n, prob = c(p, 1-p),
            replace = TRUE)
prob &lt;- (z == 0) * p0 + (z == 1) * p1
y &lt;- rbinom(n, size = m, prob)</code></pre>
<p>By use of a latent class model similar to what was used in Section 13.4, simulate from the joint posterior distribution of all parameters. Estimate values of <span class="math inline">\(p_0\)</span>, <span class="math inline">\(p_1\)</span>, and <span class="math inline">\(\pi\)</span> from the posterior and compare these estimates with the “true” values of the parameters used in the simulation.</p>
<ol start="13" style="list-style-type: decimal">
<li><strong>Determining Authorship From a Single Word</strong></li>
</ol>
<p>In the dataset <code>federalist_word_study.csv}, the variable</code>Authorship} indicates the authorship of the Federalist Papers and the variable ```Disputed} indicates the papers where the authorship is disputed.<br />
Following the work in Section 13.4.3 and the JAGS script, fit a latent variable model using the filler word “from”. Using this particular word, examine the posterior probabilities of authorship for the ten papers of disputed authorship. Is this single word helpful for determining authorship for any of the papers? Repeat this analysis using the filler word “this”.</p>
<ol start="14" style="list-style-type: decimal">
<li><strong>Determining Authorship From Multiple Words</strong></li>
</ol>
<p>Suppose one wishes to use all of the filler words “by”, “from”, “to”, “an”, “any”, “may”,
“his”, “upon”, “also”, “can”,
“of”, “on”, “there”, and“this” to determine authorship of the ten disputed papers. Using the approach described at the end of Section 13.4, write a JAGS script to fit the latent variable model. Collect a posterior sample from the posterior distribution of the classification variables. Use the posterior means of the classification variables to determine authorship for each of the ten variables.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-multiple-regression-and-logistic-models.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/13-casestudies.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
