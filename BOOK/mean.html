<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Modeling Measurement and Count Data | Probability and Bayesian Modeling</title>
  <meta name="description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Modeling Measurement and Count Data | Probability and Bayesian Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Modeling Measurement and Count Data | Probability and Bayesian Modeling" />
  
  <meta name="twitter:description" content="This is an introduction to probability and Bayesian modeling at the undergraduate level. It assumes the student has some background with calculus." />
  

<meta name="author" content="Jim Albert and Jingchen Hu" />


<meta name="date" content="2020-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="proportion.html"/>
<link rel="next" href="simulation-by-markov-chain-monte-carlo.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Probability and Bayesian Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html"><i class="fa fa-check"></i><b>1</b> Probability: A Measurement of Uncertainty</a><ul>
<li class="chapter" data-level="1.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-classical-view-of-a-probability"><i class="fa fa-check"></i><b>1.2</b> The Classical View of a Probability</a></li>
<li class="chapter" data-level="1.3" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-frequency-view-of-a-probability"><i class="fa fa-check"></i><b>1.3</b> The Frequency View of a Probability</a></li>
<li class="chapter" data-level="1.4" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-subjective-view-of-a-probability"><i class="fa fa-check"></i><b>1.4</b> The Subjective View of a Probability</a><ul>
<li class="chapter" data-level="1.4.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#measuring-probabilities-subjectively"><i class="fa fa-check"></i><b>1.4.1</b> Measuring probabilities subjectively</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-sample-space"><i class="fa fa-check"></i><b>1.5</b> The Sample Space</a><ul>
<li class="chapter" data-level="1.5.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#roll-two-fair-indistinguishable-dice"><i class="fa fa-check"></i><b>1.5.1</b> Roll two fair, indistinguishable dice</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#assigning-probabilities"><i class="fa fa-check"></i><b>1.6</b> Assigning Probabilities</a><ul>
<li class="chapter" data-level="1.6.1" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#events-and-event-operations"><i class="fa fa-check"></i><b>1.6.1</b> Events and Event Operations</a></li>
<li class="chapter" data-level="1.6.2" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-three-probability-axioms"><i class="fa fa-check"></i><b>1.6.2</b> The Three Probability Axioms</a></li>
<li class="chapter" data-level="1.6.3" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#the-complement-and-addition-properties"><i class="fa fa-check"></i><b>1.6.3</b> The Complement and Addition Properties</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="probability-a-measurement-of-uncertainty.html"><a href="probability-a-measurement-of-uncertainty.html#exercises"><i class="fa fa-check"></i><b>1.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="counting-methods.html"><a href="counting-methods.html"><i class="fa fa-check"></i><b>2</b> Counting Methods</a><ul>
<li class="chapter" data-level="2.1" data-path="counting-methods.html"><a href="counting-methods.html#introduction-rolling-dice-yahtzee-and-roulette"><i class="fa fa-check"></i><b>2.1</b> Introduction: Rolling Dice, Yahtzee, and Roulette</a></li>
<li class="chapter" data-level="2.2" data-path="counting-methods.html"><a href="counting-methods.html#equally-likely-outcomes"><i class="fa fa-check"></i><b>2.2</b> Equally Likely Outcomes</a></li>
<li class="chapter" data-level="2.3" data-path="counting-methods.html"><a href="counting-methods.html#the-multiplication-counting-rule"><i class="fa fa-check"></i><b>2.3</b> The Multiplication Counting Rule</a></li>
<li class="chapter" data-level="2.4" data-path="counting-methods.html"><a href="counting-methods.html#permutations"><i class="fa fa-check"></i><b>2.4</b> Permutations</a></li>
<li class="chapter" data-level="2.5" data-path="counting-methods.html"><a href="counting-methods.html#combinations"><i class="fa fa-check"></i><b>2.5</b> Combinations</a><ul>
<li class="chapter" data-level="2.5.1" data-path="counting-methods.html"><a href="counting-methods.html#number-of-subsets"><i class="fa fa-check"></i><b>2.5.1</b> Number of subsets</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="counting-methods.html"><a href="counting-methods.html#arrangements-of-non-distinct-objects"><i class="fa fa-check"></i><b>2.6</b> Arrangements of Non-Distinct Objects</a></li>
<li class="chapter" data-level="2.7" data-path="counting-methods.html"><a href="counting-methods.html#playing-yahtzee"><i class="fa fa-check"></i><b>2.7</b> Playing Yahtzee</a></li>
<li class="chapter" data-level="2.8" data-path="counting-methods.html"><a href="counting-methods.html#exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>3</b> Conditional Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="conditional-probability.html"><a href="conditional-probability.html#introduction-the-three-card-problem"><i class="fa fa-check"></i><b>3.1</b> Introduction: The Three Card Problem</a></li>
<li class="chapter" data-level="3.2" data-path="conditional-probability.html"><a href="conditional-probability.html#independent-events"><i class="fa fa-check"></i><b>3.2</b> Independent Events</a></li>
<li class="chapter" data-level="3.3" data-path="conditional-probability.html"><a href="conditional-probability.html#in-everyday-life"><i class="fa fa-check"></i><b>3.3</b> In Everyday Life</a></li>
<li class="chapter" data-level="3.4" data-path="conditional-probability.html"><a href="conditional-probability.html#in-a-two-way-table"><i class="fa fa-check"></i><b>3.4</b> In a Two-Way Table</a></li>
<li class="chapter" data-level="3.5" data-path="conditional-probability.html"><a href="conditional-probability.html#definition-and-the-multiplication-rule"><i class="fa fa-check"></i><b>3.5</b> Definition and the Multiplication Rule</a></li>
<li class="chapter" data-level="3.6" data-path="conditional-probability.html"><a href="conditional-probability.html#the-multiplication-rule"><i class="fa fa-check"></i><b>3.6</b> The Multiplication Rule</a></li>
<li class="chapter" data-level="3.7" data-path="conditional-probability.html"><a href="conditional-probability.html#the-multiplication-rule-under-independence"><i class="fa fa-check"></i><b>3.7</b> The Multiplication Rule Under Independence</a></li>
<li class="chapter" data-level="3.8" data-path="conditional-probability.html"><a href="conditional-probability.html#learning-using-bayes-rule"><i class="fa fa-check"></i><b>3.8</b> Learning Using Bayes’ Rule</a></li>
<li class="chapter" data-level="3.9" data-path="conditional-probability.html"><a href="conditional-probability.html#r-example-learning-about-a-spinner"><i class="fa fa-check"></i><b>3.9</b> R Example: Learning About a Spinner</a></li>
<li class="chapter" data-level="3.10" data-path="conditional-probability.html"><a href="conditional-probability.html#exercises"><i class="fa fa-check"></i><b>3.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discrete-distributions.html"><a href="discrete-distributions.html"><i class="fa fa-check"></i><b>4</b> Discrete Distributions</a><ul>
<li class="chapter" data-level="4.1" data-path="discrete-distributions.html"><a href="discrete-distributions.html#introduction-the-hat-check-problem"><i class="fa fa-check"></i><b>4.1</b> Introduction: The Hat Check Problem</a></li>
<li class="chapter" data-level="4.2" data-path="discrete-distributions.html"><a href="discrete-distributions.html#random-variable-and-probability-distribution"><i class="fa fa-check"></i><b>4.2</b> Random Variable and Probability Distribution</a></li>
<li class="chapter" data-level="4.3" data-path="discrete-distributions.html"><a href="discrete-distributions.html#probability-distribution"><i class="fa fa-check"></i><b>4.3</b> Probability distribution</a></li>
<li class="chapter" data-level="4.4" data-path="discrete-distributions.html"><a href="discrete-distributions.html#summarizing-a-probability-distribution"><i class="fa fa-check"></i><b>4.4</b> Summarizing a Probability Distribution</a></li>
<li class="chapter" data-level="4.5" data-path="discrete-distributions.html"><a href="discrete-distributions.html#standard-deviation-of-a-probability-distribution"><i class="fa fa-check"></i><b>4.5</b> Standard Deviation of a Probability Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="discrete-distributions.html"><a href="discrete-distributions.html#coin-tossing-distributions"><i class="fa fa-check"></i><b>4.6</b> Coin-Tossing Distributions</a></li>
<li class="chapter" data-level="4.7" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-probabilities"><i class="fa fa-check"></i><b>4.7</b> Binomial probabilities</a></li>
<li class="chapter" data-level="4.8" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-experiments"><i class="fa fa-check"></i><b>4.8</b> Binomial experiments</a></li>
<li class="chapter" data-level="4.9" data-path="discrete-distributions.html"><a href="discrete-distributions.html#binomial-computations"><i class="fa fa-check"></i><b>4.9</b> Binomial computations</a></li>
<li class="chapter" data-level="4.10" data-path="discrete-distributions.html"><a href="discrete-distributions.html#mean-and-standard-deviation-of-a-binomial"><i class="fa fa-check"></i><b>4.10</b> Mean and standard deviation of a Binomial</a></li>
<li class="chapter" data-level="4.11" data-path="discrete-distributions.html"><a href="discrete-distributions.html#negative-binomial-experiments"><i class="fa fa-check"></i><b>4.11</b> Negative Binomial Experiments</a></li>
<li class="chapter" data-level="4.12" data-path="discrete-distributions.html"><a href="discrete-distributions.html#exercises"><i class="fa fa-check"></i><b>4.12</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="continuous-distributions.html"><a href="continuous-distributions.html"><i class="fa fa-check"></i><b>5</b> Continuous Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="continuous-distributions.html"><a href="continuous-distributions.html#introduction-a-baseball-spinner-game"><i class="fa fa-check"></i><b>5.1</b> Introduction: A Baseball Spinner Game</a></li>
<li class="chapter" data-level="5.2" data-path="continuous-distributions.html"><a href="continuous-distributions.html#the-uniform-distribution"><i class="fa fa-check"></i><b>5.2</b> The Uniform Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="continuous-distributions.html"><a href="continuous-distributions.html#binomial-probabilities-and-the-normal-curve"><i class="fa fa-check"></i><b>5.3</b> Binomial Probabilities and the Normal Curve</a></li>
<li class="chapter" data-level="5.4" data-path="continuous-distributions.html"><a href="continuous-distributions.html#sampling-distribution-of-the-mean"><i class="fa fa-check"></i><b>5.4</b> Sampling Distribution of the Mean</a></li>
<li class="chapter" data-level="5.5" data-path="continuous-distributions.html"><a href="continuous-distributions.html#exercises"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html"><i class="fa fa-check"></i><b>6</b> Joint Probability Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#joint-probability-mass-function-sampling-from-a-box"><i class="fa fa-check"></i><b>6.1</b> Joint Probability Mass Function: Sampling From a Box</a></li>
<li class="chapter" data-level="6.2" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#multinomial-experiments"><i class="fa fa-check"></i><b>6.2</b> Multinomial Experiments</a></li>
<li class="chapter" data-level="6.3" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#joint-density-functions"><i class="fa fa-check"></i><b>6.3</b> Joint Density Functions</a></li>
<li class="chapter" data-level="6.4" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#independence-and-measuring-association"><i class="fa fa-check"></i><b>6.4</b> Independence and Measuring Association</a></li>
<li class="chapter" data-level="6.5" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#flipping-a-random-coin-the-beta-binomial-distribution"><i class="fa fa-check"></i><b>6.5</b> Flipping a Random Coin: The Beta-Binomial Distribution</a></li>
<li class="chapter" data-level="6.6" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>6.6</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="6.7" data-path="joint-probability-distributions.html"><a href="joint-probability-distributions.html#exercises"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="proportion.html"><a href="proportion.html"><i class="fa fa-check"></i><b>7</b> Learning About a Binomial Probability</a><ul>
<li class="chapter" data-level="7.1" data-path="proportion.html"><a href="proportion.html#introduction-thinking-about-a-proportion-subjectively"><i class="fa fa-check"></i><b>7.1</b> Introduction: Thinking About a Proportion Subjectively</a></li>
<li class="chapter" data-level="7.2" data-path="proportion.html"><a href="proportion.html#bayesian-inference-with-discrete-priors"><i class="fa fa-check"></i><b>7.2</b> Bayesian Inference with Discrete Priors</a><ul>
<li class="chapter" data-level="7.2.1" data-path="proportion.html"><a href="proportion.html#example-students-dining-preference"><i class="fa fa-check"></i><b>7.2.1</b> Example: students’ dining preference</a></li>
<li class="chapter" data-level="7.2.2" data-path="proportion.html"><a href="proportion.html#discrete-prior-distributions-for-proportion-p"><i class="fa fa-check"></i><b>7.2.2</b> Discrete prior distributions for proportion <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="7.2.3" data-path="proportion.html"><a href="proportion.html#likelihood"><i class="fa fa-check"></i><b>7.2.3</b> Likelihood</a></li>
<li class="chapter" data-level="7.2.4" data-path="proportion.html"><a href="proportion.html#posterior-distribution-for-proportion-p"><i class="fa fa-check"></i><b>7.2.4</b> Posterior distribution for proportion <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="7.2.5" data-path="proportion.html"><a href="proportion.html#inference-students-dining-preference"><i class="fa fa-check"></i><b>7.2.5</b> Inference: students’ dining preference</a></li>
<li class="chapter" data-level="7.2.6" data-path="proportion.html"><a href="proportion.html#discussion-using-a-discrete-prior"><i class="fa fa-check"></i><b>7.2.6</b> Discussion: using a discrete prior</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="proportion.html"><a href="proportion.html#continuous-priors"><i class="fa fa-check"></i><b>7.3</b> Continuous Priors</a><ul>
<li class="chapter" data-level="7.3.1" data-path="proportion.html"><a href="proportion.html#the-beta-distribution-and-probabilities"><i class="fa fa-check"></i><b>7.3.1</b> The Beta distribution and probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="proportion.html"><a href="proportion.html#updating-the-beta-prior"><i class="fa fa-check"></i><b>7.4</b> Updating the Beta Prior</a><ul>
<li class="chapter" data-level="7.4.1" data-path="proportion.html"><a href="proportion.html#bayes-rule-calculation"><i class="fa fa-check"></i><b>7.4.1</b> Bayes’ rule calculation</a></li>
<li class="chapter" data-level="7.4.2" data-path="proportion.html"><a href="proportion.html#from-beta-prior-to-beta-posterior"><i class="fa fa-check"></i><b>7.4.2</b> From Beta prior to Beta posterior</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="proportion.html"><a href="proportion.html#bayesian-inferences-with-continuous-priors"><i class="fa fa-check"></i><b>7.5</b> Bayesian Inferences with Continuous Priors</a><ul>
<li class="chapter" data-level="7.5.1" data-path="proportion.html"><a href="proportion.html#bayesian-hypothesis-testing"><i class="fa fa-check"></i><b>7.5.1</b> Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="7.5.2" data-path="proportion.html"><a href="proportion.html#bayesian-credible-intervals"><i class="fa fa-check"></i><b>7.5.2</b> Bayesian credible intervals</a></li>
<li class="chapter" data-level="7.5.3" data-path="proportion.html"><a href="proportion.html#bayesian-prediction"><i class="fa fa-check"></i><b>7.5.3</b> Bayesian prediction</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="proportion.html"><a href="proportion.html#predictive-checking"><i class="fa fa-check"></i><b>7.6</b> Predictive Checking</a><ul>
<li class="chapter" data-level="7.6.1" data-path="proportion.html"><a href="proportion.html#comparing-bayesian-models"><i class="fa fa-check"></i><b>7.6.1</b> Comparing Bayesian models</a></li>
<li class="chapter" data-level="7.6.2" data-path="proportion.html"><a href="proportion.html#posterior-predictive-checking"><i class="fa fa-check"></i><b>7.6.2</b> Posterior predictive checking</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="proportion.html"><a href="proportion.html#exercises"><i class="fa fa-check"></i><b>7.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mean.html"><a href="mean.html"><i class="fa fa-check"></i><b>8</b> Modeling Measurement and Count Data</a><ul>
<li class="chapter" data-level="8.1" data-path="mean.html"><a href="mean.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="mean.html"><a href="mean.html#modeling-measurements"><i class="fa fa-check"></i><b>8.2</b> Modeling Measurements</a><ul>
<li class="chapter" data-level="8.2.1" data-path="mean.html"><a href="mean.html#examples"><i class="fa fa-check"></i><b>8.2.1</b> Examples</a></li>
<li class="chapter" data-level="8.2.2" data-path="mean.html"><a href="mean.html#the-general-approach"><i class="fa fa-check"></i><b>8.2.2</b> The general approach</a></li>
<li class="chapter" data-level="8.2.3" data-path="mean.html"><a href="mean.html#outline-of-chapter"><i class="fa fa-check"></i><b>8.2.3</b> Outline of chapter</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mean.html"><a href="mean.html#Normal:Discrete"><i class="fa fa-check"></i><b>8.3</b> Bayesian Inference with Discrete Priors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="mean.html"><a href="mean.html#Normal:Discrete:Roger"><i class="fa fa-check"></i><b>8.3.1</b> Example: Roger Federer’s time-to-serve</a></li>
<li class="chapter" data-level="8.3.2" data-path="mean.html"><a href="mean.html#Normal:SamplingModel:derivation"><i class="fa fa-check"></i><b>8.3.2</b> Simplification of the likelihood</a></li>
<li class="chapter" data-level="8.3.3" data-path="mean.html"><a href="mean.html#Normal:SamplingModel:inference"><i class="fa fa-check"></i><b>8.3.3</b> Inference: Federer’s time-to-serve</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="mean.html"><a href="mean.html#Normal:Continuous"><i class="fa fa-check"></i><b>8.4</b> Continuous Priors</a><ul>
<li class="chapter" data-level="8.4.1" data-path="mean.html"><a href="mean.html#Normal:Continuous:prior"><i class="fa fa-check"></i><b>8.4.1</b> The Normal prior for mean <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="8.4.2" data-path="mean.html"><a href="mean.html#Normal:Continuous:choosing"><i class="fa fa-check"></i><b>8.4.2</b> Choosing a Normal prior</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate"><i class="fa fa-check"></i><b>8.5</b> Updating the Normal Prior</a><ul>
<li class="chapter" data-level="8.5.1" data-path="mean.html"><a href="mean.html#introduction-1"><i class="fa fa-check"></i><b>8.5.1</b> Introduction</a></li>
<li class="chapter" data-level="8.5.2" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:Overview"><i class="fa fa-check"></i><b>8.5.2</b> A quick peak at the update procedure</a></li>
<li class="chapter" data-level="8.5.3" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:BayesRule"><i class="fa fa-check"></i><b>8.5.3</b> Bayes’ rule calculation</a></li>
<li class="chapter" data-level="8.5.4" data-path="mean.html"><a href="mean.html#Normal:ContinuousUpdate:Conjugate"><i class="fa fa-check"></i><b>8.5.4</b> Conjugate Normal prior</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference"><i class="fa fa-check"></i><b>8.6</b> Bayesian Inferences for Continuous Normal Mean</a><ul>
<li class="chapter" data-level="8.6.1" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference:HTandCI"><i class="fa fa-check"></i><b>8.6.1</b> Bayesian hypothesis testing and credible interval</a></li>
<li class="chapter" data-level="8.6.2" data-path="mean.html"><a href="mean.html#Normal:ContinuousInference:Prediction"><i class="fa fa-check"></i><b>8.6.2</b> Bayesian prediction</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="mean.html"><a href="mean.html#Normal:PPC"><i class="fa fa-check"></i><b>8.7</b> Posterior Predictive Checking</a></li>
<li class="chapter" data-level="8.8" data-path="mean.html"><a href="mean.html#modeling-count-data"><i class="fa fa-check"></i><b>8.8</b> Modeling Count Data</a><ul>
<li class="chapter" data-level="8.8.1" data-path="mean.html"><a href="mean.html#examples-1"><i class="fa fa-check"></i><b>8.8.1</b> Examples</a></li>
<li class="chapter" data-level="8.8.2" data-path="mean.html"><a href="mean.html#the-poisson-distribution"><i class="fa fa-check"></i><b>8.8.2</b> The Poisson distribution</a></li>
<li class="chapter" data-level="8.8.3" data-path="mean.html"><a href="mean.html#bayesian-inferences"><i class="fa fa-check"></i><b>8.8.3</b> Bayesian inferences</a></li>
<li class="chapter" data-level="8.8.4" data-path="mean.html"><a href="mean.html#case-study-learning-about-website-counts"><i class="fa fa-check"></i><b>8.8.4</b> Case study: Learning about website counts</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="mean.html"><a href="mean.html#exercises"><i class="fa fa-check"></i><b>8.9</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>9</b> Simulation by Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="9.1" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#markov-chains"><i class="fa fa-check"></i><b>9.2</b> Markov Chains</a></li>
<li class="chapter" data-level="9.3" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#the-metropolis-algorithm"><i class="fa fa-check"></i><b>9.3</b> The Metropolis Algorithm</a></li>
<li class="chapter" data-level="9.4" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#example-cauchy-normal-problem"><i class="fa fa-check"></i><b>9.4</b> Example: Cauchy-Normal problem</a></li>
<li class="chapter" data-level="9.5" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#gibbs-sampling"><i class="fa fa-check"></i><b>9.5</b> Gibbs Sampling</a></li>
<li class="chapter" data-level="9.6" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#mcmc-inputs-and-diagnostics"><i class="fa fa-check"></i><b>9.6</b> MCMC Inputs and Diagnostics</a></li>
<li class="chapter" data-level="9.7" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#using-jags"><i class="fa fa-check"></i><b>9.7</b> Using JAGS</a></li>
<li class="chapter" data-level="9.8" data-path="simulation-by-markov-chain-monte-carlo.html"><a href="simulation-by-markov-chain-monte-carlo.html#exercises"><i class="fa fa-check"></i><b>9.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html"><i class="fa fa-check"></i><b>10</b> Bayesian Hierarchical Modeling</a><ul>
<li class="chapter" data-level="10.1" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#hierarchical-normal-modeling"><i class="fa fa-check"></i><b>10.2</b> Hierarchical Normal Modeling</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#hierarchical-beta-binomial-modeling"><i class="fa fa-check"></i><b>10.3</b> Hierarchical Beta-Binomial Modeling</a></li>
<li class="chapter" data-level="10.4" data-path="bayesian-hierarchical-modeling.html"><a href="bayesian-hierarchical-modeling.html#exercises"><i class="fa fa-check"></i><b>10.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>11</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#example-prices-and-areas-of-house-sales"><i class="fa fa-check"></i><b>11.2</b> Example: Prices and Areas of House Sales</a></li>
<li class="chapter" data-level="11.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-simple-linear-regression-model"><i class="fa fa-check"></i><b>11.3</b> A Simple Linear Regression Model</a></li>
<li class="chapter" data-level="11.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-weakly-informative-prior"><i class="fa fa-check"></i><b>11.4</b> A Weakly Informative Prior</a></li>
<li class="chapter" data-level="11.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#posterior-analysis"><i class="fa fa-check"></i><b>11.5</b> Posterior Analysis</a></li>
<li class="chapter" data-level="11.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#inference-through-mcmc"><i class="fa fa-check"></i><b>11.6</b> Inference through MCMC</a></li>
<li class="chapter" data-level="11.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#bayesian-inferences-with-simple-linear-regression"><i class="fa fa-check"></i><b>11.7</b> Bayesian Inferences with Simple Linear Regression</a></li>
<li class="chapter" data-level="11.8" data-path="mean.html"><a href="mean.html#informative-prior"><i class="fa fa-check"></i><b>11.8</b> Informative Prior</a></li>
<li class="chapter" data-level="11.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#a-conditional-means-prior"><i class="fa fa-check"></i><b>11.9</b> A Conditional Means Prior</a></li>
<li class="chapter" data-level="11.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises"><i class="fa fa-check"></i><b>11.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html"><i class="fa fa-check"></i><b>12</b> Bayesian Multiple Regression and Logistic Models</a><ul>
<li class="chapter" data-level="12.1" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#introduction"><i class="fa fa-check"></i><b>12.1</b> Introduction</a></li>
<li class="chapter" data-level="12.2" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#bayesian-multiple-linear-regression"><i class="fa fa-check"></i><b>12.2</b> Bayesian Multiple Linear Regression</a></li>
<li class="chapter" data-level="12.3" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#bayesian-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> Bayesian Logistic Regression </a></li>
<li class="chapter" data-level="12.4" data-path="bayesian-multiple-regression-and-logistic-models.html"><a href="bayesian-multiple-regression-and-logistic-models.html#exercises"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>13</b> Case Studies</a><ul>
<li class="chapter" data-level="13.1" data-path="case-studies.html"><a href="case-studies.html#introduction"><i class="fa fa-check"></i><b>13.1</b> Introduction</a></li>
<li class="chapter" data-level="13.2" data-path="case-studies.html"><a href="case-studies.html#federalist-papers-study"><i class="fa fa-check"></i><b>13.2</b> Federalist Papers Study</a></li>
<li class="chapter" data-level="13.3" data-path="case-studies.html"><a href="case-studies.html#negative-binomial-sampling"><i class="fa fa-check"></i><b>13.3</b> Negative Binomial sampling</a></li>
<li class="chapter" data-level="13.4" data-path="case-studies.html"><a href="case-studies.html#comparison-of-rates-for-two-authors"><i class="fa fa-check"></i><b>13.4</b> Comparison of rates for two authors</a></li>
<li class="chapter" data-level="13.5" data-path="case-studies.html"><a href="case-studies.html#which-words-distinguish-the-two-authors"><i class="fa fa-check"></i><b>13.5</b> Which words distinguish the two authors?</a></li>
<li class="chapter" data-level="13.6" data-path="case-studies.html"><a href="case-studies.html#career-trajectories"><i class="fa fa-check"></i><b>13.6</b> Career Trajectories</a></li>
<li class="chapter" data-level="13.7" data-path="case-studies.html"><a href="case-studies.html#latent-class-modeling"><i class="fa fa-check"></i><b>13.7</b> Latent Class Modeling</a></li>
<li class="chapter" data-level="13.8" data-path="case-studies.html"><a href="case-studies.html#exercises"><i class="fa fa-check"></i><b>13.8</b> Exercises</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Probability and Bayesian Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mean" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Modeling Measurement and Count Data</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">8.1</span> Introduction</h2>
<p>We first consider the general situation where there is a hypothetical population of individuals of interest and there is a continuous-valued measurement <span class="math inline">\(Y\)</span> associated with each individual. One represents the collection of measurements from all individuals by means of a continuous probability density <span class="math inline">\(f(y)\)</span>. As discussed in Chapter 5, one summarizes this probability density with the mean <span class="math inline">\(\mu\)</span>:
<span class="math display" id="eq:mean">\[\begin{equation}
\mu = \int y f(y) dy.
\tag{8.1}
\end{equation}\]</span>
The value <span class="math inline">\(\mu\)</span> gives us a sense of the location of a typical value of the continuous measurement <span class="math inline">\(Y\)</span>.</p>
<p>To learn about the population of measurements, a random sample of individuals <span class="math inline">\(Y_1, ..., Y_n\)</span> will be taken. The general inferential problem is to use these measurements together with any prior beliefs to learn about the population mean <span class="math inline">\(\mu\)</span>. In other words, the goal is to use the collected measurements to learn about a typical value of the population of measurements.</p>
</div>
<div id="modeling-measurements" class="section level2">
<h2><span class="header-section-number">8.2</span> Modeling Measurements</h2>
<div id="examples" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Examples</h3>
<div id="college-applications" class="section level4 unnumbered">
<h4>College applications</h4>
<p>How many college applications does a high school senior in the United States complete? Here one imagines a population of all American high school seniors and the measurement is the number of completed college applications.
The unknown quantity of interest is the mean number of applications <span class="math inline">\(\mu\)</span> completed by these high school seniors. The inferential question may be stated by asking, on average, how many college applications does an American high school senior complete. The answer to this question gives one a sense of the number of completed applications for a typical high school senior.
To learn about the average <span class="math inline">\(\mu\)</span>, it would be infeasible to collect this measurement from every high school senior in the U.S. Instead, a survey is typically conducted to a sample of high school seniors (ideally a sample representative of all American high school seniors) and based on the measurements from this sample, some inference is performed about the mean number of college applications.</p>
</div>
<div id="household-spending" class="section level4 unnumbered">
<h4>Household spending</h4>
<p>How much does a household in San Francisco spend on housing every month? One visualizes the population of households in San Francisco and the continuous measurement is the amount of money spent on housing (either rent for renters and mortgage for homeowners) for a resident.
One can ask “on average, how much does a household spend on housing every month in San Francisco?”, and the answer to this question gives one a sense of the housing costs for a typical household in San Francisco. To learn about the mean value of housing <span class="math inline">\(\mu\)</span> of all San Francisco residents, a sample survey is conducted. The mean value of the housing costs <span class="math inline">\(\bar y\)</span> from this sample of surveyed households is informative about the mean housing cost <span class="math inline">\(\mu\)</span> for all residents.</p>
</div>
<div id="weights-of-cats" class="section level4 unnumbered">
<h4>Weights of cats</h4>
<p>Suppose you have a domestic shorthair cat weighing 14 pounds and you want to find out if she is overweight. One imagines a population of all domestic shorthair cats and the continuous measurement is the weight in pounds. Suppose you were able to compute the mean weight <span class="math inline">\(\mu\)</span> of all shorthair cats. Then by comparing 14 pounds (the weight of our cat) to this mean, you would know whether your cat is overweight, or underweight, or close to the mean.
If we were able to find the distribution of the weights of all domestic shorthair cats, then one observes the proportion of weights smaller than 14 pounds in the distribution and learns if the cat is severely overweight. To learn if our cat is overweight, you can ask the vet. How does the vet know? Extensive research has been conducted periodically to record weights of a large sample of domestic shorthair cats, and by using these sample of weights, the vet performs an inference about the mean <span class="math inline">\(\mu\)</span> of the weights of all domestic shorthair cats.</p>
</div>
<div id="comment-elements-of-an-inference-problem" class="section level4 unnumbered">
<h4>Comment elements of an inference problem</h4>
<p>All three examples have common elements:</p>
<ul>
<li><p>One has an underlying population of measurements, where the measurement is an integer, such as the number of college applications, or continuous, such as a housing cost or a cat weight.</p></li>
<li><p>One is interested in learning about the value of the mean <span class="math inline">\(\mu\)</span> of the population of measurements.</p></li>
<li><p>It is impossible or impractical to collect all measurements from the population, so one will collect a sample of measurements <span class="math inline">\(Y_1, ..., Y_n\)</span> and use the observed measurements to learn about the unknown population mean <span class="math inline">\(\mu\)</span>.</p></li>
</ul>
</div>
</div>
<div id="the-general-approach" class="section level3">
<h3><span class="header-section-number">8.2.2</span> The general approach</h3>
<p>Recall the three general steps of Bayesian inference discussed in Chapter 7 in the context of an unknown proportion <span class="math inline">\(p\)</span>.</p>
<ul>
<li><p>Step 1: <strong>Prior</strong> We express an opinion about the location of the proportion <span class="math inline">\(p\)</span> before sampling.</p></li>
<li><p>Step 2: <strong>Data/Likelihood</strong> We take the sample and record the observed proportion.</p></li>
<li><p>Step 3: <strong>Posterior</strong> We use Bayes’ rule to sharpen and update the previous opinion about <span class="math inline">\(p\)</span> given the information from the sample.</p></li>
</ul>
<p>In this setting, we have a continuous population of measurements that we represent by the random variable <span class="math inline">\(Y\)</span> with density function <span class="math inline">\(f(y)\)</span>. It is convenient to assume that this population has a Normal shape with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. That is, a single measurement <span class="math inline">\(Y\)</span> is assume to come from the density function
<span class="math display" id="eq:normalpdf">\[\begin{equation}
f(y) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{(y - \mu)^2}{2 \sigma^2}\right\}, -\infty &lt; y&lt; \infty.
\tag{8.2}
\end{equation}\]</span>
displayed in Figure 8.1. To simplify the discussion, it is convenient to assume that the standard deviation <span class="math inline">\(\sigma\)</span> of the measurement distribution is known. Then the objective is to learn about the single mean measurement <span class="math inline">\(\mu\)</span>.</p>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="../LATEX/figures/chapter8/normaldensity.png" alt="Normal sampling density with mean $\mu$." width="500" />
<p class="caption">
Figure 8.1: Normal sampling density with mean <span class="math inline">\(\mu\)</span>.
</p>
</div>
<p>Step 1 in Bayesian inference is to express an opinion about the parameter. In this continuous measurement setting, one constructs a prior for the mean parameter <span class="math inline">\(\mu\)</span> that expresses one’s opinion about the location of this mean.
In this chapter, we discuss different ways to specify a prior distribution for <span class="math inline">\(\mu\)</span>. One attractive discrete approach for expressing this prior opinion, similar to the approach in Chapter 7 for a proportion <span class="math inline">\(p\)</span>, has two steps. First one constructs a list of possible values of <span class="math inline">\(\mu\)</span>, and then one assigns probabilities to the possible values to reflect one’s belief. Alternatively, we will describe the use of a continuous prior to represent one’s belief for <span class="math inline">\(\mu\)</span>. This is a more realistic approach for constructing a prior since one typically views the mean as a real-valued parameter.</p>
<p>Step 2 of our process is to collect measurements from a random sample to gain more information about the parameter <span class="math inline">\(\mu\)</span>.
In our first situation, one collects the number of applications from a sample of 100 high school seniors. In the second example, one collects a sample of 2000 housing costs, each from a sampled San Francisco household. The third example collects a sample of 200 different weights of domestic shorthair cats, each from a sampled cat. If these measurements are viewed as independent observations from a Normal sampling density with mean <span class="math inline">\(\mu\)</span>, then one constructs a likelihood function which is the joint density of the sampled measurements viewed as a function of the unknown parameter.</p>
<p>Once the prior is specified and measurements have been collected, one proceeds to Step 3 to use Bayes’ rule to update one’s prior opinion to obtain a posterior distribution for the mean <span class="math inline">\(\mu\)</span>. The algebraic implementation of Bayes’ rule is a bit more tedious when dealing with continuous data with a Normal sampling density. But we will see there is a simple procedure for computing the posterior mean and standard deviation.</p>
</div>
<div id="outline-of-chapter" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Outline of chapter</h3>
<p>Throughout this chapter, the entire inferential process is described for learning about a mean <span class="math inline">\(\mu\)</span> assuming a Normal sampling density for the measurements. This chapter discusses how to construct a prior distribution that matches one’s prior belief, how to extract information from the data by the likelihood function, and how to update one’s opinion in the posterior, combining the prior and data information in a natural way.</p>
<p>Section 8.3 introduces inference with a discrete prior distribution for the mean <span class="math inline">\(\mu\)</span> and Section 8.4 introduces the continuous family of Normal prior distributions for the mean. The inferential process with a Normal prior distribution is described in detail in Section 8.5. Section 8.6 describes some general Bayesian inference methods in this Normal data/Normal prior setting, such as Bayesian hypothesis testing, Bayesian credible intervals and Bayesian prediction. These sections describe the use of both exact analytical solutions and approximation simulation-based calculations. Section 8.7 introduces the use of the posterior predictive distribution as a general tool for checking if the observed data is consistent with predictions from the Bayesian model.</p>
<p>The chapter concludes in Section 8.8 by introducing a popular one-parameter model for counts, the Poisson distribution, and its conjugate Gamma distribution for representing prior opinion. Although this section does not deal with the Normal mean situation, the exposure to the important Gamma-Poisson conjugacy will enhance our understanding and knowledge of the analytical process of combining the prior and likelihood to obtain the posterior distribution.</p>
</div>
</div>
<div id="Normal:Discrete" class="section level2">
<h2><span class="header-section-number">8.3</span> Bayesian Inference with Discrete Priors</h2>
<div id="Normal:Discrete:Roger" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Example: Roger Federer’s time-to-serve</h3>
<p>Roger Federer is recognized as one of the greatest players in tennis history. One aspect of his play that people enjoy is his businesslike way of serving to start a point in tennis. Federer appears to be efficient in his preparation to serve and some of his service games are completed very quickly. One measures one’s service efficiency by the time-to-serve which is the measured time in seconds between the end of the previous point and the beginning of the current point.</p>
<p>Since Federer is viewed as an efficient server, this raises the question: how long, on average, is Federer’s time-to-serve? We know two things about his time-to-serve measurements. First, since they are time measurements, they are continuous variables. Second, due to a number of other variables, the measurements will vary from serve to serve. Suppose one collects a single time-to-serve measurement in seconds. denoted as <span class="math inline">\(Y\)</span>. It seems reasonable to assume <span class="math inline">\(Y\)</span> is Normally distributed with unknown mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. From previous data, we assume that the standard deviation is known and given by <span class="math inline">\(\sigma = 4\)</span> seconds.</p>
<p>Recall the Normal probability curve has the general form</p>
<p><span class="math display" id="eq:normalpdfone">\[\begin{equation}
f(y) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{(y - \mu)^2}{2 \sigma^2}\right\}, -\infty &lt; y&lt; \infty.
\tag{8.3}
\end{equation}\]</span>
Since <span class="math inline">\(\sigma = 4\)</span> is known, the only parameter in Equation (8.3) is <span class="math inline">\(\mu\)</span>. We are interested in learning about the mean time-to-serve <span class="math inline">\(\mu\)</span>.</p>
<p>A convenient first method of implementing Bayesian inference is by the use of a discrete prior.
One specifies a subjective discrete prior for Federer’s mean time-to-serve by specifying a list of plausible values for <span class="math inline">\(\mu\)</span> and assigning a probability to each of these values.</p>
<p>In particular suppose one thinks that values of the equally spaced values <span class="math inline">\(\mu\)</span> = 15, 16, <span class="math inline">\(\cdots\)</span>, 22 are plausible. In addition, one does not have any good reason to think that any of these values for the mean are more or less likely, so a Uniform prior will be assigned where each value of <span class="math inline">\(\mu\)</span> is assigned the same probability <span class="math inline">\(\frac{1}{8}\)</span>.
<span class="math display" id="eq:normaldiscreteprior">\[\begin{equation}
\pi(\mu) = \frac{1}{8}, \, \, \, \, \mu = 15, 16, ..., 22.
\tag{8.4}
\end{equation}\]</span>
Each value of <span class="math inline">\(\mu\)</span> corresponds to a particular Normal sampling curve for the time-to-serve measurement.
Figure 8.2 displays the eight possible Normal sampling curves. Our prior says that each of these eight sampling curves has the same prior probability.</p>
<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-4-1.png" alt="Eight possible Normal sampling curves corresponding to a discrete Uniform prior on $\mu$." width="672" />
<p class="caption">
Figure 8.2: Eight possible Normal sampling curves corresponding to a discrete Uniform prior on <span class="math inline">\(\mu\)</span>.
</p>
</div>
<p>To learn more about the mean <span class="math inline">\(\mu\)</span>, one collects a single time-to-serve measurement for Federer, and suppose it is 15.1 seconds, that is, one observes <span class="math inline">\(Y = 15.1\)</span>.
The likelihood function is the Normal density of the actual observation <span class="math inline">\(y\)</span> viewed as a function of the mean <span class="math inline">\(\mu\)</span> (remember that it was assumed that <span class="math inline">\(\sigma = 4\)</span> was given). By substituting in the observation <span class="math inline">\(y = 15.1\)</span> and the known value of <span class="math inline">\(\sigma = 4\)</span>, one writes the likelihood function as</p>
<p><span class="math display">\[\begin{eqnarray*}
L(\mu) = \frac{1}{\sqrt{2 \pi} 4} \exp\left\{- \frac{1}{2 (4)^2}(15.1 - \mu)^2\right\}.
\end{eqnarray*}\]</span></p>
<p>For each possible value of <span class="math inline">\(\mu\)</span>, we substitute the value into the likelihood expression. For example, the likelihood of <span class="math inline">\(\mu = 15\)</span> is equal to
<span class="math display">\[\begin{eqnarray*}
L(15) &amp;=&amp; \frac{1}{\sqrt{2 \pi} (4)} \exp\left(- \frac{1}{2 (4)^2}(15.1 - 15)^2\right) \nonumber \\
&amp;\approx &amp; 0.0997.
\end{eqnarray*}\]</span>
This calculation is repeated for each of the eight values <span class="math inline">\(\mu = 15, 16, \cdots, 22\)</span>, obtaining eight likelihood values.</p>
<p>A discrete prior has been assigned to the list of possible values of <span class="math inline">\(\mu\)</span> and one is now able to apply Bayes’ rule to obtain the posterior distribution for <span class="math inline">\(\mu\)</span>. The posterior probability of the value <span class="math inline">\(\mu = \mu_i\)</span> given the data <span class="math inline">\(y\)</span> for a discrete prior has the form
<span class="math display" id="eq:normalBayesRule1">\[\begin{equation}
\pi(\mu_i \mid y) = \frac{\pi(\mu_i) \times L(\mu_i)}{\sum_j \pi(\mu_j) \times L(\mu_j)},
\tag{8.5}
\end{equation}\]</span>
where <span class="math inline">\(\pi(\mu_i)\)</span> is the prior probability of <span class="math inline">\(\mu = \mu_i\)</span> and <span class="math inline">\(L(\mu_i)\)</span> is the likelihood function evaluated at <span class="math inline">\(\mu = \mu_i\)</span>.</p>
<p>If a discrete Uniform prior distribution for <span class="math inline">\(\mu\)</span> is assigned, one has <span class="math inline">\(\pi(\mu_i) = \frac{1}{8}\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span>, and <span class="math inline">\(\pi(\mu_i)\)</span> is canceled out from the numerator and denominator in Equation (8.5). In this case one calculates the likelihood values <span class="math inline">\(L(\mu_i)\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span> and normalizes these values to obtain the posterior probabilities <span class="math inline">\(\pi(\mu_i \mid y)\)</span>.
Table 8.1 displays the values of <span class="math inline">\(\mu\)</span> and the corresponding values of Prior, Data/Likelihood, and Posterior. Readers are encouraged to verify the results shown in the table.</p>
<p>Table 8.1. Value, prior, data/likelihood and posterior for <span class="math inline">\(\mu\)</span> with a single observation.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(\mu\)</span></th>
<th align="center">Prior</th>
<th align="center">Data/Likelihood</th>
<th align="center">Posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">15</td>
<td align="center">0.125</td>
<td align="center">0.0997</td>
<td align="center">0.1888</td>
</tr>
<tr class="even">
<td align="center">16</td>
<td align="center">0.125</td>
<td align="center">0.0972</td>
<td align="center">0.1842</td>
</tr>
<tr class="odd">
<td align="center">17</td>
<td align="center">0.125</td>
<td align="center">0.0891</td>
<td align="center">0.1688</td>
</tr>
<tr class="even">
<td align="center">18</td>
<td align="center">0.125</td>
<td align="center">0.0767</td>
<td align="center">0.1452</td>
</tr>
<tr class="odd">
<td align="center">19</td>
<td align="center">0.125</td>
<td align="center">0.0620</td>
<td align="center">0.1174</td>
</tr>
<tr class="even">
<td align="center">20</td>
<td align="center">0.125</td>
<td align="center">0.0471</td>
<td align="center">0.0892</td>
</tr>
<tr class="odd">
<td align="center">21</td>
<td align="center">0.125</td>
<td align="center">0.0336</td>
<td align="center">0.0637</td>
</tr>
<tr class="even">
<td align="center">22</td>
<td align="center">0.125</td>
<td align="center">0.0225</td>
<td align="center">0.0427</td>
</tr>
</tbody>
</table>
<p>With the single measurement of time-to-serve of <span class="math inline">\(y = 15.1\)</span>, one sees from Table 8.1 that the posterior distribution for <span class="math inline">\(\mu\)</span> favors values <span class="math inline">\(\mu\)</span> = 15, and 16. In fact, the posterior probabilities decrease as a function of <span class="math inline">\(\mu\)</span>. The Prior column reminds us that the prior distribution is Uniform. Bayesian inference uses the collected data to sharpen one’s belief about the unknown parameter from the prior distribution to the posterior distribution. For this single observation, the sample mean is <span class="math inline">\(y = 15.1\)</span> and the <span class="math inline">\(\mu\)</span> value closest to the sample mean (<span class="math inline">\(\mu = 15\)</span>) is assigned the highest posterior probability.</p>
<p>Typically one collects multiple time-to-serve measurements. Suppose one collects <span class="math inline">\(n\)</span> time-to-serve measurements, denoted as <span class="math inline">\(Y_1, ..., Y_n\)</span>, that are Normally distributed with mean <span class="math inline">\(\mu\)</span> and fixed standard deviation <span class="math inline">\(\sigma = 4\)</span>. Each observation follows the same Normal density
<span class="math display" id="eq:normalpdfiid">\[\begin{equation}
f(y_i) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{\frac{-(y_i - \mu)^2}{2 \sigma^2}\right\}, -\infty &lt; y_i &lt; \infty.
\tag{8.6}
\end{equation}\]</span>
Again since <span class="math inline">\(\sigma = 4\)</span> is known, the only parameter in Equation (8.6) is <span class="math inline">\(\mu\)</span> and we are interested in learning about this mean parameter <span class="math inline">\(\mu\)</span>.
Suppose the same discrete Uniform prior is used as in Equation (8.4) and graphed in Figure 8.2. The mean <span class="math inline">\(\mu\)</span> takes on the values <span class="math inline">\(\{15, 16, \cdots, 22\}\)</span> with each value assigned the same probability of <span class="math inline">\(\frac{1}{8}\)</span>.</p>
<p>Suppose one collects a sample of 20 times-to-serve for Federer:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="mean.html#cb1-1"></a><span class="fl">15.1</span> <span class="fl">11.8</span> <span class="fl">21.0</span> <span class="fl">22.7</span> <span class="fl">18.6</span> <span class="fl">16.2</span> <span class="fl">11.1</span> <span class="fl">13.2</span> <span class="fl">20.4</span> <span class="fl">19.2</span> </span>
<span id="cb1-2"><a href="mean.html#cb1-2"></a><span class="fl">21.2</span> <span class="fl">14.3</span> <span class="fl">18.6</span> <span class="fl">16.8</span> <span class="fl">20.3</span> <span class="fl">19.9</span> <span class="fl">15.0</span> <span class="fl">13.4</span> <span class="fl">19.9</span> <span class="fl">15.3</span></span></code></pre></div>
<p>When multiple time-to-serve measurements are taken, the likelihood function is the joint density of the actual observed values <span class="math inline">\(y_1, ..., y_n\)</span> viewed as a function of the mean <span class="math inline">\(\mu\)</span>. After some algebra (detailed derivation in Section 8.3.2), one writes the likelihood function as
<span class="math display" id="eq:normaldiscretejointlikelihood">\[\begin{eqnarray}
L(\mu) &amp; = &amp;\prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{1}{2 \sigma^2}(y_i - \mu)^2\right\} \nonumber \\
&amp; \propto &amp;\exp\left\{-\frac{n}{2 \sigma^2}(\bar y - \mu)^2\right\} \nonumber \\
&amp; = &amp; \exp\left\{-\frac{20}{2 (4)^2}(\bar y - \mu)^2\right\} ,
\tag{8.7}
\end{eqnarray}\]</span>
where we have substituted the known values <span class="math inline">\(n = 20\)</span> and the standard deviation <span class="math inline">\(\sigma = 4\)</span>.
From our sample, we compute the sample mean <span class="math inline">\(\bar y = (15.1 + 11.8 + ... + 15.3) / 20 = 17.2\)</span>. The value of <span class="math inline">\(\bar y\)</span> is substituted into Equation (8.7), and for each possible value of <span class="math inline">\(\mu\)</span>, we substitute the value to find the corresponding likelihood. For example, the likelihood of <span class="math inline">\(\mu = 15\)</span> is equal to
<span class="math display">\[\begin{align*}
L(15) &amp; = \exp\left\{-\frac{20}{2 (4)^2}(17.2 - 15)^2\right\} \nonumber \\ 
&amp; \approx 0.022.
\end{align*}\]</span>
This calculation is repeated for each of the eight values <span class="math inline">\(\mu = 15, 16, ..., 22\)</span>, obtaining eight likelihood values.</p>
<p>One now applies Bayes’ rule to obtain the posterior distribution for <span class="math inline">\(\mu\)</span>. The posterior probability of <span class="math inline">\(\mu = \mu_i\)</span> given the sequence of recorded times-to-serve <span class="math inline">\(y_1, \cdots, y_n\)</span> has the form
<span class="math display" id="eq:normalBayesRule">\[\begin{equation}
\pi(\mu_i \mid y_1, \cdots, y_n) = \frac{\pi(\mu_i) \times L(\mu_i)}{\sum_j \pi(\mu_j) \times L(\mu_j)},
\tag{8.8}
\end{equation}\]</span>
where <span class="math inline">\(\pi(\mu_i)\)</span> is the prior probability of <span class="math inline">\(\mu = \mu_i\)</span> and <span class="math inline">\(L(\mu_i)\)</span> is the likelihood function evaluated at <span class="math inline">\(\mu = \mu_i\)</span>. We saw in equation <a href="mean.html#eq:normaldiscretejointlikelihood">(8.7)</a> that only the sample mean, <span class="math inline">\(\bar{y}\)</span>, is needed in the calculation of the likelihood, so <span class="math inline">\(\bar{y}\)</span> is used in place of <span class="math inline">\(y_1, \cdots, y_n\)</span> in the formula.</p>
<p>With a discrete Uniform prior distribution for <span class="math inline">\(\mu\)</span>, again one has <span class="math inline">\(\pi(\mu_i) = \frac{1}{8}\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span> and <span class="math inline">\(\pi(\mu_i)\)</span> is canceled out from the numerator and denominator in Equation (8.8). One calculates the posterior probabilities by computing <span class="math inline">\(L(\mu_i)\)</span> for all <span class="math inline">\(i = 1, \cdots, 8\)</span> and normalizing these values. Table 8.2 displays the values of <span class="math inline">\(\mu\)</span> and the corresponding values of Prior, Data/Likelihood, and Posterior. Readers are encouraged to verify the results shown in the table.</p>
<p>Table 8.2. Value, prior, data/likelihood, and posterior for <span class="math inline">\(\mu\)</span> with <span class="math inline">\(n\)</span> observations.</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(\mu\)</span></th>
<th align="center">Prior</th>
<th align="center">Data/Likelihood</th>
<th align="center">Posterior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">15</td>
<td align="center">0.125</td>
<td align="center">0.0217</td>
<td align="center">0.0217</td>
</tr>
<tr class="even">
<td align="center">16</td>
<td align="center">0.125</td>
<td align="center">0.1813</td>
<td align="center">0.1815</td>
</tr>
<tr class="odd">
<td align="center">17</td>
<td align="center">0.125</td>
<td align="center">0.4350</td>
<td align="center">0.4353</td>
</tr>
<tr class="even">
<td align="center">18</td>
<td align="center">0.125</td>
<td align="center">0.2990</td>
<td align="center">0.2992</td>
</tr>
<tr class="odd">
<td align="center">19</td>
<td align="center">0.125</td>
<td align="center">0.0589</td>
<td align="center">0.0589</td>
</tr>
<tr class="even">
<td align="center">20</td>
<td align="center">0.125</td>
<td align="center">0.0033</td>
<td align="center">0.0033</td>
</tr>
<tr class="odd">
<td align="center">21</td>
<td align="center">0.125</td>
<td align="center">0.0001</td>
<td align="center">0.0001</td>
</tr>
<tr class="even">
<td align="center">22</td>
<td align="center">0.125</td>
<td align="center">0.0000</td>
<td align="center">0.0000</td>
</tr>
</tbody>
</table>
<p>It is helpful to construct a graph (see Figure 8.3) where one contrasts the prior and probability probabilities for the mean time-to-serve <span class="math inline">\(\mu\)</span>. While the prior distribution is flat, the posterior distribution for <span class="math inline">\(\mu\)</span> favors the values <span class="math inline">\(\mu\)</span> = 16, 17, and 18 seconds.
Bayesian inference uses the observed data to revise one’s belief about the unknown parameter from the prior distribution to the posterior distribution. Recall that the sample mean <span class="math inline">\(\bar{y}\)</span> = 17.2 seconds. From Table 8.2 and Figure 8.3 one sees the clear effect of the observed sample mean – <span class="math inline">\(\mu\)</span> is likely to be close to the value 17.2.</p>
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-6-1.png" alt="Prior and posterior probabilities of the Normal mean $\mu$ with a sample of observations." width="672" />
<p class="caption">
Figure 8.3: Prior and posterior probabilities of the Normal mean <span class="math inline">\(\mu\)</span> with a sample of observations.
</p>
</div>
</div>
<div id="Normal:SamplingModel:derivation" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Simplification of the likelihood</h3>
<p>The likelihood function is the joint density of the observations <span class="math inline">\(y_1, ..., y_n\)</span>, viewed as a function of the mean <span class="math inline">\(\mu\)</span> (since <span class="math inline">\(\sigma=4\)</span> is given). With <span class="math inline">\(n\)</span> observations being <em>identically and independently distributed (i.i.d.)</em> as <span class="math inline">\({\rm{Normal}}({\mu, 4})\)</span>, the likelihood function is the product of Normal density terms. In the algebra work that will be done shortly, the likelihood, as a function of <span class="math inline">\(\mu\)</span>, is found to be Normal with mean <span class="math inline">\(\bar y\)</span> and standard deviation <span class="math inline">\(\sigma / \sqrt{n}\)</span>.</p>
<p>The calculation of the posterior probabilities is an application of Bayes’ rule illustrated in earlier chapters. One creates a data frame of values <code>mu</code> and corresponding probabilities <code>Prior</code>. One computes the likelihood values in the variable <code>Likelihood</code> and the posterior probabilities are found using the <code>bayesian_crank()</code> function.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="mean.html#cb2-1"></a>df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">mu =</span> <span class="kw">seq</span>(<span class="dv">15</span>, <span class="dv">22</span>, <span class="dv">1</span>),</span>
<span id="cb2-2"><a href="mean.html#cb2-2"></a>                 <span class="dt">Prior =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">8</span>, <span class="dv">8</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb2-3"><a href="mean.html#cb2-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Likelihood =</span> <span class="kw">dnorm</span>(mu, <span class="fl">17.2</span>, <span class="dv">4</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">20</span>))) </span>
<span id="cb2-4"><a href="mean.html#cb2-4"></a></span>
<span id="cb2-5"><a href="mean.html#cb2-5"></a>df &lt;-<span class="st"> </span><span class="kw">bayesian_crank</span>(df) </span>
<span id="cb2-6"><a href="mean.html#cb2-6"></a><span class="kw">round</span>(df, <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##   mu Prior Likelihood Product Posterior
## 1 15 0.125     0.0217  0.0027    0.0217
## 2 16 0.125     0.1813  0.0227    0.1815
## 3 17 0.125     0.4350  0.0544    0.4353
## 4 18 0.125     0.2990  0.0374    0.2992
## 5 19 0.125     0.0589  0.0074    0.0589
## 6 20 0.125     0.0033  0.0004    0.0033
## 7 21 0.125     0.0001  0.0000    0.0001
## 8 22 0.125     0.0000  0.0000    0.0000</code></pre>
<div id="derivation-of-lmu-propto-exp-left-fracn2-sigma2bary---mu2right" class="section level4 unnumbered">
<h4>Derivation of <span class="math inline">\(L(\mu) \propto \exp \left(-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right)\)</span></h4>
<p>In the following, we combine the terms in the exponent, expand all of the summation terms, and complete the square to get the result.</p>
<p><span class="math display" id="eq:normalDev">\[\begin{eqnarray}
L(\mu) &amp;=&amp;  \prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{1}{2 \sigma^2}(y_i - \mu)^2\right\}  \nonumber \\
	   &amp;=&amp; \left(\frac{1}{\sqrt{2 \pi}\sigma}\right)^n \exp\left\{-\frac{1}{2 \sigma^2} \sum_{i=1}^{n} (y_i - \mu)^2\right\}\nonumber \\
&amp;\propto&amp; \exp \left\{ -\frac{1}{2 \sigma^2} \sum_{i=1}^{n} (y_i^2 - 2\mu y_i + \mu^2)\right\} \nonumber \\
\texttt{[expand the $\sum$ terms]} &amp;=&amp; \exp \left\{ -\frac{1}{2 \sigma^2} \left( \sum_{i=1}^{n} y_i^2 - 2\mu \sum_{i=1}^{n} y_i + n\mu^2 \right) \right\} \nonumber \\
&amp;\propto&amp; \exp \left\{- \frac{1}{2 \sigma^2} \left(-2 \mu \sum_{i=1}^{n} y_i + n \mu^2 \right) \right\}\nonumber \\
\texttt{[replace $\sum$ with $n\bar{y}$]} &amp;=&amp; \exp \left\{ - \frac{1}{2 \sigma^2} \left(-2 n \mu \bar{y} + n \mu^2 \right) \right\}\nonumber \\
\texttt{[complete the square]} &amp;=&amp; \exp \left\{ -\frac{n}{2 \sigma^2} (\mu^2 - 2\mu \bar{y} + \bar{y}^2) + \frac{n}{2 \sigma^2} \bar{y}^2\right\} \nonumber \\
&amp;\propto&amp; \exp \left\{-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right\}
\tag{8.9}
\end{eqnarray}\]</span></p>
</div>
<div id="sufficient-statistic" class="section level4 unnumbered">
<h4>Sufficient statistic</h4>
<p>There are different ways of writing and simplifying the likelihood function. One can choose to keep the product sign and each <span class="math inline">\(y_i\)</span> term, and leave the likelihood function as
<span class="math display" id="eq:sufficient1">\[\begin{equation}
L(\mu) =  \prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma} \exp\left\{- \frac{1}{2 \sigma^2}(y_i - \mu)^2\right\}.
\tag{8.10}
\end{equation}\]</span>
Doing so requires one to calculate the individual likelihood from each time-to-serve measurement <span class="math inline">\(y_i\)</span> and multiply these values to obtain the function <span class="math inline">\(L(\mu)\)</span> used to obtain the posterior probability.</p>
<p>If one instead simplifies the likelihood to be
<span class="math display" id="eq:sufficient2">\[\begin{equation}
L(\mu) \propto \exp \left\{-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right\},
\tag{8.11}
\end{equation}\]</span>
all the proportionality constants drop out in the calculation of the posterior probabilities for different values of <span class="math inline">\(\mu\)</span>. In the application of Bayes’ rule, one only needs to know the number of observations <span class="math inline">\(n\)</span> and the mean time to serve <span class="math inline">\(\bar{y}\)</span> to calculate the posterior. Since the likelihood function depends on the data only through the value <span class="math inline">\(\bar{y}\)</span>, the statistic <span class="math inline">\(\bar{y}\)</span> is called a <strong>sufficient statistic</strong> for the mean <span class="math inline">\(\mu\)</span>.</p>
</div>
</div>
<div id="Normal:SamplingModel:inference" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Inference: Federer’s time-to-serve</h3>
<p>What has one learned about Federer’s mean time-to-serve from this Bayesian analysis? Our prior said that any of the eight possible values of <span class="math inline">\(\mu\)</span> were equally likely with probability <span class="math inline">\(0.125\)</span>. After observing the sample of 20 measurements, one believes <span class="math inline">\(\mu\)</span> is most likely <span class="math inline">\(16\)</span>, 17, and <span class="math inline">\(18\)</span> seconds, with respective probabilities <span class="math inline">\(0.181, 0.425\)</span>, and <span class="math inline">\(0.299\)</span>. In fact, if one adds up the posterior probabilities, one says that <span class="math inline">\(\mu\)</span> is in the set {16, 17, 18} seconds with probability <span class="math inline">\(0.915\)</span>.
<span class="math display">\[\begin{eqnarray*}
 Prob(16 \leq \mu \leq 18) = 0.181 + 0.435 + 0.299 = 0.915
\end{eqnarray*}\]</span>
This region of values of <span class="math inline">\(\mu\)</span> is called a <span class="math inline">\(91.5\%\)</span> posterior probability region for the mean time-to-serve <span class="math inline">\(\mu\)</span>.</p>
</div>
</div>
<div id="Normal:Continuous" class="section level2">
<h2><span class="header-section-number">8.4</span> Continuous Priors</h2>
<div id="Normal:Continuous:prior" class="section level3">
<h3><span class="header-section-number">8.4.1</span> The Normal prior for mean <span class="math inline">\(\mu\)</span></h3>
<p>Returning to our example, one is interested in learning about the time-to-serve for the tennis player Roger Federer. His serving times are believed to be Normally distributed with unknown mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma = 4\)</span>. The focus is on learning about the mean value <span class="math inline">\(\mu\)</span>.</p>
<p>In the prior construction in Section 8.3, we assumed <span class="math inline">\(\mu\)</span> was discrete, taking only integer values from <span class="math inline">\(15\)</span> to <span class="math inline">\(22\)</span>. However, the mean time-to-serve <span class="math inline">\(\mu\)</span> does not have to be an integer. In fact, it is more realistic to assume <span class="math inline">\(\mu\)</span> is continuous-valued. One widely-used approach for representing one’s belief about a Normal mean is based on a Normal prior density with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\sigma_0\)</span>, that is
<span class="math display">\[\begin{eqnarray*}
\mu \sim {\rm{Normal}}(\mu_0, \sigma_0).
\end{eqnarray*}\]</span></p>
<p>There are two parameters for this Normal prior: the value <span class="math inline">\(\mu_0\)</span> represents one’s “best guess” at the mean time-to-serve <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma_0\)</span> indicates how sure one thinks about the guess.</p>
<p>To illustrate the use of different priors for <span class="math inline">\(\mu\)</span>, let’s consider the opinion of one tennis fan Joe who has strong prior information about the mean. His best guess at Federer’s mean time-to-serve is 18 seconds so he lets <span class="math inline">\(\mu_0 = 18\)</span>. He is very sure of this guess and so he chooses <span class="math inline">\(\sigma_0\)</span> to be the relatively small value of <span class="math inline">\(0.4\)</span>.
In contrast, a second tennis fan Kate also thinks that Federer’s mean time-to-serve is 18 seconds, but does not have a strong belief in this guess and chooses the large value <span class="math inline">\(2\)</span> of the standard deviation <span class="math inline">\(\sigma_0\)</span>. Figure 8.4 shows these two Normal priors for the mean time-to-serve <span class="math inline">\(\mu\)</span>.</p>
<div class="figure"><span id="fig:unnamed-chunk-8"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-8-1.png" alt="Two priors for the Normal mean $\mu$." width="672" />
<p class="caption">
Figure 8.4: Two priors for the Normal mean <span class="math inline">\(\mu\)</span>.
</p>
</div>
<p>Both curves are symmetric and bell-shaped, centered at <span class="math inline">\(\mu_0\)</span> = 18. The main difference is the spread of the two curves: a Normal(8, 0.4) curve is much more concentrated around the mean <span class="math inline">\(\mu_0\)</span> = 18 compared to the Normal(8, 2) curve. Since the value of the probability density function at a point reflects the probability at that value, the Normal(8, 0.4) prior reflects the belief that the mean time to serve will most likely be around <span class="math inline">\(\mu_0\)</span> = 18 seconds, whereas the Normal(8, 2) prior indicates that the mean <span class="math inline">\(\mu\)</span> could be as small as 15 seconds and as large as 20 seconds.</p>
</div>
<div id="Normal:Continuous:choosing" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Choosing a Normal prior</h3>
<div id="informative-prior" class="section level4 unnumbered">
<h4>Informative prior</h4>
<p>How does one in practice choose a Normal prior for <span class="math inline">\(\mu\)</span> that reflects prior beliefs about the location of this parameter? One indirect strategy for choosing for selecting values of the prior parameters <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\sigma_0\)</span> is based on the specification of quantiles. On the basis of one’s prior beliefs, one specifies two quantiles of the Normal density. Then the Normal parameters are found by matching these two quantiles to a particular Normal curve.</p>
<p>Recall the definition of a quantile — in this setting it is a value of the mean <span class="math inline">\(\mu\)</span> such that the probability of being smaller than that value is a given probability. To construct one’s prior for Federer’s mean time-to-serve, one thinks first about two quantiles. Suppose one specifies the 0.5 quantile to be 18 seconds — this means that <span class="math inline">\(\mu\)</span> is equally likely to be smaller or larger than 18 seconds. Next, one decides that the 0.9 quantile is 20 seconds. This means that one’s probability that <span class="math inline">\(\mu\)</span> is smaller than 20 seconds is 90%. Given values of these two quantiles, the unique Normal curve is found that matches this information.</p>
<p>The matching is performed by the R function <code>normal.select()</code>. One inputs two quantiles by <code>list</code> statements, and the output is the mean and standard deviation of the Normal prior.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="mean.html#cb4-1"></a><span class="kw">normal.select</span>(<span class="kw">list</span>(<span class="dt">p =</span> <span class="fl">0.5</span>, <span class="dt">x =</span> <span class="dv">18</span>), <span class="kw">list</span>(<span class="dt">p =</span> <span class="fl">0.9</span>, <span class="dt">x =</span> <span class="dv">20</span>))</span></code></pre></div>
<pre><code>## $mu
## [1] 18
## 
## $sigma
## [1] 1.560608</code></pre>
<p>The Normal curve with mean <span class="math inline">\(\mu_0 = 18\)</span> and <span class="math inline">\(\sigma_0 = 1.56\)</span>, displayed in Figure 8.5, matches the prior information stated by the two quantiles.</p>
<div class="figure"><span id="fig:unnamed-chunk-10"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-10-1.png" alt="A person's Normal prior for Federer's mean time-to-serve $\mu$." width="672" />
<p class="caption">
Figure 8.5: A person’s Normal prior for Federer’s mean time-to-serve <span class="math inline">\(\mu\)</span>.
</p>
</div>
<p>Since our measurement skills are limited, this prior is just an approximation to one’s beliefs about <span class="math inline">\(\mu\)</span>. We recommend in practice that one perform several checks to see if this Normal prior makes sense. Several functions are available to help in this prior checking.</p>
<p>For example, one finds the 0.25 quantile of our prior using the <code>qnorm()</code> function.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="mean.html#cb6-1"></a><span class="kw">qnorm</span>(<span class="fl">0.25</span>, <span class="dv">18</span>, <span class="fl">1.56</span>)</span></code></pre></div>
<pre><code>## [1] 16.9478</code></pre>
<p>This prior says that the prior probability that <span class="math inline">\(\mu\)</span> is smaller than 16.95 is 25%. If this does not seem reasonable, one would make adjustments in the values of the Normal mean and standard deviation until a reasonable Normal prior is found.</p>
</div>
<div id="weekly-informative-prior" class="section level4 unnumbered">
<h4>Weekly informative prior</h4>
<p>We have been assuming that one has some information about the mean parameter <span class="math inline">\(\mu\)</span> that is represented by a Normal prior. What would a user do in the situation where little is known about the location on <span class="math inline">\(\mu\)</span>? For a Normal prior, the standard deviation <span class="math inline">\(\sigma_0\)</span> represents the sureness of one’s belief in one’s guess <span class="math inline">\(\mu_0\)</span> at the value of the mean. If one is really unsure about any guess at <span class="math inline">\(\mu\)</span>, then one assigns the standard deviation <span class="math inline">\(\sigma_0\)</span> a large value. Then the choice of the prior mean will not matter, so we suggest using a Normal(0, <span class="math inline">\(\sigma_0\)</span>) with a large value for <span class="math inline">\(\sigma_0\)</span>.
This prior indicates that <span class="math inline">\(\mu\)</span> may plausibly range over a large interval and represents weakly informative prior belief about the parameter.</p>
<p>As will be seen later in this chapter, when a vague prior is chosen, the posterior inference for <span class="math inline">\(\mu\)</span> will largely be driven by the data. This behavior is desirable since this person knows little about the location of <span class="math inline">\(\mu\)</span> <em>a priori</em> in this situation and wants the data to inform about the location of <span class="math inline">\(\mu\)</span> with little influence by the prior.</p>
</div>
</div>
</div>
<div id="Normal:ContinuousUpdate" class="section level2">
<h2><span class="header-section-number">8.5</span> Updating the Normal Prior</h2>
<div id="introduction-1" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Introduction</h3>
<p>Continuing our discussion on learning about the mean time-to-serve for Roger Federer, the current prior beliefs about Federer’s mean time-to-serve <span class="math inline">\(\mu\)</span> are represented by a Normal curve with mean <span class="math inline">\(18\)</span> seconds and standard deviation 1.56 seconds.</p>
<p>Next some data is collected — Federer’s time-to-serves are recorded for 20 serves and the sample mean is <span class="math inline">\(17.2\)</span> seconds. Recall that we are assuming the population standard deviation <span class="math inline">\(\sigma = 4\)</span> seconds. The likelihood is given by
<span class="math display" id="eq:update1">\[\begin{equation}
L(\mu) \propto \exp \left\{-\frac{n}{2 \sigma^2}(\bar{y} - \mu)^2\right\},
\tag{8.12}
\end{equation}\]</span>
and with substitution of the values <span class="math inline">\(\bar y = 17.2\)</span>, <span class="math inline">\(n = 20\)</span>, and <span class="math inline">\(\sigma = 4\)</span>, we obtain
<span class="math display" id="eq:NormalSampling">\[\begin{eqnarray}
L(\mu) &amp;\propto&amp; \exp \left\{-\frac{20}{2 (4)^2}(17.2 - \mu)^2\right\} \nonumber \\
&amp;=&amp; \exp \left\{-\frac{1}{2(4/\sqrt{20})^2}(\mu - 17.2)^2\right\}.
\tag{8.13}
\end{eqnarray}\]</span>
Viewing the likelihood as a function of the parameter <span class="math inline">\(\mu\)</span> as in Equation (8.13), the likelihood is recognized as a Normal density with mean <span class="math inline">\(\bar y = 17.2\)</span> and standard deviation <span class="math inline">\(\sigma / \sqrt{n} = 4 / \sqrt{20} = 0.89\)</span>.</p>
<p>The Bayes’ rule calculation is very familiar to the reader — one obtains the posterior density curve by multiplying the Normal prior by the likelihood. If one writes down the product of the Normal likelihood and the Normal prior density and works through some messy algebra, one will discover that the posterior density also has the Normal density form.</p>
<p>The Normal prior is said to be <em>conjugate</em> since the prior and posterior densities come from the same distribution family: Normal. To be more specific, suppose the observation has a Normal sampling density with unknown mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma\)</span>. If one specifies a Normal prior for the unknown mean <span class="math inline">\(\mu\)</span> with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\sigma_0\)</span>, one obtains a Normal posterior for <span class="math inline">\(\mu\)</span> with updated parameters <span class="math inline">\(\mu_n\)</span> and <span class="math inline">\(\sigma_n\)</span>.</p>
<p>In Section 8.5.2, we provide a quick peak at this posterior updating without worrying about the mathematical derivation and Section 8.5.3 describes the details of the Bayes’ rule calculation. Section 8.5.4 looks at the conjugacy more closely and provides some insight on the effects of prior and likelihood on the posterior distribution.</p>
</div>
<div id="Normal:ContinuousUpdate:Overview" class="section level3">
<h3><span class="header-section-number">8.5.2</span> A quick peak at the update procedure</h3>
<p>It is convenient to describe the updating procedure by use of a table.
In Table 8.3, there are rows corresponding to Prior, Data/Likelihood, and Posterior and columns corresponding to Mean, Precision, and Standard Deviation. The mean and standard deviation of the Normal prior are placed in the “Prior” row, and the sample mean and standard error are placed in the “Data/Likelihood” row.</p>
<p>Table 8.3. Updating the Normal prior: step 1.</p>
<table>
<thead>
<tr class="header">
<th align="left">Type</th>
<th align="right">Mean</th>
<th align="right">Precision</th>
<th align="right">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Prior</td>
<td align="right">18.00</td>
<td align="right"></td>
<td align="right">1.56</td>
</tr>
<tr class="even">
<td align="left">Data/Likelihood</td>
<td align="right">17.20</td>
<td align="right"></td>
<td align="right">0.89</td>
</tr>
<tr class="odd">
<td align="left">Posterior</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>We define the <em>precision</em>, <span class="math inline">\(\phi\)</span>, to be the reciprocal of the square of the standard deviation. We
compute the precisions of the prior and data from the given standard deviations:
<span class="math display">\[\begin{equation*}
\phi_{prior} = \frac{1}{\sigma_0^2} = \frac{1}{1.56^2} = 0.41, \, \, \,
\phi_{data} = \frac{1}{\sigma^2 / n} = \frac{1}{0.89^2} = 1.26.
\end{equation*}\]</span>
We enter the precisions in the corresponding rows of Table 8.4 .</p>
<p>Table 8.4. Updating the Normal prior: step 2.</p>
<table>
<thead>
<tr class="header">
<th align="left">Type</th>
<th align="right">Mean</th>
<th align="right">Precision</th>
<th align="right">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Prior</td>
<td align="right">18.00</td>
<td align="right">0.41</td>
<td align="right">1.56</td>
</tr>
<tr class="even">
<td align="left">Data/Likelihood</td>
<td align="right">17.20</td>
<td align="right">1.26</td>
<td align="right">0.89</td>
</tr>
<tr class="odd">
<td align="left">Posterior</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>We will shortly see that the Posterior precision is the sum of the Prior precision and the Data/Likelihood precisions:<br />
<span class="math display">\[\begin{equation*}
\phi_{post} = \phi_{prior} + \phi_{data} = 0.41 + 1.26 = 1.67.
\end{equation*}\]</span>
Once the posterior precision is computed, the posterior standard deviation is computed as the reciprocal of the square root of the precision.
<span class="math display">\[\begin{equation*}
\sigma_n = \frac{1}{\sqrt{\phi_{post}}} = \frac{1}{\sqrt{1.67}} = 0.77.
\end{equation*}\]</span>
These precisions and standard deviations are entered into Table 8.5.</p>
<p>Table 8.5. Updating the Normal prior: step 3.</p>
<table>
<thead>
<tr class="header">
<th align="left">Type</th>
<th align="right">Mean</th>
<th align="right">Precision</th>
<th align="right">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Prior</td>
<td align="right">18.00</td>
<td align="right">0.41</td>
<td align="right">1.56</td>
</tr>
<tr class="even">
<td align="left">Data/Likelihood</td>
<td align="right">17.20</td>
<td align="right">1.26</td>
<td align="right">0.89</td>
</tr>
<tr class="odd">
<td align="left">Posterior</td>
<td align="right"></td>
<td align="right">1.67</td>
<td align="right">0.77</td>
</tr>
</tbody>
</table>
<p>The posterior mean is a weighted average of the Prior and Data/Likelihood means where the weights are given by the corresponding precisions. That is, the formula is given by
<span class="math display" id="eq:update2">\[\begin{eqnarray}
\mu_n = \frac{\phi_{prior} \times \mu_0 + \phi_{data} \times \bar y}{\phi_{prior} + \phi_{data}}.
\tag{8.14}
\end{eqnarray}\]</span>
By making appropriate substitutions, we obtain the posterior mean:
<span class="math display">\[\begin{eqnarray*}
\mu_n = \frac{0.41 \times 18.00 + 1.26 \times 17.20}{0.41 + 1.26} = 17.40.
\end{eqnarray*}\]</span>
The posterior density is Normal with mean <span class="math inline">\(17.40\)</span> seconds and standard deviation <span class="math inline">\(0.77\)</span> seconds. See Table 8.6 for the final update step.</p>
<p>Table 8.6. Updating the Normal prior: step 4.</p>
<table>
<thead>
<tr class="header">
<th align="left">Type</th>
<th align="right">Mean</th>
<th align="right">Precision</th>
<th align="right">Stand</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Prior</td>
<td align="right">18.00</td>
<td align="right">0.41</td>
<td align="right">1.56</td>
</tr>
<tr class="even">
<td align="left">Data/Likelihood</td>
<td align="right">17.20</td>
<td align="right">1.26</td>
<td align="right">0.89</td>
</tr>
<tr class="odd">
<td align="left">Posterior</td>
<td align="right">17.40</td>
<td align="right">1.67</td>
<td align="right">0.77</td>
</tr>
</tbody>
</table>
<p>The Normal updating is performed by the R function <code>normal_update()</code>. One inputs two vectors – <code>prior</code> is a vector of the prior mean and standard deviation and <code>data</code> is a vector of the sample mean and standard error. The output is a vector of the posterior mean and posterior standard deviation.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="mean.html#cb8-1"></a>prior &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">18</span>, <span class="fl">1.56</span>)</span>
<span id="cb8-2"><a href="mean.html#cb8-2"></a>data &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">17.20</span>, <span class="fl">0.89</span>)</span>
<span id="cb8-3"><a href="mean.html#cb8-3"></a><span class="kw">normal_update</span>(prior, data)</span></code></pre></div>
<pre><code>## [1] 17.3964473  0.7730412</code></pre>
<p>The prior and posterior densities are displayed in Figure 8.6. As usually the case, the posterior density has a smaller spread since the posterior has more information than the prior about Federer’s mean time-to-serve. More information about a parameter indicates less uncertainty and a smaller spread of the posterior density. In the process from prior to posterior, one sees how the data modifies one’s initial belief about the parameter <span class="math inline">\(\mu\)</span>.</p>
<div class="figure"><span id="fig:unnamed-chunk-13"></span>
<img src="LATEX/figures/chapter8/normalpriorpost.png" alt="Prior and posterior curves for Federer's mean time-to-serve $\mu$." width="500" />
<p class="caption">
Figure 8.6: Prior and posterior curves for Federer’s mean time-to-serve <span class="math inline">\(\mu\)</span>.
</p>
</div>
</div>
<div id="Normal:ContinuousUpdate:BayesRule" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Bayes’ rule calculation</h3>
<p>Section 8.5.2 gave an overview of the updating procedure for a Normal prior and Normal sampling. In this section we explain (1) why it is preferable to work with the precisions instead of the standard deviations; (2) why the precisions act as the weights in the calculation of the posterior mean and (3) why the posterior is a Normal distribution.</p>
<p>Recall a precision is the reciprocal of the square of the standard deviation. We use <span class="math inline">\(\phi = \frac{1}{\sigma^2}\)</span> to represent the precision of a single observation in the Normal data/likelihood, and <span class="math inline">\(\phi_0 = \frac{1}{\sigma_0^2}\)</span> to represent the precision in the Normal prior.</p>
<ul>
<li>We write down the likelihood of <span class="math inline">\(\mu\)</span>, combining terms, and writing the expression in terms of the precision <span class="math inline">\(\phi\)</span>.
<span class="math display" id="eq:update3">\[\begin{eqnarray}
y_1, \cdots, y_n \mid \mu, \sigma &amp;\overset{i.i.d.}{\sim}&amp; {\rm{Normal}}(\mu, \sigma)\\% \equiv {\rm{Normal}}(\mu, \frac{1}{\phi}) \\ 
\tag{8.15}
\end{eqnarray}\]</span>
<span class="math display" id="eq:update4">\[\begin{eqnarray}
L(\mu) = f(y_1, \cdots, y_n \mid \mu, \sigma) &amp;=&amp; \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(y_i-\mu)^2\right\} \nonumber \\
&amp;=&amp; \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}}\phi^{\frac{1}{2}} \exp\left\{-\frac{\phi}{2}(y_i - \mu)^2)\right\}\nonumber \\
&amp;=&amp; \left(\frac{1}{\sqrt{2\pi}}\right)^n \phi^{\frac{n}{2}} \exp\left\{-\frac{\phi}{2}\sum_{i=1}^{n}(y_i - \mu)^2)\right\}\nonumber \\
\tag{8.16}
\end{eqnarray}\]</span></li>
</ul>
<p>Note that <span class="math inline">\(\sigma\)</span> is assumed known, therefore the likelihood function is only in terms of <span class="math inline">\(\mu\)</span>, i.e. <span class="math inline">\(L(\mu)\)</span>.</p>
<ul>
<li><p>In similar fashion, we write down the prior density for <span class="math inline">\(\mu\)</span> including the prior precision <span class="math inline">\(\phi_0\)</span>.
<span class="math display" id="eq:update5">\[\begin{eqnarray}
\mu  &amp;\sim&amp; {\rm{Normal}}(\mu_0, \sigma_0) \\%\equiv {\rm{Normal}}(\mu_0, \frac{1}{\phi_0}) \\
\tag{8.17}
\end{eqnarray}\]</span>
<span class="math display" id="eq:update6">\[\begin{eqnarray}
\pi(\mu) &amp;=&amp; \frac{1}{\sqrt{2\pi}\sigma_0} \exp\left\{-\frac{1}{2\sigma_0^2}(\mu - \mu_0)^2)\right\}\nonumber \\
&amp;=&amp; \frac{1}{\sqrt{2\pi}}\phi_0^{\frac{1}{2}} \exp\left\{-\frac{\phi_0}{2}(\mu - \mu_0)^2\right\}
\tag{8.18}
\end{eqnarray}\]</span></p></li>
<li><p>Bayes’ rule is applied by multiplying the prior by the likelihood to obtain the posterior. In deriving the posterior of <span class="math inline">\(\mu\)</span>, the manipulations require careful consideration regarding what is known. The only unknown variable is <span class="math inline">\(\mu\)</span>, so any “constants” or known quantities not depending on <span class="math inline">\(\mu\)</span> can be dropped/added with the proportionality sign “<span class="math inline">\(\propto\)</span>”.</p></li>
</ul>
<p><span class="math display" id="eq:normalDev2">\[\begin{eqnarray}
\pi(\mu \mid y_1, \cdots, y_n, \sigma) &amp;\propto&amp;  \pi(\mu) L(\mu) \nonumber \\
&amp;\propto&amp; \exp\left\{-\frac{\phi_0}{2}(\mu - \mu_0)^2\right\} \times \exp\left\{-\frac{n\phi}{2}(\mu - \bar{y})^2\right\} \nonumber \\
&amp;\propto&amp; \exp\left\{-\frac{1}{2}(\phi_0 +n\phi)\mu^2 + \frac{1}{2}(2\mu_0\phi_0 + 2n\phi\bar{y})\mu\right\} \nonumber \\
\texttt{[complete the square]} &amp;\propto&amp; \exp\left\{-\frac{1}{2}(\phi_0 + n\phi)(\mu - \frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi})^2\right\} \\
\tag{8.19}
\end{eqnarray}\]</span></p>
<p>Looking closely at the final expression, one recognizes that the posterior for <span class="math inline">\(\mu\)</span> is a Normal density with mean and precision parameters.
Specifically we recognize <span class="math inline">\((\phi_0 + n \phi)\)</span> as the posterior precision and <span class="math inline">\((\frac{\phi_0 \mu_0 + n \phi\bar{y}}{\phi_0 + n \phi})\)</span> as the posterior mean.
Summarizing, we have derived the following posterior distribution of <span class="math inline">\(\mu\)</span>,</p>
<p><span class="math display" id="eq:update7">\[\begin{eqnarray}
\mu \mid y_1, \cdots, y_n, \sigma \sim {\rm{Normal}}\left(\frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi}, \sqrt{\frac{1}{\phi_0 + n \phi}}\right).
\tag{8.20}
\end{eqnarray}\]</span></p>
<p>In passing, it should be noted that the same result would be attained using the standard deviations, <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\sigma_0\)</span>, instead of the precisions, <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\phi_0\)</span>. It is preferable to work with the precisions due to the relative simplicity of the notation. In particular, one sees in Table Table 8.5 that the posterior precision is the sum of the prior and data/likelihood precisions, that is, the posterior precision <span class="math inline">\(\phi_n = \phi_0 + n \phi\)</span>.</p>
</div>
<div id="Normal:ContinuousUpdate:Conjugate" class="section level3">
<h3><span class="header-section-number">8.5.4</span> Conjugate Normal prior</h3>
<p>Let’s summarize our calculations in Section 8.5.3.
We collect a sequence of continuous observations that are assumed identically and independently distributed as <span class="math inline">\(\textrm{Normal}(\mu, \sigma)\)</span>, and a Normal prior is assigned to the mean parameter <span class="math inline">\(\mu\)</span>.</p>
<ul>
<li><p>The sampling model:
<span class="math display" id="eq:conjugate1">\[\begin{eqnarray}
Y_1, \cdots, Y_n \mid \mu, \sigma &amp;\overset{i.i.d.}{\sim}&amp; {\rm{Normal}}(\mu, \sigma) 
\tag{8.21}
\end{eqnarray}\]</span>
When <span class="math inline">\(\sigma\)</span> (or <span class="math inline">\(\phi\)</span>) is known, and mean <span class="math inline">\(\mu\)</span> is the only parameter in the likelihood.</p></li>
<li><p>The prior distribution:
<span class="math display" id="eq:conjugate2">\[\begin{eqnarray}
\mu  &amp;\sim&amp; {\rm{Normal}}(\mu_0, \sigma_0) 
\tag{8.22}
\end{eqnarray}\]</span></p></li>
<li><p>After <span class="math inline">\(Y_1 = y_1, ..., Y_n = y_n\)</span> are observed, the posterior distribution for the mean <span class="math inline">\(\mu\)</span> is another Normal distribution with mean <span class="math inline">\(\frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi}\)</span> and precision <span class="math inline">\(\phi_0 + n \phi\)</span> (thus standard deviation <span class="math inline">\(\sqrt{\frac{1}{\phi_0 + n \phi}}\)</span>):</p></li>
</ul>
<p><span class="math display" id="eq:normalposterior">\[\begin{eqnarray}
\mu \mid y_1, \cdots, y_n, \sigma \sim {\rm{Normal}}\left(\frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi}, \sqrt{\frac{1}{\phi_0 + n \phi}}\right).
\tag{8.23}
\end{eqnarray}\]</span></p>
<p>In this situation where the sampling standard deviation <span class="math inline">\(\sigma\)</span> is known, the Normal density is a conjugate prior for the mean of a Normal distribution, as the posterior distribution for <span class="math inline">\(\mu\)</span> is another Normal density with updated parameters. Conjugacy is a convenient property as the posterior distribution for <span class="math inline">\(\mu\)</span> has a convenient functional form. Conjugacy allows one to conduct Bayesian inference through exact analytical solutions and simulation. Also conjugacy provides insight on how the data and prior are combined in the posterior distribution.</p>
<div id="the-posterior-compromises-between-the-prior-and-the-sample" class="section level4 unnumbered">
<h4>The posterior compromises between the prior and the sample</h4>
<p>Recall that Bayesian inference is a general approach where one initializes a prior belief for an unknown quantity, collects data expressed through a likelihood function, and combines prior and likelihood to give an updated belief for the unknown quantity. In Chapter 7, we have seen how the posterior mean of a proportion is a compromise between the prior mean and sample proportion (refer to Section 7.4.2 as needed).
In the current Normal mean case, the posterior mean is similarly viewed as an estimate that compromises between the prior mean and sample mean. One rewrites the posterior mean in Equation (8.23) as follows:
<span class="math display" id="eq:conjugate3">\[\begin{eqnarray}
\mu_n = \frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi} &amp;=&amp; \frac{\phi_0}{\phi_0 + n\phi} \mu_0 +  
\frac{n\phi}{\phi_0 + n\phi}  \bar{y}.
\tag{8.24}
\end{eqnarray}\]</span>
The prior precision is equal to <span class="math inline">\(\phi_0\)</span> and the precision in the likelihood for any <span class="math inline">\(y_i\)</span> is <span class="math inline">\(\phi\)</span>. Since there are <span class="math inline">\(n\)</span> observations, the precision in the joint likelihood is <span class="math inline">\(n\phi\)</span>. The posterior mean is a weighted average of the prior mean <span class="math inline">\(\mu_0\)</span> and sample mean <span class="math inline">\(\bar y\)</span> where the weights are proportional to the associated precisions.</p>
</div>
<div id="the-posterior-accumulates-information-in-the-prior-and-the-sample" class="section level4 unnumbered">
<h4>The posterior accumulates information in the prior and the sample</h4>
<p>In addition, the precision of the posterior Normal mean is the sum of the precisions of the prior and likelihood. That is,
<span class="math display" id="eq:conjugate4">\[\begin{equation}
 \phi_n = \phi_0 + n \phi.
 \tag{8.25}
 \end{equation}\]</span>
The implication is that the posterior standard deviation will always be smaller than either the prior standard deviation or the sampling standard error:
<span class="math display">\[\begin{equation*}
 \sigma_n &lt; \sigma_0, \, \, \, \sigma_n &lt; \frac{\sigma}{\sqrt{n}}.
 \end{equation*}\]</span></p>
</div>
</div>
</div>
<div id="Normal:ContinuousInference" class="section level2">
<h2><span class="header-section-number">8.6</span> Bayesian Inferences for Continuous Normal Mean</h2>
<p>Continuing with the example about Federer’s time-to-serve, our Normal prior had mean 18 seconds and standard deviation 1.56 seconds. After collecting 20 time-to-serve measurements with a sample mean of 17.2, the posterior distribution <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span> reflects our opinion about the mean time-to-serve.</p>
<p>Bayesian inferences about the mean <span class="math inline">\(\mu\)</span> are based on various summaries of this posterior Normal distribution. Because the exact posterior distribution of mean <span class="math inline">\(\mu\)</span> is Normal, it is convenient to use R functions such as <code>pnorm()</code> and <code>qnorm()</code> to conduct Bayesian hypothesis testing and construct Bayesian credible intervals. Simulation-based methods utilizing functions such as <code>rnorm()</code> are also useful to provide approximations to those inferences. A sequence of examples are given in Section 8.6.1.</p>
<p>Predictions of future data are also of interest. For example, one might want to predict the next time-to-serve measurement based on the posterior distribution of <span class="math inline">\(\mu\)</span> being <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span>. In Section 8.6.2, details of the prediction procedure and examples are provided.</p>
<div id="Normal:ContinuousInference:HTandCI" class="section level3">
<h3><span class="header-section-number">8.6.1</span> Bayesian hypothesis testing and credible interval</h3>
<div id="a-testing-problem" class="section level4 unnumbered">
<h4>A testing problem</h4>
<p>In a <em>testing</em> problem, one is interested in checking the validity of a statement about a population quantity. In our tennis example, suppose someone says that Federer takes on average at least 19 seconds to serve. Is this a reasonable statement?</p>
<p>The current beliefs about Federer’s mean time-to-serve are summarized by a Normal distribution with mean 17.4 seconds and standard deviation 0.77 seconds. To assess if the statement "<span class="math inline">\(\mu\)</span> is 19 seconds or more" is reasonable, one simply computes its posterior probability, <span class="math inline">\(Prob(\mu \geq 19 \mid \mu_n = 17.4, \sigma_n = 0.77)\)</span>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="mean.html#cb10-1"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">19</span>, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span></code></pre></div>
<pre><code>## [1] 0.01885827</code></pre>
<p>This probability is about 0.019, a small value, so one would conclude that this person’s statement is unlikely to be true.</p>
<p>This is the exact solution using the <code>pnorm()</code> function with mean 17.4 and standard deviation 0.77. As seen in Chapter 7, simulation provides an alternative approach to obtaining the probability <span class="math inline">\(Prob(\mu \geq 19 \mid \mu_n = 17.4, \sigma_n = 0.77)\)</span>.
To implement the simulation approach, recall that one generates a large number of values from the posterior distribution and summarizes this simulated sample. In particular, using the following R script, one generates 1000 values from the <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span> distribution and approximates the probability of “<span class="math inline">\(\mu\)</span> is 19 seconds or more” by computing the percentage of values that falls above 19.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="mean.html#cb12-1"></a>S &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb12-2"><a href="mean.html#cb12-2"></a>NormalSamples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span>
<span id="cb12-3"><a href="mean.html#cb12-3"></a><span class="kw">sum</span>(NormalSamples <span class="op">&gt;=</span><span class="st"> </span><span class="dv">19</span>) <span class="op">/</span><span class="st"> </span>S</span></code></pre></div>
<pre><code>## [1] 0.024</code></pre>
<p>The reader might notice that the approximated value of 0.024 differs from the exact answer of 0.019 using the <code>pnorm()</code> function. One way to improve the accuracy of the approximation is by increasing the number of simulated values. For example, increasing S from 1000 to 10,000 provides a better approximation to the exact probability 0.019.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="mean.html#cb14-1"></a>S &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb14-2"><a href="mean.html#cb14-2"></a>NormalSamples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span>
<span id="cb14-3"><a href="mean.html#cb14-3"></a><span class="kw">sum</span>(NormalSamples <span class="op">&gt;=</span><span class="st"> </span><span class="dv">19</span>) <span class="op">/</span><span class="st"> </span>S</span></code></pre></div>
<pre><code>## [1] 0.0175</code></pre>
</div>
<div id="a-bayesian-interval-estimate" class="section level4 unnumbered">
<h4>A Bayesian interval estimate</h4>
<p>Bayesian credible intervals for the mean parameter <span class="math inline">\(\mu\)</span> can be achieved both by exact calculation and simulation. Recall that a Bayesian credible interval is an interval that contains the unknown parameter with a certain probability content. For example, a 90% Bayesian credible interval for the parameter <span class="math inline">\(\mu\)</span> is an interval containing <span class="math inline">\(\mu\)</span> with a probability of 0.90.</p>
<p>The exact interval is obtained by using the R function <code>qnorm()</code>. For example, with the posterior distribution for <span class="math inline">\(\mu\)</span> being <span class="math inline">\(\textrm{Normal}(17.4, 0.77)\)</span>, the following R script shows that a 90% central Bayesian credible interval is (16.133, 18.667). That is, the posterior probability of <span class="math inline">\(\mu\)</span> falls between 16.133 and 18.667 is exactly 90%.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="mean.html#cb16-1"></a><span class="kw">qnorm</span>(<span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>), <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span></code></pre></div>
<pre><code>## [1] 16.13346 18.66654</code></pre>
<p>For simulation-based inference, one generates a large number of values from its posterior distribution, then finds the 5th and 95th sample quantiles to obtain the middle 90% of the generated values. Below one sees that a 90% credible interval for posterior of <span class="math inline">\(\mu\)</span> is approximately (16.151, 18.691).</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="mean.html#cb18-1"></a>S &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb18-2"><a href="mean.html#cb18-2"></a>NormalSamples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span>
<span id="cb18-3"><a href="mean.html#cb18-3"></a><span class="kw">quantile</span>(NormalSamples, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span></code></pre></div>
<pre><code>##       5%      95% 
## 16.15061 18.69062</code></pre>
<p>The Bayesian credible intervals can also be used for testing hypothesis. Suppose one again wants to evaluate the statement “Federer takes on average at least 19 seconds to serve.” One answers this question by computing the 90% credible interval. One notes that the values of <span class="math inline">\(\mu\)</span> "at least 19" are not included in the exact 90% credible interval (16.15, 18.69). The interpretation is that the probability is at least 0.90 that Federer’s average time-to-service is smaller than 19 seconds. One could obtain a wider credible interval, say by computing a central 95% credible interval (see the R output below), and observe that 19 is out of the interval. This indicates we are 95% confident that 19 seconds is not the value of Federer’s average time-to-serve.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="mean.html#cb20-1"></a><span class="kw">qnorm</span>(<span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>), <span class="fl">17.4</span>, <span class="fl">0.77</span>)</span></code></pre></div>
<pre><code>## [1] 15.89083 18.90917</code></pre>
<p>On the basis of this credible interval calculation, one concludes that the statement about Federer’s time-to-serve is unlikely to be true. This conclusion is consistent with the typical Bayesian hypothesis testing procedure given at the beginning of this section.</p>
</div>
</div>
<div id="Normal:ContinuousInference:Prediction" class="section level3">
<h3><span class="header-section-number">8.6.2</span> Bayesian prediction</h3>
<p>Suppose one is interested in predicting Federer’s future time-to-serve. Since one has already updated the belief about the parameter, the mean <span class="math inline">\(\mu\)</span>, the prediction is made based on its posterior predictive distribution.</p>
<p>How to make one future prediction of Federer’s time-to-serve? In Chapter 7, we have seen two different approaches for predicting of a new survey outcome of students’ dining preferences. One approach in Chapter 7 is based on the derivation of the exact posterior predictive distribution <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid Y = y)\)</span> which was shown to be a Beta-Binomial distribution. The second approach is a simulation-based approach, which involves two steps: first, sample a value of the parameter from its posterior distribution (a Beta distribution), and second, sample a prediction from the data model based on the sampled parameter draw (a Binomial distribution). When the sample size in the simulation-based approach is sufficiently large, a prediction interval from the simulation-based approach is an accurate approximation to the exact prediction interval.</p>
<div id="exact-predictive-distribution" class="section level4 unnumbered">
<h4>Exact predictive distribution</h4>
<p>We first describe the exact posterior predictive distribution. Consider making a prediction of a single Federer’s time-to-serve <span class="math inline">\(\tilde{Y}\)</span>. In general, suppose the sampling density of <span class="math inline">\(\tilde{Y}\)</span> given <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> is <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid \mu)\)</span> and suppose the current beliefs about <span class="math inline">\(\mu\)</span> are represented by the density <span class="math inline">\(\pi(\mu)\)</span>. The joint density of <span class="math inline">\((\tilde{y}, \mu)\)</span> is given by the product
<span class="math display" id="eq:pred1">\[\begin{equation}
f(\tilde{Y} = \tilde{y}, \mu) = f(\tilde{Y} = \tilde{y} \mid \mu) \pi(\mu),
\tag{8.26}
\end{equation}\]</span>
and by integrating out <span class="math inline">\(\mu\)</span>, the predictive density of <span class="math inline">\(\tilde{Y}\)</span> is given by
<span class="math display" id="eq:pred2">\[\begin{equation}
f(\tilde{Y} = \tilde{y}) = \int f(\tilde{Y} = \tilde{y} \mid \mu) \pi(\mu) d\mu.
\tag{8.27}
\end{equation}\]</span></p>
<p>The computation of the predictive density is possible for this Normal sampling model with a Normal prior. It is assumed that <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid \mu)\)</span> is Normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> and that the current beliefs about <span class="math inline">\(\mu\)</span> are described by a Normal density with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\sigma_0\)</span>. Then it is possible to integrate out <span class="math inline">\(\mu\)</span> from the joint density of <span class="math inline">\((\tilde{y}, \mu)\)</span> and one finds that the predictive density for <span class="math inline">\(\tilde{Y}\)</span> is Normal with mean and standard deviation given by
<span class="math display" id="eq:NormalResults">\[\begin{equation}
E(\tilde{Y}) = \mu_0, \, \, SD(\tilde{Y}) = \sqrt{\sigma^2 + \sigma_0^2}.
\tag{8.28}
\end{equation}\]</span></p>
<p>This result can be used to derive the posterior predictive distribution of <span class="math inline">\(f(\tilde{Y} = \tilde{y} \mid Y_1, \cdots, Y_n)\)</span>, where <span class="math inline">\(\tilde{Y}\)</span> is a future observation and <span class="math inline">\(Y_1, \cdots, Y_n\)</span> are <span class="math inline">\(n\)</span> <span class="math inline">\(i.i.d.\)</span> observations from a Normal sampling density with unknown mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma\)</span>. After observing the sample values <span class="math inline">\(y_1, \cdots, y_n\)</span>, the current beliefs about the mean <span class="math inline">\(\mu\)</span> are represented by a Normal<span class="math inline">\((\mu_n, \sigma_n)\)</span> density, where the mean and standard deviation are given by
<span class="math display" id="eq:pred3">\[\begin{equation}
\mu_n = \frac{\phi_0 \mu_0 + n\phi\bar{y} }{\phi_0 + n \phi},  \sigma_n = \sqrt{\frac{1}{\phi_0 + n \phi}}.
\tag{8.29}
\end{equation}\]</span>
Then by applying our general result in Equation (8.28), the posterior predictive density of the single future observation <span class="math inline">\(\tilde{Y}\)</span> is Normal with mean <span class="math inline">\(\mu_n\)</span> and standard deviation <span class="math inline">\(\sqrt{\sigma^2 + \sigma_n^2}.\)</span> That is,
<span class="math display" id="eq:pred4">\[\begin{eqnarray}
\tilde{Y} = \tilde{y} \mid y_1, \cdots, y_n, \sigma \sim \textrm{Normal}(\mu_n, \sqrt{\sigma^2 + \sigma_n^2}).
\tag{8.30}
\end{eqnarray}\]</span></p>
<p>An important aspect of the predictive distribution for <span class="math inline">\(\tilde{Y}\)</span> is on the variance term <span class="math inline">\(\sigma^2 + \sigma_n^2\)</span>. The variability of a future prediction comes from two sources: (1) the data model variance <span class="math inline">\(\sigma^2\)</span>, and (2) the posterior variance <span class="math inline">\(\sigma_n^2\)</span>. Recall that the posterior variance <span class="math inline">\(\sigma_n^2 = \frac{1}{\phi_0 + n\phi}\)</span>. If one fixes values of <span class="math inline">\(\phi_0\)</span> and <span class="math inline">\(\phi\)</span> and allow the sample size <span class="math inline">\(n\)</span> to grow, the posterior variance will approach zero. In this "large <span class="math inline">\(n\)</span>" case, the uncertainty in inference about the population mean <span class="math inline">\(\mu\)</span> will decrease – essentially we are certain about the location of <span class="math inline">\(\mu\)</span>.
However the uncertainty in prediction will not decrease towards zero. In contrast, in this large sample case, the variance of <span class="math inline">\(\tilde{Y}\)</span> will decrease and approach the sampling variance <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="predictions-by-simulation" class="section level4 unnumbered">
<h4>Predictions by simulation</h4>
<p>The alternative method of computing the predictive distribution is by simulation. In this setting, there are two unknowns – the mean parameter <span class="math inline">\(\mu\)</span> and the future observation <span class="math inline">\(\tilde Y\)</span>. One simulates a value from the predictive distribution in two steps: first, one simulates a value of the parameter <span class="math inline">\(\mu\)</span> from its posterior distribution; second, use this simulated parameter draw to simulate a future observation <span class="math inline">\(\tilde Y\)</span> from the data model. In particular, the following algorithm is used to simulate a single value from the posterior predictive distribution.</p>
<ol style="list-style-type: decimal">
<li><p>Sample a value of <span class="math inline">\(\mu\)</span> from its posterior distribution
<span class="math display" id="eq:predictionstep1">\[\begin{eqnarray}
\mu \sim \textrm{Normal}\left(\frac{\phi_0\mu_0 + n\phi\bar{y}}{\phi_0 + n\phi}, \sqrt{\frac{1}{\phi_0 + n\phi}}\right),
\tag{8.31}
\end{eqnarray}\]</span></p></li>
<li><p>Sample a new observation <span class="math inline">\(\tilde{Y}\)</span> from the data model (i.e. a prediction)
<span class="math display" id="eq:predictionstep2">\[\begin{eqnarray}
\tilde{Y} \sim \textrm{Normal}(\mu, \sigma).
\tag{8.32}
\end{eqnarray}\]</span></p></li>
</ol>
<p>This two-step procedure is implemented for our time-to-serve example using the following R script.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="mean.html#cb22-1"></a>sigma &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb22-2"><a href="mean.html#cb22-2"></a>mu_n &lt;-<span class="st"> </span><span class="fl">17.4</span></span>
<span id="cb22-3"><a href="mean.html#cb22-3"></a>sigma_n &lt;-<span class="st"> </span><span class="fl">0.77</span></span>
<span id="cb22-4"><a href="mean.html#cb22-4"></a>pred_mu_sim &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, mu_n, sigma_n)</span>
<span id="cb22-5"><a href="mean.html#cb22-5"></a>(pred_y_sim &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, pred_mu_sim, sigma))</span></code></pre></div>
<pre><code>## [1] 16.04772</code></pre>
<p>The script can easily be updated to create <span class="math inline">\(S\)</span> = 1000 predictions, which is helpful to make summary about predictions.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="mean.html#cb24-1"></a>S &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb24-2"><a href="mean.html#cb24-2"></a>pred_mu_sim &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, mu_n, sigma_n)</span>
<span id="cb24-3"><a href="mean.html#cb24-3"></a>pred_y_sim &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, pred_mu_sim, sigma)</span></code></pre></div>
<p>The vector <code>pred_y_sim</code> contains 1000 predictions of Federer’s time-to-serve.</p>
<div class="figure"><span id="fig:unnamed-chunk-27"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-27-1.png" alt="Display of the exact and simulated time-to-serve for Federer's example." width="672" />
<p class="caption">
Figure 8.7: Display of the exact and simulated time-to-serve for Federer’s example.
</p>
</div>
<p>To evaluate the accuracy of the simulation-based predictions, Figure 8.7 displays the exact and a density estimate of the simulation-based predictive densities for a single time-to-serve measurement. One observes pretty good agreement using these two computation methods in this example.</p>
</div>
</div>
</div>
<div id="Normal:PPC" class="section level2">
<h2><span class="header-section-number">8.7</span> Posterior Predictive Checking</h2>
<p>In Section 8.6, the use of the posterior predictive distribution for predicting a future time-to-serve measurement was described. As discussed in Chapter 7, this distribution is also helpful for assessing the suitability of the Bayesian model.</p>
<p>In our example, we observed 20 times-to-serve for Federer. The question is whether these observed times are consistent with replicated data from the posterior predictive distribution. In this setting, replicated refers to the same sample size as our original sample. In other words, if one takes samples of 20 from the posterior predictive distribution, do these replicated datasets resemble the observed sample?</p>
<p>Since the population standard deviation is known as <span class="math inline">\(\sigma = 4\)</span> seconds, the sampling distribution of <span class="math inline">\(Y\)</span> is Normal with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. One simulates replicated data <span class="math inline">\(\tilde Y_1, ..., \tilde Y_{20}\)</span> from the posterior predictive distribution in two steps:</p>
<ol style="list-style-type: decimal">
<li><p>Sample a value of <span class="math inline">\(\mu\)</span> from its posterior distribution
<span class="math display" id="eq:ppc1">\[\begin{eqnarray}
\mu \sim \textrm{Normal}\left(\frac{\phi_0\mu_0 + n\phi\bar{y}}{\phi_0 + n\phi}, \sqrt{\frac{1}{\phi_0 + n\phi}}\right).
\tag{8.33}
\end{eqnarray}\]</span></p></li>
<li><p>Sample <span class="math inline">\(\tilde Y_1, ..., \tilde Y_{20}\)</span> from the data model
<span class="math display" id="eq:ppc2">\[\begin{eqnarray}
\tilde{Y} \sim \textrm{Normal}(\mu, \sigma).
\tag{8.34}
\end{eqnarray}\]</span></p></li>
</ol>
<p>This method is implemented in the following R script to simulate 1000 replicated samples from the posterior predictive distribution. The vector <code>pred_mu_sim</code> contains draws from the posterior distribution and the matrix <code>ytilde</code> contains the simulated predictions where each row of the matrix is a simulated sample of 20 future times.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="mean.html#cb25-1"></a>sigma &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb25-2"><a href="mean.html#cb25-2"></a>mu_n &lt;-<span class="st"> </span><span class="fl">17.4</span></span>
<span id="cb25-3"><a href="mean.html#cb25-3"></a>sigma_n &lt;-<span class="st"> </span><span class="fl">0.77</span></span>
<span id="cb25-4"><a href="mean.html#cb25-4"></a>S &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb25-5"><a href="mean.html#cb25-5"></a>pred_mu_sim &lt;-<span class="st"> </span><span class="kw">rnorm</span>(S, mu_n, sigma_n)</span>
<span id="cb25-6"><a href="mean.html#cb25-6"></a>sim_ytilde &lt;-<span class="st"> </span><span class="cf">function</span>(j){</span>
<span id="cb25-7"><a href="mean.html#cb25-7"></a>  <span class="kw">rnorm</span>(<span class="dv">20</span>, pred_mu_sim[j], sigma)</span>
<span id="cb25-8"><a href="mean.html#cb25-8"></a>}</span>
<span id="cb25-9"><a href="mean.html#cb25-9"></a>ytilde &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>S, sim_ytilde))</span></code></pre></div>
<p>To judge goodness of fit, we wish to compare these simulated replicated datasets from the posterior predictive distribution with the observed data. One convenient way to implement this comparison is to compute some “testing function”, <span class="math inline">\(T(\tilde y)\)</span>, on each replicated dataset. If we have 1000 replicated datasets, one has 1000 values of the testing function. One constructs a graph of these values and overlays the value of the testing function on the observed data <span class="math inline">\(T(y)\)</span>. If the observed value is in the tail of the posterior predictive distribution of <span class="math inline">\(T(\tilde y)\)</span>, this indicates some misfit of the observed data with the Bayesian model.</p>
<p>To implement this procedure, one needs to choose a testing function <span class="math inline">\(T(\tilde y)\)</span>. Suppose, for example, one decides to use the sample mean <span class="math inline">\(T(\tilde y) = \sum \tilde y_j / 20\)</span>. In the R script, we compute the sample mean on each row of the simulated prediction matrix.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="mean.html#cb26-1"></a>pred_ybar_sim &lt;-<span class="st"> </span><span class="kw">apply</span>(ytilde, <span class="dv">1</span>, mean)</span></code></pre></div>
<p>Figure 8.8 displays a density estimate of the simulated values from the posterior predictive distribution of <span class="math inline">\(\bar Y\)</span> and the observed value of the sample mean <span class="math inline">\(\bar Y = 17.20\)</span> is displayed as a vertical line. Since this observed mean is in the middle of this distribution, one concludes that this observation is consistent with samples predicted from the Bayesian model. It should be noted that this conclusion about model fit is sensitive to the choice of checking function <span class="math inline">\(T()\)</span>. In the end-of-chapter exercises, the reader will explore the suitability of this model using alternative choices for the checking function.</p>
<div class="figure"><span id="fig:unnamed-chunk-31"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-31-1.png" alt="Display of the posterior predictive mean time-to-serve for twenty observations. The observed mean time-to-serve value is displayed by a vertical line." width="672" />
<p class="caption">
Figure 8.8: Display of the posterior predictive mean time-to-serve for twenty observations. The observed mean time-to-serve value is displayed by a vertical line.
</p>
</div>
</div>
<div id="modeling-count-data" class="section level2">
<h2><span class="header-section-number">8.8</span> Modeling Count Data</h2>
<p>To further illustrate the Bayesian approach to inference for measurements, consider Poisson sampling, a popular model for count data. One assumes that one observes a random sample from a Poisson distribution with an unknown rate parameter <span class="math inline">\(\lambda\)</span>. The conjugate prior for the Poisson mean is the Gamma distribution. This scenario provides further practice in various Bayesian computations, such as computing the likelihood function and posterior distribution, and obtaining the predictive distribution to learn about future data. In this section, we focus on the main results and the detailed derivations are left as end-of-chapter exercises.</p>
<div id="examples-1" class="section level3">
<h3><span class="header-section-number">8.8.1</span> Examples</h3>
<p><strong>Counts of patients in an emergency room</strong></p>
<p>A hospital wants to determine how many doctors and nurses to assign on their emergency room (ER) team between 10pm and 11pm during the week. An important piece of information is the count of patients arriving in the ER in this one-hour period.</p>
<p>For a count measurement variable such as the count of patients, a popular sampling model is the Poisson distribution. This distribution is used to model the number of times an event occurs in an interval of time or space. In the current example, the event is a patient’s arrival to the ER, and the time interval is the period between 10pm and 11pm. The hospital wishes to learn about the average count of patients arriving to the ER each hour. Perhaps more importantly, the hospital wants to predict the patient count since that will directly address the scheduling of doctors and nurses question.</p>
<p><strong>Counts of visitors to a website</strong></p>
<p>As a second example, suppose one is interested in monitoring the popularity of a particular blog focusing on baseball analytics. Table 8.7 displays the number of visitors viewing this blog for 28 days during June of 2019. In this setting, the event of interest is a visit to the blog website and the time interval is a single day. The blog author is particularly interested in learning about the average number of visitors during the days Monday through Friday and predicting the number of visits for a future day in the summer of 2019.</p>
<p>Table 8.7. Number of visitors to a baseball blog site during different days during June, 2019.</p>
<table>
<thead>
<tr class="header">
<th align="right"></th>
<th align="right">Fri</th>
<th align="right">Sat</th>
<th align="right">Sun</th>
<th align="right">Mon</th>
<th align="right">Tue</th>
<th align="right">Wed</th>
<th align="right">Thu</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Week 1</td>
<td align="right">95</td>
<td align="right">81</td>
<td align="right">85</td>
<td align="right">100</td>
<td align="right">111</td>
<td align="right">130</td>
<td align="right">113</td>
</tr>
<tr class="even">
<td align="right">Week 2</td>
<td align="right">92</td>
<td align="right">65</td>
<td align="right">78</td>
<td align="right">96</td>
<td align="right">118</td>
<td align="right">120</td>
<td align="right">104</td>
</tr>
<tr class="odd">
<td align="right">Week 3</td>
<td align="right">91</td>
<td align="right">91</td>
<td align="right">79</td>
<td align="right">106</td>
<td align="right">91</td>
<td align="right">114</td>
<td align="right">110</td>
</tr>
<tr class="even">
<td align="right">Week 4</td>
<td align="right">98</td>
<td align="right">61</td>
<td align="right">84</td>
<td align="right">96</td>
<td align="right">126</td>
<td align="right">119</td>
<td align="right">90</td>
</tr>
</tbody>
</table>
<p>Count of visitors to blog during 28 days during June
2019.</p>
</div>
<div id="the-poisson-distribution" class="section level3">
<h3><span class="header-section-number">8.8.2</span> The Poisson distribution</h3>
<p>
Let the random variable <span class="math inline">\(Y\)</span> denote the number of occurrences of an event in an interval with sample space <span class="math inline">\(\{0, 1, 2, \cdots \}\)</span>. In contrast to the Normally distributed continuous measurement, note that <span class="math inline">\(Y\)</span> only takes integer values from 0 to infinity.
The variable <span class="math inline">\(Y\)</span> follows a Poisson distribution with rate parameter <span class="math inline">\(\lambda\)</span> when the probability mass function (pmf) of observing <span class="math inline">\(y\)</span> events in an interval is given by</p>
<p><span class="math display" id="eq:Poissonpmf">\[\begin{eqnarray}
f(Y = y \mid \lambda) = e^{-\lambda}\frac{\lambda^y}{y!}, \, \, y = 0, 1, 2, ...
\tag{8.35}
\end{eqnarray}\]</span>
where <span class="math inline">\(\lambda\)</span> is the average number of events per interval, <span class="math inline">\(e = 2.71828...\)</span> is Euler’s number, and <span class="math inline">\(y!\)</span> is the factorial of <span class="math inline">\(y\)</span>.</p>
<p>The Poisson sampling model is based on several assumptions about the sampling process. One assumes that the time interval is fixed, counts of arrivals occurring during different time intervals are independent, and the rate <span class="math inline">\(\lambda\)</span> at which the arrivals occur is constant over time.
To check the suitability of the Poisson distribution for the examples, one needs to check the conditions one by one.</p>
<ol style="list-style-type: decimal">
<li>The time interval is fixed in the ER example as we observe patient arrivals during a one hour period between 10pm and 11pm. For the blog visits example, the fixed time period is one day.</li>
<li>In both examples, one assumes that events occur independently during different time intervals.
In the ER example it is reasonable to assume that the time of one patient’s arrival does not influence the time of another patient’s arrival. For the website visits example, if different people are visiting the website on different days, then one could assume the number of visits on one day would be independent of the number of visits on another day.</li>
<li>Is it reasonable to assume the rate <span class="math inline">\(\lambda\)</span> at which events occur is constant through the time interval? In the ER example, one might not think that the rate of patient arrivals would change much through one hour during the evening, so it seems reasonable to assume that the average number of events is constant in the fixed interval.
Similarly, if one focuses on weekdays, then for the website visits example, it is reasonable to assume that the average number of visits remains constant across days.</li>
</ol>
<p>In some situations, the second and third conditions will be violated.
In our ER example, the occurrence of serious accidents may bring multiple groups of patients to the ER at certain time intervals. In this case, arrival times of patients may not be independent and the arrival rate <span class="math inline">\(\lambda\)</span> in one subinterval will be higher than the arrival rate of another subinterval. When such situations occur, one needs to decide about the severity of the violation of the conditions and possibly use an alternative sampling model instead of the Poisson.</p>
<p>As evident in Equation (8.35), the Poisson distribution has only one parameter, the rate parameter <span class="math inline">\(\lambda\)</span>, so the Poisson sampling model belongs to the family of one-parameter sampling models.
The Binomial data model with success probability <span class="math inline">\(p\)</span> and the Normal data model with mean parameter <span class="math inline">\(\mu\)</span> (with known standard deviation) are two other examples of one-parameter models. One distinguishes these models by the type of possible sample values, discrete or continuous.
The Binomial random variable is the number of successes and the Poisson random variable is a count of arrivals, so they both are discrete one-parameter models. In contrast, the Normal sampling data model is a continuous one-parameter model.</p>
</div>
<div id="bayesian-inferences" class="section level3">
<h3><span class="header-section-number">8.8.3</span> Bayesian inferences</h3>
<p>
The reader should be familiar with the typical procedure of Bayesian inference and prediction for one-parameter models. We rewrite this procedure in the context of the Poisson sampling model.</p>
<ol style="list-style-type: decimal">
<li><strong>[Step 1]</strong> One constructs a prior expressing an opinion about the location of the rate <span class="math inline">\(\lambda\)</span> before any data is collected.</li>
<li><strong>[Step 2]</strong> One takes the sample of intervals and records the number of arrivals in each interval. From this data, one forms the likelihood, the probability of these observations expressed as a function of <span class="math inline">\(\lambda\)</span>.</li>
<li><strong>[Step 3]</strong> One uses Bayes’ rule to compute the posterior – this distribution updates the prior opinion about <span class="math inline">\(\lambda\)</span> given the information from the data.</li>
<li>In addition, one computes the predictive distribution to learn about the number of arrivals in future intervals. The posterior predictive distribution is also useful in checking the appropriateness of our model.</li>
</ol>
<p><strong>Gamma prior distribution</strong></p>
<p>One begins by constructing a prior density to express one’s opinion about the rate parameter <span class="math inline">\(\lambda\)</span>. Since the rate is a positive continuous parameter, one needs to construct a prior density that places its support only on positive values. The convenient choice of prior distributions for Poisson sampling is the Gamma distribution which has a density function given by
<span class="math display" id="eq:Gammaprior">\[\begin{eqnarray}
\pi(\lambda \mid \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma{(\alpha)}} \lambda^{\alpha-1}e^{-\beta \lambda}, \,\,\, \text{for}\,\, \lambda &gt; 0, \,\, \text{and}\,\,  \alpha, \beta &gt; 0,
\tag{8.36}
\end{eqnarray}\]</span>
where <span class="math inline">\(\Gamma(\alpha)\)</span> is the Gamma function evaluated at <span class="math inline">\(\alpha\)</span>.
The Gamma density is a continuous density where the support is on positive values. It depends on two parameters, a positive shape parameter <span class="math inline">\(\alpha\)</span> and a positive rate parameter <span class="math inline">\(\beta\)</span>.</p>
<p>The Gamma density is a flexible family of distributions that can reflect many different types of prior beliefs about the location of the parameter <span class="math inline">\(\lambda\)</span>. One chooses values of the shape <span class="math inline">\(\alpha\)</span> and the rate <span class="math inline">\(\beta\)</span> so that the Gamma density matches one’s prior information about the location of <span class="math inline">\(\lambda\)</span>. In R, the function <code>dgamma()</code> gives the density, <code>pgamma()</code> gives the distribution function and <code>qgamma()</code> gives the quantile function for the Gamma distribution. These functions are helpful in graphing the prior and choosing values of the shape and rate parameters that match prior statements about Gamma percentiles and probabilities. We provide an illustration of choosing a subjective Gamma prior in the example.</p>
<p><strong>Sampling and the likelihood</strong></p>
<p>Suppose that <span class="math inline">\(Y_1, ..., Y_n\)</span> represent the observed counts in <span class="math inline">\(n\)</span> time intervals where the counts are independent and each <span class="math inline">\(Y_i\)</span> follows a Poisson distribution with rate <span class="math inline">\(\lambda\)</span>. The joint mass function of <span class="math inline">\(Y_1, ..., Y_n\)</span> is obtained by multiplying the Poisson densities.
<span class="math display" id="eq:Gammalik1">\[\begin{eqnarray}
f(Y_1 = y_1, ... ,  Y_n = y_n \mid \lambda ) &amp;=&amp; \prod_{i=1}^{n}f(y_i \mid \lambda) \nonumber \\ 
								       &amp;\propto&amp; \lambda^{\sum_{i=1}^{n}y_i} e^{-n\lambda}.	  
\tag{8.37} 
\end{eqnarray}\]</span>
Once the counts <span class="math inline">\(y_1, ..., y_n\)</span> are observed, the likelihood of <span class="math inline">\(\lambda\)</span> is the joint probability of observing this data, viewed as a function of the rate parameter <span class="math inline">\(\lambda\)</span>.
<span class="math display" id="eq:Gammalik2">\[\begin{equation}
L(\lambda) = \lambda^{\sum_{i=1}^{n}y_i} e^{-n\lambda}.
\tag{8.38} 
\end{equation}\]</span></p>

<p>If the rate parameter <span class="math inline">\(\lambda\)</span> in the Poisson sampling model follows a Gamma prior distribution, then it turns out that the posterior distribution for <span class="math inline">\(\lambda\)</span> will also have a Gamma density with updated parameters. This demonstrates that the Gamma density is the conjugate distribution for Poisson sampling as the prior and posterior densities both come from the same family of distribution: Gamma.</p>
<p>We begin by assuming that the Poisson parameter <span class="math inline">\(\lambda\)</span> has a Gamma distribution with shape and rate parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, that is, <span class="math inline">\(\lambda \sim\)</span> Gamma<span class="math inline">\((\alpha, \beta)\)</span>. If one multiplies the Gamma prior by the likelihood function <span class="math inline">\(L(\lambda)\)</span>, then in an end-of-chapter exercise you will show that the posterior density of <span class="math inline">\(\lambda\)</span> is Gamma<span class="math inline">\((\alpha_n, \beta_n)\)</span>, where the updated parameters <span class="math inline">\(\alpha_n\)</span> and <span class="math inline">\(\beta_n\)</span> are given by
<span class="math display" id="eq:Gammaupdate">\[\begin{equation}
\alpha_n = \alpha + \sum_{i=1}^n y_i, \, \, \, \beta_n = \beta + n.
\tag{8.39} 
\end{equation}\]</span></p>
<p><strong>Inference about <span class="math inline">\(\lambda\)</span></strong></p>
<p>Once the posterior distribution has been derived, then all inferences about the Poisson parameter <span class="math inline">\(\lambda\)</span> are performed by computing particular summaries of the Gamma posterior distribution. In particular, one may be interested in testing if <span class="math inline">\(\lambda\)</span> falls in a particular region by computing a posterior probability. All of these computations are facilitated using the <code>pgamma()</code>, <code>qgamma()</code>, and <code>rgamma()</code> functions. Or one may be interested in constructing an interval estimate for <span class="math inline">\(\lambda\)</span>. In the end-of-chapter exercises, there are opportunities to perform these inferences using a dataset containing a sample of ER arrival counts.</p>
<p><strong>Prediction of future data</strong></p>
<p>One advantage of using a conjugate prior is that the predictive density for a future observation <span class="math inline">\(\tilde Y\)</span> is available in closed form. Suppose <span class="math inline">\(\lambda\)</span> is assigned a <span class="math inline">\(\textrm{Gamma}(\alpha, \beta\)</span>) prior. Then the prior predictive density of <span class="math inline">\(\tilde Y\)</span> is given by
<span class="math display" id="eq:PoissonPred">\[\begin{eqnarray}
f(\tilde{Y} = \tilde y)  &amp;=&amp; \int f(\tilde{Y} = \tilde{y} \mid \lambda) \pi(\lambda) \lambda  \nonumber \\ 
&amp; =&amp; \int \frac{e^{-\lambda} \lambda^{\tilde y}} {\tilde y!}  
\frac{\beta^{\alpha}}{\Gamma{(\alpha)}} \lambda^{\alpha-1}e^{-\beta \lambda} d \lambda  \nonumber \\
&amp;=&amp; \frac{\Gamma(\alpha + \tilde y)}{\Gamma(\alpha)} 
\frac{\beta^\alpha}{(\beta + 1)^{\tilde y + \alpha}}.
\tag{8.40} 
\end{eqnarray}\]</span></p>
<p>In addition, the posterior distribution of <span class="math inline">\(\lambda\)</span> also has the Gamma form with updated parameters <span class="math inline">\(\alpha_n\)</span> and <span class="math inline">\(\beta_n\)</span>. So Equation (8.40) also provides the posterior predictive distribution for a future count <span class="math inline">\(\tilde Y\)</span> using the updated parameter values.</p>
<p>For prediction purposes, there are several ways of summarizing the predictive distribution. One can use the formula in Equation (8.40) to directly compute <span class="math inline">\(f(\tilde Y)\)</span> for a list of values of <span class="math inline">\(\tilde Y\)</span> and then one uses the computed probabilities to form a prediction interval for <span class="math inline">\(\tilde Y\)</span>. Alternately, one simulates values of <span class="math inline">\(\tilde Y\)</span> in a two-step process. For example, if one wants to simulate a draw from the posterior predictive distribution, one would first simulate a value <span class="math inline">\(\lambda\)</span> from its posterior distribution, and given that simulated draw <span class="math inline">\(\lambda^*\)</span>, simulate <span class="math inline">\(\tilde Y\)</span> from a Poisson distribution with mean <span class="math inline">\(\lambda^*\)</span>. Repeating this process for a large number of iterations provides a sample from the posterior prediction distribution that one uses to construct a prediction interval.</p>
</div>
<div id="case-study-learning-about-website-counts" class="section level3">
<h3><span class="header-section-number">8.8.4</span> Case study: Learning about website counts</h3>
<p>Let’s return to the website example where one is interested in learning about the average weekday visits to a baseball analytics blog site. One observes the counts <span class="math inline">\(y_1, ..., y_{20}\)</span> displayed in the “Mon”, “Tue”, “Wed”, “Thu”, “Fri” columns of Table 8.7.
We assume the {<span class="math inline">\(y_i\)</span>} represent a random sample from a Poisson distribution with mean parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>Suppose one’s prior guess at the value of <span class="math inline">\(\lambda\)</span> is 80 and one wishes to match this information with a Gamma(<span class="math inline">\(\alpha, \beta\)</span>) prior. Two helpful facts about the Gamma distribution are that the mean and variance are equal to <span class="math inline">\(\mu = \alpha / \beta\)</span> and <span class="math inline">\(\sigma^2 =\alpha / \beta^2 = \mu / \beta,\)</span> respectively. Figure 8.9 displays three Gamma curves for values <span class="math inline">\((\alpha, \beta\)</span>) = (80, 1), (40, 0.5), and (20, 0.25). Each of these Gamma curves have a mean of 80 and the curves become more diffuse as the parameter <span class="math inline">\(\beta\)</span> moves from 1 to 0.25. After some thought, the user believes that the Gamma(80, 1) matches her prior beliefs. To check, she computes a prior probability interval. Using the <code>qgamma()</code> function, she finds that her 90% prior probability interval is <span class="math inline">\(Prob(65.9 &lt; \lambda &lt; 95.3) = 0.90\)</span> and this appears to be a reasonable approximation to her prior beliefs.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-32"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-32-1.png" alt="Three Gamma($\alpha, \beta$) plausible prior distributions for the average number of weekday visits to the website." width="672" />
<p class="caption">
Figure 8.9: Three Gamma(<span class="math inline">\(\alpha, \beta\)</span>) plausible prior distributions for the average number of weekday visits to the website.
</p>
</div>
<p>From the data, we compute <span class="math inline">\(\sum_{i=1}^{20} y_i = 2120\)</span> and the sample size is <span class="math inline">\(n = 20\)</span>. The posterior distribution is Gamma(<span class="math inline">\(\alpha_n, \beta_n)\)</span> where the updated parameters are
<span class="math display">\[\begin{equation*}
\alpha_n = 80 + 2120 = 2200, \, \, \beta_n = 1 + 20 = 21.
\end{equation*}\]</span>
Figure 8.10 displays the Gamma posterior curve for <span class="math inline">\(\lambda\)</span>. This figure displays a 90% probability interval which is found using the <code>qgamma()</code> function to be (101.1, 108.5). The interpretation is that the average number of visits lies between 101.1 and 108.5 with probability 0.90.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-33"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-33-1.png" alt="Posterior curve for the mean number of visits $\lambda$ to the website.  The shaded region shows the limits of a 90% interval estimate." width="672" />
<p class="caption">
Figure 8.10: Posterior curve for the mean number of visits <span class="math inline">\(\lambda\)</span> to the website. The shaded region shows the limits of a 90% interval estimate.
</p>
</div>
<p>Suppose the user is interested in predicting the number of blog visits <span class="math inline">\(\tilde Y\)</span> at a future summer weekday. One simulates the posterior predictive distribution by first simulating 1000 values from the Gamma posterior, and then simulating values of <span class="math inline">\(\tilde Y\)</span> from Poisson distributions where the Poisson means come from the posterior. Figure 8.11 displays a histogram of the simulated values from the predictive distribution. The 5th and 95th quantiles of this distribution are computed to be 88 and 123 – there is a 90% probability that that the number of visitors in a future weekday will fall in the interval (88, 123).</p>
<pre><code>##     5%    95% 
##  88.95 123.00</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-34"></span>
<img src="bookdown-demo_files/figure-html/unnamed-chunk-34-1.png" alt="Histogram of a simulated sample from the posterior predictive distribution of the number of visitors to the website on a future day." width="672" />
<p class="caption">
Figure 8.11: Histogram of a simulated sample from the posterior predictive distribution of the number of visitors to the website on a future day.
</p>
</div>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">8.9</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><strong>Another Set of Federer’s Time-to-Serve Measurements (Discrete Priors)</strong></li>
</ol>
<p>Suppose another set of thirty Federer’s time-to-serve measurements are collected with an observed mean of 19 seconds. Assume the same Uniform discrete prior on the values <span class="math inline">\(\mu\)</span> = 15, 16, …, 22. The prior and the likelihood function are displayed below.</p>
<p><span class="math display">\[\begin{equation*}
\pi(\mu) = \frac{1}{8}, \, \, \, \, \mu = 15, 16, ..., 22,
\end{equation*}\]</span></p>
<p><span class="math display">\[\begin{equation*}
L(\mu) \propto \exp\left(-\frac{n}{2 \sigma^2}(\bar y - \mu)^2\right).
\end{equation*}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Assuming <span class="math inline">\(\sigma = 4\)</span>, perform the Bayes’ rule calculation to find the posterior distribution for <span class="math inline">\(\mu\)</span>.<br />
</li>
<li>Using the posterior, find a "best" estimate at <span class="math inline">\(\mu\)</span> and an interval of values that contains <span class="math inline">\(\mu\)</span> with probability at least 0.5.</li>
</ol>
<ol start="2" style="list-style-type: decimal">
<li><strong>Temperature in Bismarck</strong></li>
</ol>
<p>Suppose one is interested in learning about the average January daily temperature (in degrees Fahrenheit) in Bismarck, North Dakota. One assumes that the daily temperature <span class="math inline">\(Y\)</span> is Normally distributed with mean <span class="math inline">\(\mu\)</span> and known standard deviation <span class="math inline">\(\sigma = 10\)</span>. Suppose that one’s prior is Uniformly distribution over the values <span class="math inline">\(\mu = 5, 10, 15, 20, 25, 30, 35\)</span>. Suppose one observes the temperature for one January day to be 28 degrees. Find the posterior distribution of <span class="math inline">\(\mu\)</span> and compute the posterior probability the mean is at least as large as 30 degrees.</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Choosing A Normal Prior</strong></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose Sam believes that the 0.25 quantile of the mean of Federer’s time-to-serve <span class="math inline">\(\mu\)</span> is 14 seconds and the 0.8 quantile is 21 seconds. Using the <code>normal.select()</code> function, construct a Normal prior distribution to match this belief.</p></li>
<li><p>Suppose Sam also believes that the 0.10 quantile of his prior is equal to 10.5 seconds. Is this statement consistent with the Normal prior chosen in part (a)? If not, how could you adjust the prior to reconcile this statement about the 0.10 quantile?</p></li>
</ol>
<ol start="4" style="list-style-type: decimal">
<li><strong>Choosing A Normal Prior</strong></li>
</ol>
<p>Another way of choosing a Normal prior for Federer’s mean time-to-serve <span class="math inline">\(\mu\)</span> is to specify statements about the prior predictive distribution for a future time-to-serve measurement <span class="math inline">\(\tilde Y\)</span>. Using results from Section 8.5.2, if <span class="math inline">\(\mu\)</span> has a Normal prior with mean <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\sigma_0\)</span>, then the predictive density of <span class="math inline">\(\tilde Y\)</span> is Normal with mean <span class="math inline">\(\mu_0\)</span> and standard deviation <span class="math inline">\(\sqrt{\sigma^2 + \sigma_0^2}\)</span>, where we are assuming that the sampling standard deviation <span class="math inline">\(\sigma = 4\)</span> seconds.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Suppose your best guess at <span class="math inline">\(\tilde Y\)</span> is 15 seconds, and you are 90 percent confident that <span class="math inline">\(\tilde Y\)</span> is smaller than 25 seconds. Find the Normal prior for <span class="math inline">\(\mu\)</span> that matches this prior information about the future time-to-serve.</p></li>
<li><p>Suppose instead that you are 90% confident that the future time-to-serve is between 18 and 24 seconds. Find the Normal prior for <span class="math inline">\(\mu\)</span> that matches this prior belief.</p></li>
</ol>
<ol start="5" style="list-style-type: decimal">
<li><strong>Bayesian Hypothesis Testing</strong></li>
</ol>
<p>The posterior distribution for the mean time-to-serve <span class="math inline">\(\mu\)</span> for Federer is Normal with mean 17.4 seconds and standard deviation 0.77 seconds.</p>
<ol style="list-style-type: lower-alpha">
<li>Using this posterior, evaluate the plausibility of the statement “Federer’s mean time-to-serve is at least 16.5 seconds.”</li>
<li>Is it reasonable to say that Federer’s mean time-to-serve falls between 17 and 18 seconds? Explain.</li>
</ol>
<ol start="6" style="list-style-type: decimal">
<li><strong>Bayesian Credible Interval</strong></li>
</ol>
<p>The posterior distribution for the mean time-to-serve <span class="math inline">\(\mu\)</span> for Federer is Normal with mean 17.4 seconds and standard deviation 0.77 seconds.</p>
<ol style="list-style-type: lower-alpha">
<li>Construct a central 98% credible interval for <span class="math inline">\(\mu\)</span>.</li>
<li>Can you use the credible interval to test the hypothesis “Federer’s mean time-to-serve is 16.5 seconds”? Explain.</li>
</ol>
<ol start="7" style="list-style-type: decimal">
<li><strong>Posterior Predictive Distribution</strong></li>
</ol>
<p>Write an R script to generate <span class="math inline">\(S = 1000\)</span> predictions of a single time-to-serve of Federer based on the posterior predictive distribution using the results given in Equation (8.31) and Equation (8.32).</p>
<ol style="list-style-type: lower-alpha">
<li>Compare the exact posterior predictive distribution (Equation (8.30)) with the density estimate of the simulated predictions.</li>
<li>Construct a 90% prediction interval for the future time-to-serve.</li>
</ol>
<ol start="8" style="list-style-type: decimal">
<li><strong>Posterior Predictive Checking</strong></li>
</ol>
<p>The posterior predictive distribution can be used to check the suitability of the Normal sampling/Normal prior model for Federer’s time-to-serve data. The function <code>post_pred_check()</code> simulates samples of <span class="math inline">\(n = 20\)</span> from the posterior predictive function, and for each sample, computes a value of the checking function <span class="math inline">\(T(\tilde y)\)</span>.</p>
<pre><code>post_pred_check &lt;- function(test_function){
  mu_n &lt;- 17.4
  sigma_n &lt;- 0.77
  sigma &lt;- 4
  n &lt;- 20
  one_sim &lt;- function(){
    mu &lt;- rnorm(1, mu_n, sigma_n)
    test_function(rnorm(n, mu, sigma))
  }
  replicate(1000, one_sim())
}</code></pre>
<p>The output of the function is 1000 draws from the posterior predictive distribution of <span class="math inline">\(T\)</span>. If the checking function is <span class="math inline">\(\max (y)\)</span>, then one would obtain 1000 draws from the posterior predictive distribution by typing</p>
<pre><code>post_pred_check(max)</code></pre>
<p>If the value of the checking function on the observed time-to-serves <span class="math inline">\(T(y)\)</span> is unusual relative to this posterior predictive distribution of <span class="math inline">\(T\)</span>, this would cast doubt on the model. The observed times-to-serve for Federer are displayed in Section 8.3.1. and repeated below.</p>
<p>15.1 11.8 21.0 22.7 18.6 16.2 11.1 13.2 20.4 19.2
21.2 14.3 18.6 16.8 20.3 19.9 15.0 13.4 19.9 15.3</p>
<ol style="list-style-type: lower-alpha">
<li><p>Use the function <code>post_pred_check()</code> with the checking function <span class="math inline">\(T(y) = \max (y)\)</span> to check the suitability of the Bayesian model.</p></li>
<li><p>Use the function <code>post_pred_check()</code> with the checking function <span class="math inline">\(T(y) = \textrm{sd}(y)\)</span> to check the suitability of the Bayesian model.</p></li>
</ol>
<ol start="9" style="list-style-type: decimal">
<li><strong>Taxi Cab Fares</strong>
</li>
</ol>
<p>Suppose a city manager is interested in learning about the mean fare <span class="math inline">\(\mu\)</span> for taxi cabs in New York City.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose the manager believes that <span class="math inline">\(\mu\)</span> is smaller than $8 with probability 0.25, and that <span class="math inline">\(\mu\)</span> is smaller than $12 with probability 0.75. Find a Normal prior that matches this prior information.</li>
<li>The manager collects the fares for twenty fares and observes the values (in dollars): 7.5, 8.5, 9.5, 6.5, 7.0, 6.0, 7.0, 16.0, 8.0, 8.5, 9.5, 13.5, 4.5, 8.5, 7.5, 13.0, 6.5, 9.5, 21.0, 6.5. Assuming these fares are Normally distributed with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma = 4\)</span>, find the posterior distribution for the mean <span class="math inline">\(\mu\)</span>.</li>
<li>Construct a 90% interval estimate for the mean fare <span class="math inline">\(\mu\)</span>.</li>
</ol>
<ol start="10" style="list-style-type: decimal">
<li><strong>Taxi Cab Fares (continued)</strong></li>
</ol>
<p>Suppose that a visitor to New York City has little knowledge about the mean taxi cab fare.</p>
<ol style="list-style-type: lower-alpha">
<li>Construct a weakly informative prior for <span class="math inline">\(\mu\)</span>.</li>
<li>Use the data from Exercise 9 to compute the posterior distribution for the mean fare.</li>
<li>Construct a 90% interval estimate for the mean fare and compare your interval with the interval computed in Exercise 9 using an informative prior.</li>
</ol>
<ol start="11" style="list-style-type: decimal">
<li><strong>Taxi Cab Fares (continued)</strong></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>In Exercise 9, one finds the posterior distribution for the mean fare <span class="math inline">\(\mu\)</span>. Write an R function to simulate a sample of twenty fares from the posterior predictive distribution.</li>
<li>Looking at the observed data, one sees an unusually large fare of $21. To see if this fare is unusual for our model, first revise your function in part (a) to simulate the maximum fare of a sample of twenty fares from the posterior predictive distribution. Then repeat this process 1000 times, collecting the maximum fares for 1000 predictive samples.</li>
<li>Construct a graph of the maximum fares. Is the fare of $21 large relative to the prediction distribution of maximum fares?<br />
</li>
<li>Based on the answer to part (c), what does that say about the suitability of our model?</li>
</ol>
<ol start="12" style="list-style-type: decimal">
<li><strong>Student Sleeping Times</strong>
</li>
</ol>
<p>How many hours do college students sleep, on the average? Recently, some introductory students were asked when they went to bed and when they woke the following morning. A following random sample of 14 sleeping times (in hours) were recorded: 9.0, 7.5, 7.0, 8.0, 5.0, 6.5, 8.5, 7.0, 9.0, 7.0, 5.5, 6.0, 8.5, 7.5. Assume that these measurements follow a Normal sampling distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, where we are given that <span class="math inline">\(\sigma = 1.5\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose that John believes a priori that the mean amount of sleep <span class="math inline">\(\mu\)</span> is Normal with mean 8 hours and standard deviation 1 hour. Find the posterior distribution of <span class="math inline">\(\mu\)</span>.</li>
<li>Construct a 90% interval estimate for the mean <span class="math inline">\(\mu\)</span>.</li>
<li>Let <span class="math inline">\(y^*\)</span> denote the sleeping time for a randomly selected student. Find the predictive distribution for <span class="math inline">\(y^*\)</span> and use this to construct a 90% prediction interval.</li>
</ol>
<ol start="13" style="list-style-type: decimal">
<li><strong>Student Sleeping Times (continued)</strong></li>
</ol>
<p>Suppose two other people are interested in learning about the mean sleeping times of college students. Mary’s prior is Normal with mean 8 hours and standard deviation 0.1 – she is pretty confident that the mean sleeping time is close to 8 hours. In contrast, Larry is very uncertain about the location of <span class="math inline">\(\mu\)</span> and assigns a Normal prior with mean 8 hours and standard deviation 3 hours.</p>
<ol style="list-style-type: lower-alpha">
<li>Find the posterior distributions of <span class="math inline">\(\mu\)</span> using Mary’s prior and using Larry’s prior.</li>
<li>Construct 90% interval estimates for <span class="math inline">\(\mu\)</span> using Mary’s and Larry’s priors.</li>
<li>Compare the interval estimates with the interval estimates constructed in Exercise 12(b) using Mary’s prior. Is the location of the interval estimate sensitive to the choice of prior? If so, explain the differences.</li>
</ol>
<ol start="14" style="list-style-type: decimal">
<li><strong>Comparing Two Means - IQ Tests on School Children</strong>
</li>
</ol>
<p>Do teachers’ expectations impact academic development of children? To find out, researchers gave an IQ test to a group of 12 elementary school children. They randomly picked six children and told teachers that the test predicts them to have high potential for accelerated growth (accelerated group); for the other six students in the group, the researchers told teachers that the test predicts them to have no potential for growth (no growth group). At the end of school year, they gave IQ tests again to all 12 students, and the change in IQ scores of each student is recorded. The following table shows the IQ score change of students in the accelerated group and the no growth group.</p>
<table>
<thead>
<tr class="header">
<th align="center">Group</th>
<th align="center">IQ score change</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Accelerated</td>
<td align="center">20, 10, 19, 15, 9, 18</td>
</tr>
<tr class="even">
<td align="center">No growth</td>
<td align="center">3, 2, 6, 10, 11, 5</td>
</tr>
</tbody>
</table>
<p>The sample means of the accelerated group and the no growth group are respectively <span class="math inline">\(\bar{y}_A = 15.2\)</span> and <span class="math inline">\(\bar{y}_N = 6.2\)</span>. Consider independent sampling models, where the IQ scores for the accelerated group (no growth group) are assumed Normal with mean <span class="math inline">\(\mu_A\)</span> (<span class="math inline">\(\mu_N\)</span>) with known standard deviation <span class="math inline">\(\sigma = 4\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
Y_{A, i} &amp;\overset{i.i.d.}{\sim}&amp; \textrm{Normal}(\mu_A, 4), \,\,\, \text{for}\,\, i = 1, \cdots n_A, \\
Y_{N, i} &amp;\overset{i.i.d.}{\sim}&amp; \textrm{Normal}(\mu_N, 4), \,\,\, \text{for}\,\, i = 1, \cdots n_N, 
\end{eqnarray*}\]</span>
where <span class="math inline">\(n_A = n_N = 6\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Assuming independent sampling, write down the likelihood function of the means <span class="math inline">\((\mu_A, \mu_B)\)</span>.</li>
<li>Assume that one’s prior beliefs about <span class="math inline">\(\mu_A\)</span> and <span class="math inline">\(\mu_N\)</span> are independent, where <span class="math inline">\(\mu_A \sim \textrm{Normal}(\gamma_A, \tau_A)\)</span> and <span class="math inline">\(\mu_N \sim \textrm{Normal}(\gamma_N, \tau_N)\)</span>. Show that the posterior distributions for <span class="math inline">\(\mu_A\)</span> and <span class="math inline">\(\mu_N\)</span> are independent Normal and find the mean and standard deviation parameters for each distribution.</li>
</ol>
<ol start="15" style="list-style-type: decimal">
<li><strong>Comparing Two Means - IQ Tests on School Children (continued)</strong></li>
</ol>
<p>In Exercise 14, you should have established that the mean IQ score changes <span class="math inline">\(\mu_A\)</span> and <span class="math inline">\(\mu_N\)</span> have independent Normal posterior distributions. Assume that one has vague prior beliefs and <span class="math inline">\(\mu_A \sim \textrm{Normal}(0, 20)\)</span> and <span class="math inline">\(\mu_N \sim \textrm{Normal}(0, 20)\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Is the average improvement for the accelerated group larger than that for the no growth group? Consider the parameter <span class="math inline">\(\delta = \mu_A - \mu_N\)</span> to measure the difference in means. The question now becomes finding the posterior probability of <span class="math inline">\(\delta &gt; 0\)</span>, i.e. <span class="math inline">\(p(\mu_A - \mu_N &gt; 0 \mid y_A, y_N)\)</span>, where <span class="math inline">\(y_A\)</span> and <span class="math inline">\(y_N\)</span> are the vectors of recorded IQ score change. [Hint: simulate a vector <span class="math inline">\(s_A\)</span> of posterior samples of <span class="math inline">\(\mu_A\)</span> and another vector <span class="math inline">\(s_N\)</span> of posterior samples of <span class="math inline">\(\mu_N\)</span> (make sure to use the same number of samples) and subtract <span class="math inline">\(s_N\)</span> from <span class="math inline">\(s_A\)</span>, which gives us a vector of posterior differences between <span class="math inline">\(s_N\)</span> and <span class="math inline">\(s_A\)</span>. This vector of posterior differences serves as an approximation to the posterior distribution of <span class="math inline">\(\delta\)</span>.]</p></li>
<li><p>What is the probability that a randomly selected child assigned to the accelerated group will have larger improvement than a randomly selected child assigned to the no growth group? Consider <span class="math inline">\(\tilde{Y}_A\)</span> and <span class="math inline">\(\tilde{Y}_N\)</span> to be random variables for predicted IQ score change for the accelerated group and the no growth group, respectively. The question now becomes finding the posterior predictive probability of <span class="math inline">\(\tilde{Y}_A &gt; \tilde{Y}_N\)</span>, i.e. <span class="math inline">\(p(\tilde{Y}_A &gt; \tilde{Y}_N \mid y_A, y_N)\)</span>, where <span class="math inline">\(y_A\)</span> and <span class="math inline">\(y_N\)</span> are the vectors of recorded IQ score change, each of length 6. [Hint: Show that the posterior predictive distributions of <span class="math inline">\(\tilde{Y}_A\)</span> and <span class="math inline">\(\tilde{Y}_N\)</span> are independent. Simulate predicted IQ score changes from the posterior predictive distributions for the two groups, then simulate the posterior predictive distribution of <span class="math inline">\(\tilde{Y}_A - \tilde{Y}_N\)</span> by taking the difference of simulated draws.]</p></li>
</ol>
<ol start="16" style="list-style-type: decimal">
<li><strong>Comparing Two Means - Prices of Diamonds</strong></li>
</ol>
<p>Weights of diamonds are measured in carats. The difference between the size of a 0.99 carat diamond and a 1 carat diamond is most likely undetectable to the naked human eye, but the price of a 1 carat diamond tends to be much higher than the price of a 0.99 carat diamond. To find out if it is truly the case, data on point prices (the prices of 0.99 carat diamonds divided by 99, and the prices of 1 carat diamonds divided by 100) of <span class="math inline">\(n_{99} = 23\)</span> of 0.99 carat diamonds and <span class="math inline">\(n_{100} = 25\)</span> of 1 carat diamonds were collected and stored in the files <code>pt99price.csv</code> and <code>pt100price.csv</code>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Explore the two datasets by making plots and computing summary statistics. What are the findings?</p></li>
<li><p>Consider independent Normal sampling models for these datasets with a fixed and known value of the standard deviation. From your exploratory work, choose a value for the standard deviation.</p></li>
<li><p>Choose appropriate weakly informative prior distributions, and use posterior simulation to answer whether the average point price of the 1 carat diamonds is higher than that of the 0.99 diamonds.</p></li>
<li><p>Perform posterior predictive checks of the Bayesian inferences obtained in part (c).</p></li>
</ol>
<ol start="17" style="list-style-type: decimal">
<li><strong>Gamma-Poisson Conjugacy Derivation</strong></li>
</ol>
<p>Section 8.8.3 presents the Bayesian update results for Poisson sampling with the use of the Gamma conjugate prior.</p>
<ol style="list-style-type: lower-alpha">
<li>Verify the equation for the likelihood in Equation (8.37). [Hint:</li>
</ol>
<p><span class="math display">\[\begin{eqnarray*}
f( Y_1 = y_1, ..., Y_n = y_n \mid \lambda ) &amp;=&amp; \prod_{i=1}^{n}f(y_i \mid \lambda) \nonumber \\ 
								       &amp;=&amp; \prod_{i=1}^{n} \frac{1}{y_i!} \lambda^{y_i} e^{-\lambda},
\end{eqnarray*}\]</span>
the joint sampling density of <span class="math inline">\(n\)</span> <span class="math inline">\(i.i.d.\)</span> Poisson distributed random variables.]</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Assuming that the Poisson parameter <span class="math inline">\(\lambda\)</span> has a Gamma prior with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span>, show that the posterior distribution of <span class="math inline">\(\lambda\)</span> has a Gamma functional form and find the parameters of this Gamma distribution.</li>
</ol>
<ol start="18" style="list-style-type: decimal">
<li><strong>The Number of ER Visits: the Prior</strong></li>
</ol>
<p>Suppose two people, Pedro and Mia, have different prior beliefs about the average number of ER visits during the 10pm - 11pm time period. Pedro’s prior information is matched to a Gamma distribution with parameters <span class="math inline">\(\alpha = 70\)</span> and <span class="math inline">\(\beta = 10\)</span>, and Mia’s beliefs are matched to a Gamma distribution with <span class="math inline">\(\alpha = 33.3\)</span> and <span class="math inline">\(\beta = 3.3\)</span>. The two Gamma priors are displayed in Figure 8.12.</p>

<div class="figure"><span id="fig:unnamed-chunk-35"></span>
<img src="../LATEX/figures/chapter8/gammapriors.png" alt="Two Gamma priors for the average number of visits to ER during a particular hour in the evening." width="500" />
<p class="caption">
Figure 8.12: Two Gamma priors for the average number of visits to ER during a particular hour in the evening.
</p>
</div>
<ol style="list-style-type: lower-alpha">
<li><p>Compare the priors of Pedro and Mia with respect to average value and spread. Which person believes that there will be more ER visits, on average? Which person is more confident of his/her "best guess" at the average number of ER visits?</p></li>
<li><p>Using the <code>qgamma()</code> function, construct 90% interval estimates for <span class="math inline">\(\lambda\)</span> using Pedro’s prior and Mia’s prior.</p></li>
<li><p>After some thought, Pedro believes that his best prior guess at <span class="math inline">\(\lambda\)</span> is correct, but he is less confident in this guess. Explain how Pedro can adjust the parameters of his Gamma prior to reflect this new prior belief.</p></li>
<li><p>Mia also revisits her prior. Her best guess at the average number of ER visits is now 3 larger than her previous best guess, but the degree of confidence in this guess hasn’t changed. Explain how Mia can adjust the parameters of her Gamma prior to reflect this new prior belief.</p></li>
</ol>
<ol start="19" style="list-style-type: decimal">
<li><strong>The Number of ER Visits</strong></li>
</ol>
<p>A hospital collects the number of patients in the emergency room (ER) admitted between 10 pm and 11 pm for each day of a week. Table 8.8 records the day and the number of ER visits for the given day.</p>
<p>Table 8.8 Number of ER visits during the period 10 pm to 11 pm for each day for a particular week at a hospital.</p>
<table>
<thead>
<tr class="header">
<th align="center">Day</th>
<th align="center">Number of ER visits</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Sunday</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">Monday</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">Tuesday</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">Wednesday</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center">Thursday</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">Friday</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center">Saturday</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
<p>Suppose one assumes Poisson sampling for the counts, and a conjugate Gamma prior with parameters <span class="math inline">\(\alpha = 70\)</span> and <span class="math inline">\(\beta = 10\)</span> for the Poisson rate parameter <span class="math inline">\(\lambda\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Given the sample shown in Table , obtain the posterior distribution for <span class="math inline">\(\lambda\)</span> through the Gamma-Poisson conjugacy. Obtain a 95% posterior credible interval for <span class="math inline">\(\lambda\)</span>.</p></li>
<li><p>Suppose a hospital administrator states that the average number of ER visits during any evening hour does not exceed 6. By computing a posterior probability, evaluate the validity of the administrator’s statement.</p></li>
<li><p>The hospital is interested in predicting the number of ER visits between 10 pm and 11 pm for another week. Use simulations to generate posterior predictions of the number of ER visits for another week (seven days).</p></li>
</ol>
<ol start="20" style="list-style-type: decimal">
<li><strong>Times Between Traffic Accidents</strong></li>
</ol>
<p>The Exponential distribution is often used as a model to describe the time between events, such as traffic accidents. A random variable <span class="math inline">\(Y\)</span> has an Exponential distribution if its pdf is as follows.</p>
<p><span class="math display" id="eq:exponential">\[\begin{equation}
  f(y \mid \lambda) =\begin{cases}
    \lambda \exp(-\lambda y), &amp; \text{if $y \geq 0$}.\\
    0, &amp; \text{if $y &lt; 0$}.
  \end{cases}
\tag{8.41}
\end{equation}\]</span>
Here, the parameter <span class="math inline">\(\lambda &gt; 0\)</span>, considered as the rate of event occurrences. This is a one-parameter model.</p>
<ol style="list-style-type: lower-alpha">
<li><p>The Gamma distribution is a conjugate prior distribution for the rate parameter <span class="math inline">\(\lambda\)</span> in the Exponential data model. Use the prior distribution <span class="math inline">\(\lambda \sim \textrm{Gamma}(a, b)\)</span>, and find its posterior distribution <span class="math inline">\(\pi(\lambda \mid y_1, \cdots, y_n)\)</span>, where <span class="math inline">\(y_i \overset{i.i.d.}{\sim} \textrm{Exponential}(\lambda)\)</span> for <span class="math inline">\(i = 1, \cdots, n\)</span>.</p></li>
<li><p>Suppose 10 times between traffic accidents are collected: 1.5, 15, 60.3, 30.5, 2.8, 56.4, 27, 6.4, 110.7, 25.4 (in minutes). With the posterior distribution derived in part (a), use Monte Carlo approximation to calculate the posterior mean, median, and a middle 95% credible interval for the rate <span class="math inline">\(\lambda\)</span>. [Hint: choose the appropriate R functions from <code>dgamma()</code>, <code>pgamma()</code>\index{pgamma(), <code>qgamma()</code>, and <code>rgamma()</code>.]</p></li>
<li><p>Use Monte Carlo approximation to generate another set of 10 predicted times between events. [Hint: <code>rexp()</code> generates random draws from an Exponential distribution.]</p></li>
</ol>
<ol start="21" style="list-style-type: decimal">
<li><strong>Modeling Survival Times</strong></li>
</ol>
<p>The Weibull distribution is often used as a model for survival
times in biomedical, demographic, and engineering analyses. A
random variable <span class="math inline">\(Y\)</span> has a Weibull distribution if its pdf is as
follows.
<span class="math display" id="eq:weibull">\[\begin{eqnarray}
f(y \mid \alpha, \lambda) = \lambda \alpha y^{\alpha -1}
\exp(-\lambda y^\alpha) \,\,\,\,\,\,\,\,\, \text{for } y &gt; 0.
\tag{8.42}
\end{eqnarray}\]</span>
Here, <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(\lambda&gt;0\)</span> are parameters of the
distribution. For this problem, assume that <span class="math inline">\(\alpha = \alpha_0\)</span> is known, but <span class="math inline">\(\lambda\)</span> is not known, i.e. a simplified case of a one-parameter model. Also assume that
software routines for simulating from Weibull distributions are available (e.g. <code>rweibull()</code>)</p>
<ol style="list-style-type: lower-alpha">
<li><p>Assuming a prior distribution <span class="math inline">\(\pi(\lambda \mid \alpha  = \alpha_0) \propto 1\)</span>, find its posterior <span class="math inline">\(\pi(\lambda \mid y_1, \dots, y_n, \alpha = \alpha_0)\)</span>, where <span class="math inline">\(y_i \overset{i.i.d.}{\sim} \textrm{Weibull}(\lambda, \alpha = \alpha_0)\)</span> for <span class="math inline">\(i = 1, \cdots, n\)</span>. Write the name of the distribution and
expressions for its parameter values.</p></li>
<li><p>Using the posterior distribution derived in part (a), explain
step-by-step how you would use Monte Carlo simulation to approximate
the posterior median survival time, assuming
that <span class="math inline">\(\alpha = \alpha_0\)</span>.</p></li>
<li><p>What family of distributions represents the conjugate prior
distributions for <span class="math inline">\(\lambda\)</span>, assuming
that <span class="math inline">\(\alpha = \alpha_0\)</span>.</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="proportion.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simulation-by-markov-chain-monte-carlo.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-mean.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
